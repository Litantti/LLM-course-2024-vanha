{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/Litantti/LLM-course-2024/blob/main/CoPro-simple-local-rag%20turku-embedding%2016.12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"oniz63GGyTGy"},"source":["<a target=\"_blank\" href=\"https://colab.research.google.com/github/mrdbourke/simple-local-rag/blob/main/00-simple-local-rag.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>\n","\n","# Create and run a local RAG pipeline from scratch\n","\n","The goal of this notebook is to build a RAG (Retrieval Augmented Generation) pipeline from scratch and have it run on a local GPU.\n","\n","Specifically, we'd like to be able to open a PDF file, ask questions (queries) of it and have them answered by a Large Language Model (LLM).\n","\n","There are frameworks that replicate this kind of workflow, including [LlamaIndex](https://www.llamaindex.ai/) and [LangChain](https://www.langchain.com/), however, the goal of building from scratch is to be able to inspect and customize all the parts."]},{"cell_type":"markdown","metadata":{"id":"_Q6iJNuPyTG1"},"source":["## What is RAG?\n","\n","RAG stands for Retrieval Augmented Generation.\n","\n","It was introduced in the paper [*Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*](https://arxiv.org/abs/2005.11401).\n","\n","Each step can be roughly broken down to:\n","\n","* **Retrieval** - Seeking relevant information from a source given a query. For example, getting relevant passages of Wikipedia text from a database given a question.\n","* **Augmented** - Using the relevant retrieved information to modify an input to a generative model (e.g. an LLM).\n","* **Generation** - Generating an output given an input. For example, in the case of an LLM, generating a passage of text given an input prompt."]},{"cell_type":"markdown","metadata":{"id":"aaTgyZFuyTG2"},"source":["## Why RAG?\n","\n","The main goal of RAG is to improve the generation outptus of LLMs.\n","\n","Two primary improvements can be seen as:\n","1. **Preventing hallucinations** - LLMs are incredible but they are prone to potential hallucination, as in, generating something that *looks* correct but isn't. RAG pipelines can help LLMs generate more factual outputs by providing them with factual (retrieved) inputs. And even if the generated answer from a RAG pipeline doesn't seem correct, because of retrieval, you also have access to the sources where it came from.\n","2. **Work with custom data** - Many base LLMs are trained with internet-scale text data. This means they have a great ability to model language, however, they often lack specific knowledge. RAG systems can provide LLMs with domain-specific data such as medical information or company documentation and thus customized their outputs to suit specific use cases.\n","\n","The authors of the original RAG paper mentioned above outlined these two points in their discussion.\n","\n","> This work offers several positive societal benefits over previous work: the fact that it is more\n","strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less\n","with generations that are more factual, and offers more control and interpretability. RAG could be\n","employed in a wide variety of scenarios with direct benefit to society, for example by endowing it\n","with a medical index and asking it open-domain questions on that topic, or by helping people be more\n","effective at their jobs.\n","\n","RAG can also be a much quicker solution to implement than fine-tuning an LLM on specific data.\n"]},{"cell_type":"markdown","metadata":{"id":"PjVPMrvlyTG3"},"source":["\n","## What kind of problems can RAG be used for?\n","\n","RAG can help anywhere there is a specific set of information that an LLM may not have in its training data (e.g. anything not publicly accessible on the internet).\n","\n","For example you could use RAG for:\n","* **Customer support Q&A chat** - By treating your existing customer support documentation as a resource, when a customer asks a question, you could have a system retrieve relevant documentation snippets and then have an LLM craft those snippets into an answer. Think of this as a \"chatbot for your documentation\". Klarna, a large financial company, [uses a system like this](https://www.klarna.com/international/press/klarna-ai-assistant-handles-two-thirds-of-customer-service-chats-in-its-first-month/) to save $40M per year on customer support costs.\n","* **Email chain analysis** - Let's say you're an insurance company with long threads of emails between customers and insurance agents. Instead of searching through each individual email, you could retrieve relevant passages and have an LLM create strucutred outputs of insurance claims.\n","* **Company internal documentation chat** - If you've worked at a large company, you know how hard it can be to get an answer sometimes. Why not let a RAG system index your company information and have an LLM answer questions you may have? The benefit of RAG is that you will have references to resources to learn more if the LLM answer doesn't suffice.\n","* **Textbook Q&A** - Let's say you're studying for your exams and constantly flicking through a large textbook looking for answers to your quesitons. RAG can help provide answers as well as references to learn more.\n","\n","All of these have the common theme of retrieving relevant resources and then presenting them in an understandable way using an LLM.\n","\n","From this angle, you can consider an LLM a calculator for words.\n"]},{"cell_type":"markdown","metadata":{"id":"GUlapyg0yTG4"},"source":["## Why local?\n","\n","Privacy, speed, cost.\n","\n","Running locally means you use your own hardware.\n","\n","From a privacy standpoint, this means you don't have send potentially sensitive data to an API.\n","\n","From a speed standpoint, it means you won't necessarily have to wait for an API queue or downtime, if your hardware is running, the pipeline can run.\n","\n","And from a cost standpoint, running on your own hardware often has a heavier starting cost but little to no costs after that.\n","\n","Performance wise, LLM APIs may still perform better than an open-source model running locally on general tasks but there are more and more examples appearing of smaller, focused models outperforming larger models.\n"]},{"cell_type":"markdown","metadata":{"id":"2UIEKORIyTG4"},"source":["## Key terms\n","\n","| Term | Description |\n","| ----- | ----- |\n","| **Token** | A sub-word piece of text. For example, \"hello, world!\" could be split into [\"hello\", \",\", \"world\", \"!\"]. A token can be a whole word,<br> part of a word or group of punctuation characters. 1 token ~= 4 characters in English, 100 tokens ~= 75 words.<br> Text gets broken into tokens before being passed to an LLM. |\n","| **Embedding** | A learned numerical representation of a piece of data. For example, a sentence of text could be represented by a vector with<br> 768 values. Similar pieces of text (in meaning) will ideally have similar values. |\n","| **Embedding model** | A model designed to accept input data and output a numerical representation. For example, a text embedding model may take in 384 <br>tokens of text and turn it into a vector of size 768. An embedding model can and often is different to an LLM model. |\n","| **Similarity search/vector search** | Similarity search/vector search aims to find two vectors which are close together in high-demensional space. For example, <br>two pieces of similar text passed through an embedding model should have a high similarity score, whereas two pieces of text about<br> different topics will have a lower similarity score. Common similarity score measures are dot product and cosine similarity. |\n","| **Large Language Model (LLM)** | A model which has been trained to numerically represent the patterns in text. A generative LLM will continue a sequence when given a sequence. <br>For example, given a sequence of the text \"hello, world!\", a genertive LLM may produce \"we're going to build a RAG pipeline today!\".<br> This generation will be highly dependant on the training data and prompt. |\n","| **LLM context window** | The number of tokens a LLM can accept as input. For example, as of March 2024, GPT-4 has a default context window of 32k tokens<br> (about 96 pages of text) but can go up to 128k if needed. A recent open-source LLM from Google, Gemma (March 2024) has a context<br> window of 8,192 tokens (about 24 pages of text). A higher context window means an LLM can accept more relevant information<br> to assist with a query. For example, in a RAG pipeline, if a model has a larger context window, it can accept more reference items<br> from the retrieval system to aid with its generation. |\n","| **Prompt** | A common term for describing the input to a generative LLM. The idea of \"[prompt engineering](https://en.wikipedia.org/wiki/Prompt_engineering)\" is to structure a text-based<br> (or potentially image-based as well) input to a generative LLM in a specific way so that the generated output is ideal. This technique is<br> possible because of a LLMs capacity for in-context learning, as in, it is able to use its representation of language to breakdown <br>the prompt and recognize what a suitable output may be (note: the output of LLMs is probable, so terms like \"may output\" are used). |\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6ev13j3HyTG4"},"source":[" ## What we're going to build\n","\n","We're going to build RAG pipeline which enables us to chat with a PDF document, specifically an open-source [nutrition textbook](https://pressbooks.oer.hawaii.edu/humannutrition2/), ~1200 pages long.\n","\n","You could call our project NutriChat!\n","\n","We'll write the code to:\n","1. Open a PDF document (you could use almost any PDF here).\n","2. Format the text of the PDF textbook ready for an embedding model (this process is known as text splitting/chunking).\n","3. Embed all of the chunks of text in the textbook and turn them into numerical representation which we can store for later.\n","4. Build a retrieval system that uses vector search to find relevant chunks of text based on a query.\n","5. Create a prompt that incorporates the retrieved pieces of text.\n","6. Generate an answer to a query based on passages from the textbook.\n","\n","The above steps can broken down into two major sections:\n","1. Document preprocessing/embedding creation (steps 1-3).\n","2. Search and answer (steps 4-6).\n","\n","And that's the structure we'll follow.\n","\n","It's similar to the workflow outlined on the NVIDIA blog which [details a local RAG pipeline](https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/).\n","\n","<img src=\"https://github.com/mrdbourke/simple-local-rag/blob/main/images/simple-local-rag-workflow-flowchart.png?raw=true\" alt=\"flowchart of a local RAG workflow\" />"]},{"cell_type":"markdown","metadata":{"id":"ABM5EkGZyTG5"},"source":["## Requirements and setup\n","\n","* Local NVIDIA GPU (I used a NVIDIA RTX 4090 on a Windows 11 machine) or Google Colab with access to a GPU.\n","* Environment setup (see [setup details on GitHub](https://github.com/mrdbourke/simple-local-rag/?tab=readme-ov-file#setup)).\n","* Data source (for example, a PDF).\n","* Internet connection (to download the models, but once you have them, it'll run offline)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zf9IwrJ1yTG5","outputId":"7aa964ff-1bba-4255-836b-892ef8a0b175","executionInfo":{"status":"ok","timestamp":1734538991734,"user_tz":-120,"elapsed":41503,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Running in Google Colab, installing requirements.\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Collecting PyMuPDF\n","  Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n","Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyMuPDF\n","Successfully installed PyMuPDF-1.25.1\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.5)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n","Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.26.5)\n","Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n","Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n","Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bitsandbytes\n","Successfully installed bitsandbytes-0.45.0\n","Collecting flash-attn\n","  Downloading flash_attn-2.7.2.post1.tar.gz (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.5.1+cu121)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n","Building wheels for collected packages: flash-attn\n","  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for flash-attn: filename=flash_attn-2.7.2.post1-cp310-cp310-linux_x86_64.whl size=190160474 sha256=0b454d9e650bfc437cc71335080172a5d05f51eab355636c9d5b7321fec7318e\n","  Stored in directory: /root/.cache/pip/wheels/da/ec/5b/b2c37a8e4f755ad82492a822463bca0817f0e0e11de874b550\n","Successfully built flash-attn\n","Installing collected packages: flash-attn\n","Successfully installed flash-attn-2.7.2.post1\n"]}],"source":["# Perform Google Colab installs (if running in Google Colab)\n","import os\n","\n","if \"COLAB_GPU\" in os.environ:\n","    print(\"[INFO] Running in Google Colab, installing requirements.\")\n","    !pip install -U torch # requires torch 2.1.1+ (for efficient sdpa implementation)\n","    !pip install PyMuPDF # for reading PDFs with Python\n","    !pip install tqdm # for progress bars\n","    !pip install sentence-transformers # for embedding models\n","    !pip install accelerate # for quantization model loading\n","    !pip install bitsandbytes # for quantizing models (less storage space)\n","    !pip install flash-attn --no-build-isolation # for faster attention mechanism = faster LLM inference"]},{"cell_type":"markdown","metadata":{"id":"vakTt5gbyTG6"},"source":["## 1. Document/Text Processing and Embedding Creation\n","\n","Ingredients:\n","* PDF document of choice.\n","* Embedding model of choice.\n","\n","Steps:\n","1. Import PDF document.\n","2. Process text for embedding (e.g. split into chunks of sentences).\n","3. Embed text chunks with embedding model.\n","4. Save embeddings to file for later use (embeddings will store on file for many years or until you lose your hard drive)."]},{"cell_type":"markdown","metadata":{"id":"GmSRjBhKyTG6"},"source":["### Import PDF Document\n","\n","This will work with many other kinds of documents.\n","\n","However, we'll start with PDF since many people have PDFs.\n","\n","But just keep in mind, text files, email chains, support documentation, articles and more can also work.\n","\n","We're going to pretend we're nutrition students at the University of Hawai'i, reading through the open-source PDF textbook [*Human Nutrition: 2020 Edition*](https://pressbooks.oer.hawaii.edu/humannutrition2/).\n","\n","There are several libraries to open PDFs with Python but I found that [PyMuPDF](https://github.com/pymupdf/pymupdf) works quite well in many cases.\n","\n","First we'll download the PDF if it doesn't exist."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"8aMClDENyTG6","outputId":"7ddb3122-7a48-4c8a-bb03-b33a6c9dc4dd","executionInfo":{"status":"ok","timestamp":1734539200008,"user_tz":-120,"elapsed":69249,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en-core-web-sm@ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl#sha256=86cc141f63942d4b2c5fcee06630fd6f904788d2f0ab005cce45aadb8fb73889 (from -r rv.txt (line 31))\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting altair==5.4.1 (from -r rv.txt (line 1))\n","  Downloading altair-5.4.1-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: annotated-types==0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 2)) (0.7.0)\n","Collecting anyio==4.6.2.post1 (from -r rv.txt (line 3))\n","  Downloading anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\n","Collecting appnope==0.1.4 (from -r rv.txt (line 4))\n","  Downloading appnope-0.1.4-py2.py3-none-any.whl.metadata (908 bytes)\n","Requirement already satisfied: argon2-cffi==23.1.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 5)) (23.1.0)\n","Requirement already satisfied: argon2-cffi-bindings==21.2.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 6)) (21.2.0)\n","Collecting arrow==1.3.0 (from -r rv.txt (line 7))\n","  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n","Collecting asttokens==2.4.1 (from -r rv.txt (line 8))\n","  Downloading asttokens-2.4.1-py2.py3-none-any.whl.metadata (5.2 kB)\n","Collecting async-lru==2.0.4 (from -r rv.txt (line 9))\n","  Downloading async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n","Requirement already satisfied: attrs==24.2.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 10)) (24.2.0)\n","Requirement already satisfied: babel==2.16.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 11)) (2.16.0)\n","Requirement already satisfied: beautifulsoup4==4.12.3 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 12)) (4.12.3)\n","Requirement already satisfied: bleach==6.2.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 13)) (6.2.0)\n","Requirement already satisfied: blinker==1.9.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 14)) (1.9.0)\n","Collecting blis==0.7.11 (from -r rv.txt (line 15))\n","  Downloading blis-0.7.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n","Requirement already satisfied: cachetools==5.5.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 16)) (5.5.0)\n","Requirement already satisfied: catalogue==2.0.10 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 17)) (2.0.10)\n","Requirement already satisfied: certifi==2024.8.30 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 18)) (2024.8.30)\n","Requirement already satisfied: cffi==1.17.1 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 19)) (1.17.1)\n","Requirement already satisfied: charset-normalizer==3.4.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 20)) (3.4.0)\n","Requirement already satisfied: click==8.1.7 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 21)) (8.1.7)\n","Requirement already satisfied: cloudpathlib==0.20.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 22)) (0.20.0)\n","Collecting comm==0.2.2 (from -r rv.txt (line 23))\n","  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: confection==0.1.5 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 24)) (0.1.5)\n","Collecting contourpy==1.3.0 (from -r rv.txt (line 25))\n","  Downloading contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n","Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 26)) (0.12.1)\n","Collecting cymem==2.0.8 (from -r rv.txt (line 27))\n","  Downloading cymem-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n","Collecting debugpy==1.8.7 (from -r rv.txt (line 28))\n","  Downloading debugpy-1.8.7-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n","Collecting decorator==5.1.1 (from -r rv.txt (line 29))\n","  Downloading decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\n","Requirement already satisfied: defusedxml==0.7.1 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 30)) (0.7.1)\n","Collecting executing==2.1.0 (from -r rv.txt (line 32))\n","  Downloading executing-2.1.0-py2.py3-none-any.whl.metadata (8.9 kB)\n","Collecting fastjsonschema==2.20.0 (from -r rv.txt (line 33))\n","  Downloading fastjsonschema-2.20.0-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: filelock==3.16.1 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 34)) (3.16.1)\n","Collecting fonttools==4.54.1 (from -r rv.txt (line 35))\n","  Downloading fonttools-4.54.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (163 kB)\n","Collecting fqdn==1.5.1 (from -r rv.txt (line 36))\n","  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: fsspec==2024.10.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 37)) (2024.10.0)\n","Requirement already satisfied: gitdb==4.0.11 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 38)) (4.0.11)\n","Requirement already satisfied: GitPython==3.1.43 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 39)) (3.1.43)\n","Requirement already satisfied: h11==0.14.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 40)) (0.14.0)\n","Collecting httpcore==1.0.6 (from -r rv.txt (line 41))\n","  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n","Collecting httpx==0.27.2 (from -r rv.txt (line 42))\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Collecting huggingface-hub==0.26.2 (from -r rv.txt (line 43))\n","  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: idna==3.10 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 44)) (3.10)\n","Collecting ipykernel==6.29.5 (from -r rv.txt (line 45))\n","  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n","Collecting ipython==8.29.0 (from -r rv.txt (line 46))\n","  Downloading ipython-8.29.0-py3-none-any.whl.metadata (5.0 kB)\n","Collecting ipywidgets==8.1.5 (from -r rv.txt (line 47))\n","  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n","Collecting isoduration==20.11.0 (from -r rv.txt (line 48))\n","  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n","Collecting jedi==0.19.1 (from -r rv.txt (line 49))\n","  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: Jinja2==3.1.4 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 50)) (3.1.4)\n","Requirement already satisfied: joblib==1.4.2 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 51)) (1.4.2)\n","Collecting json5==0.9.25 (from -r rv.txt (line 52))\n","  Downloading json5-0.9.25-py3-none-any.whl.metadata (30 kB)\n","Requirement already satisfied: jsonpointer==3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 53)) (3.0.0)\n","Requirement already satisfied: jsonschema==4.23.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 54)) (4.23.0)\n","Requirement already satisfied: jsonschema-specifications==2024.10.1 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 55)) (2024.10.1)\n","Collecting jupyter==1.1.1 (from -r rv.txt (line 56))\n","  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n","Collecting jupyter-console==6.6.3 (from -r rv.txt (line 57))\n","  Downloading jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)\n","Collecting jupyter-events==0.10.0 (from -r rv.txt (line 58))\n","  Downloading jupyter_events-0.10.0-py3-none-any.whl.metadata (5.9 kB)\n","Collecting jupyter-lsp==2.2.5 (from -r rv.txt (line 59))\n","  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n","Collecting jupyter_client==8.6.3 (from -r rv.txt (line 60))\n","  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n","Requirement already satisfied: jupyter_core==5.7.2 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 61)) (5.7.2)\n","Collecting jupyter_server==2.14.2 (from -r rv.txt (line 62))\n","  Downloading jupyter_server-2.14.2-py3-none-any.whl.metadata (8.4 kB)\n","Collecting jupyter_server_terminals==0.5.3 (from -r rv.txt (line 63))\n","  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n","Collecting jupyterlab==4.2.5 (from -r rv.txt (line 64))\n","  Downloading jupyterlab-4.2.5-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: jupyterlab_pygments==0.3.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 65)) (0.3.0)\n","Collecting jupyterlab_server==2.27.3 (from -r rv.txt (line 66))\n","  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n","Requirement already satisfied: jupyterlab_widgets==3.0.13 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 67)) (3.0.13)\n","Requirement already satisfied: kiwisolver==1.4.7 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 68)) (1.4.7)\n","Collecting langcodes==3.4.1 (from -r rv.txt (line 69))\n","  Downloading langcodes-3.4.1-py3-none-any.whl.metadata (29 kB)\n","Collecting language_data==1.2.0 (from -r rv.txt (line 70))\n","  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n","Requirement already satisfied: marisa-trie==1.2.1 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 71)) (1.2.1)\n","Requirement already satisfied: markdown-it-py==3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 72)) (3.0.0)\n","Requirement already satisfied: MarkupSafe==3.0.2 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 73)) (3.0.2)\n","Collecting matplotlib==3.8.3 (from -r rv.txt (line 74))\n","  Downloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n","Requirement already satisfied: matplotlib-inline==0.1.7 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 75)) (0.1.7)\n","Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 76)) (0.1.2)\n","Requirement already satisfied: mistune==3.0.2 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 77)) (3.0.2)\n","Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 78)) (1.3.0)\n","Collecting murmurhash==1.0.10 (from -r rv.txt (line 79))\n","  Downloading murmurhash-1.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n","Collecting narwhals==1.14.2 (from -r rv.txt (line 80))\n","  Downloading narwhals-1.14.2-py3-none-any.whl.metadata (7.5 kB)\n","Collecting nbclient==0.10.0 (from -r rv.txt (line 81))\n","  Downloading nbclient-0.10.0-py3-none-any.whl.metadata (7.8 kB)\n","Requirement already satisfied: nbconvert==7.16.4 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 82)) (7.16.4)\n","Requirement already satisfied: nbformat==5.10.4 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 83)) (5.10.4)\n","Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 84)) (1.6.0)\n","Requirement already satisfied: networkx==3.4.2 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 85)) (3.4.2)\n","Collecting notebook==7.2.2 (from -r rv.txt (line 86))\n","  Downloading notebook-7.2.2-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: notebook_shim==0.2.4 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 87)) (0.2.4)\n","Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 88)) (1.26.4)\n","Collecting overrides==7.7.0 (from -r rv.txt (line 89))\n","  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n","Collecting packaging==24.1 (from -r rv.txt (line 90))\n","  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n","Collecting pandas==2.2.1 (from -r rv.txt (line 91))\n","  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n","Requirement already satisfied: pandocfilters==1.5.1 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 92)) (1.5.1)\n","Requirement already satisfied: parso==0.8.4 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 93)) (0.8.4)\n","Requirement already satisfied: pexpect==4.9.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 94)) (4.9.0)\n","Requirement already satisfied: pillow==11.0.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 95)) (11.0.0)\n","Requirement already satisfied: platformdirs==4.3.6 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 96)) (4.3.6)\n","Requirement already satisfied: preshed==3.0.9 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 97)) (3.0.9)\n","Collecting prometheus_client==0.21.0 (from -r rv.txt (line 98))\n","  Downloading prometheus_client-0.21.0-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: prompt_toolkit==3.0.48 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 99)) (3.0.48)\n","Collecting protobuf==5.28.3 (from -r rv.txt (line 100))\n","  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n","Collecting psutil==6.1.0 (from -r rv.txt (line 101))\n","  Downloading psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n","Requirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 102)) (0.7.0)\n","Collecting pure_eval==0.2.3 (from -r rv.txt (line 103))\n","  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n","Collecting pyarrow==18.0.0 (from -r rv.txt (line 104))\n","  Downloading pyarrow-18.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n","Requirement already satisfied: pycparser==2.22 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 105)) (2.22)\n","Collecting pydantic==2.9.2 (from -r rv.txt (line 106))\n","  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n","Collecting pydantic_core==2.23.4 (from -r rv.txt (line 107))\n","  Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Collecting pydeck==0.9.1 (from -r rv.txt (line 108))\n","  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: Pygments==2.18.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 109)) (2.18.0)\n","Collecting PyMuPDF==1.23.26 (from -r rv.txt (line 110))\n","  Downloading PyMuPDF-1.23.26-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n","Collecting PyMuPDFb==1.23.22 (from -r rv.txt (line 111))\n","  Downloading PyMuPDFb-1.23.22-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n","Requirement already satisfied: pyparsing==3.2.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 112)) (3.2.0)\n","Collecting python-dateutil==2.9.0.post0 (from -r rv.txt (line 113))\n","  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n","Collecting python-json-logger==2.0.7 (from -r rv.txt (line 114))\n","  Downloading python_json_logger-2.0.7-py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: pytz==2024.2 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 115)) (2024.2)\n","Requirement already satisfied: PyYAML==6.0.2 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 116)) (6.0.2)\n","Collecting pyzmq==26.2.0 (from -r rv.txt (line 117))\n","  Downloading pyzmq-26.2.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.2 kB)\n","Requirement already satisfied: referencing==0.35.1 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 118)) (0.35.1)\n","Requirement already satisfied: regex==2024.9.11 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 119)) (2024.9.11)\n","Collecting requests==2.31.0 (from -r rv.txt (line 120))\n","  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n","Collecting rfc3339-validator==0.1.4 (from -r rv.txt (line 121))\n","  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n","Collecting rfc3986-validator==0.1.1 (from -r rv.txt (line 122))\n","  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: rich==13.9.4 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 123)) (13.9.4)\n","Collecting rpds-py==0.20.1 (from -r rv.txt (line 124))\n","  Downloading rpds_py-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n","Requirement already satisfied: safetensors==0.4.5 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 125)) (0.4.5)\n","Requirement already satisfied: scikit-learn==1.5.2 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 126)) (1.5.2)\n","Collecting scipy==1.14.1 (from -r rv.txt (line 127))\n","  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","Requirement already satisfied: Send2Trash==1.8.3 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 128)) (1.8.3)\n","Collecting sentence-transformers==2.5.1 (from -r rv.txt (line 129))\n","  Downloading sentence_transformers-2.5.1-py3-none-any.whl.metadata (11 kB)\n","Collecting setuptools==75.3.0 (from -r rv.txt (line 130))\n","  Downloading setuptools-75.3.0-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: shellingham==1.5.4 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 131)) (1.5.4)\n","Collecting six==1.16.0 (from -r rv.txt (line 132))\n","  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: smart-open==7.0.5 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 133)) (7.0.5)\n","Requirement already satisfied: smmap==5.0.1 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 134)) (5.0.1)\n","Requirement already satisfied: sniffio==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 135)) (1.3.1)\n","Requirement already satisfied: soupsieve==2.6 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 136)) (2.6)\n","Collecting spacy==3.7.5 (from -r rv.txt (line 137))\n","  Downloading spacy-3.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n","Requirement already satisfied: spacy-legacy==3.0.12 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 138)) (3.0.12)\n","Requirement already satisfied: spacy-loggers==1.0.5 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 139)) (1.0.5)\n","Collecting srsly==2.4.8 (from -r rv.txt (line 140))\n","  Downloading srsly-2.4.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Collecting stack-data==0.6.3 (from -r rv.txt (line 141))\n","  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n","Collecting stqdm==0.0.5 (from -r rv.txt (line 142))\n","  Downloading stqdm-0.0.5-py3-none-any.whl.metadata (3.0 kB)\n","Collecting streamlit==1.40.1 (from -r rv.txt (line 143))\n","  Downloading streamlit-1.40.1-py2.py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 144)) (1.13.1)\n","Requirement already satisfied: tenacity==9.0.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 145)) (9.0.0)\n","Requirement already satisfied: terminado==0.18.1 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 146)) (0.18.1)\n","Collecting thinc==8.2.5 (from -r rv.txt (line 147))\n","  Downloading thinc-8.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n","Requirement already satisfied: threadpoolctl==3.5.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 148)) (3.5.0)\n","Requirement already satisfied: tinycss2==1.4.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 149)) (1.4.0)\n","Collecting tokenizers==0.15.2 (from -r rv.txt (line 150))\n","  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: toml==0.10.2 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 151)) (0.10.2)\n","Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 152)) (2.5.1+cu121)\n","Collecting tornado==6.4.1 (from -r rv.txt (line 153))\n","  Downloading tornado-6.4.1-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n","Collecting tqdm==4.66.2 (from -r rv.txt (line 154))\n","  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n","Collecting traitlets==5.14.3 (from -r rv.txt (line 155))\n","  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n","Collecting transformers==4.38.2 (from -r rv.txt (line 156))\n","  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n","Collecting typer==0.12.5 (from -r rv.txt (line 157))\n","  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n","Collecting types-python-dateutil==2.9.0.20241003 (from -r rv.txt (line 158))\n","  Downloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: typing_extensions==4.12.2 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 159)) (4.12.2)\n","Requirement already satisfied: tzdata==2024.2 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 160)) (2024.2)\n","Collecting uri-template==1.3.0 (from -r rv.txt (line 161))\n","  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n","Requirement already satisfied: urllib3==2.2.3 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 162)) (2.2.3)\n","Requirement already satisfied: wasabi==1.1.3 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 163)) (1.1.3)\n","Requirement already satisfied: wcwidth==0.2.13 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 164)) (0.2.13)\n","Requirement already satisfied: weasel==0.4.1 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 165)) (0.4.1)\n","Collecting webcolors==24.8.0 (from -r rv.txt (line 166))\n","  Downloading webcolors-24.8.0-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: webencodings==0.5.1 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 167)) (0.5.1)\n","Requirement already satisfied: websocket-client==1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r rv.txt (line 168)) (1.8.0)\n","Collecting widgetsnbextension==4.0.13 (from -r rv.txt (line 169))\n","  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n","Collecting wrapt==1.16.0 (from -r rv.txt (line 170))\n","  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio==4.6.2.post1->-r rv.txt (line 3)) (1.2.2)\n","Requirement already satisfied: tomli>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from jupyterlab==4.2.5->-r rv.txt (line 64)) (2.2.1)\n","Collecting watchdog<7,>=2.1.5 (from streamlit==1.40.1->-r rv.txt (line 143))\n","  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n","Downloading altair-5.4.1-py3-none-any.whl (658 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m658.1/658.1 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n","Downloading appnope-0.1.4-py2.py3-none-any.whl (4.3 kB)\n","Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n","Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n","Downloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n","Downloading blis-0.7.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m127.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n","Downloading contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n","Downloading cymem-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n","Downloading debugpy-1.8.7-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m129.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n","Downloading executing-2.1.0-py2.py3-none-any.whl (25 kB)\n","Downloading fastjsonschema-2.20.0-py3-none-any.whl (23 kB)\n","Downloading fonttools-4.54.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m130.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n","Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n","Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n","Downloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n","Downloading ipython-8.29.0-py3-none-any.whl (819 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.9/819.9 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n","Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n","Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading json5-0.9.25-py3-none-any.whl (30 kB)\n","Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n","Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n","Downloading jupyter_events-0.10.0-py3-none-any.whl (18 kB)\n","Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n","Downloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n","Downloading jupyter_server-2.14.2-py3-none-any.whl (383 kB)\n","Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n","Downloading jupyterlab-4.2.5-py3-none-any.whl (11.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m131.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n","Downloading langcodes-3.4.1-py3-none-any.whl (182 kB)\n","Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading murmurhash-1.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n","Downloading narwhals-1.14.2-py3-none-any.whl (225 kB)\n","Downloading nbclient-0.10.0-py3-none-any.whl (25 kB)\n","Downloading notebook-7.2.2-py3-none-any.whl (5.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m147.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n","Downloading packaging-24.1-py3-none-any.whl (53 kB)\n","Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m157.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading prometheus_client-0.21.0-py3-none-any.whl (54 kB)\n","Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n","Downloading psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n","Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n","Downloading pyarrow-18.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m137.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n","Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m152.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PyMuPDF-1.23.26-cp310-none-manylinux2014_x86_64.whl (4.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PyMuPDFb-1.23.22-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m152.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n","Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n","Downloading pyzmq-26.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (868 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n","Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n","Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n","Downloading rpds_py-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (360 kB)\n","Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m163.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sentence_transformers-2.5.1-py3-none-any.whl (156 kB)\n","Downloading setuptools-75.3.0-py3-none-any.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n","Downloading spacy-3.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m154.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading srsly-2.4.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (493 kB)\n","Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n","Downloading stqdm-0.0.5-py3-none-any.whl (11 kB)\n","Downloading streamlit-1.40.1-py2.py3-none-any.whl (8.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading thinc-8.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (922 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.4/922.4 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tornado-6.4.1-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n","Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n","Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n","Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typer-0.12.5-py3-none-any.whl (47 kB)\n","Downloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl (9.7 kB)\n","Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n","Downloading webcolors-24.8.0-py3-none-any.whl (15 kB)\n","Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n","Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n","Installing collected packages: pure_eval, fastjsonschema, cymem, wrapt, widgetsnbextension, webcolors, watchdog, uri-template, types-python-dateutil, traitlets, tqdm, tornado, srsly, six, setuptools, scipy, rpds-py, rfc3986-validator, requests, pyzmq, python-json-logger, PyMuPDFb, pydantic_core, pyarrow, psutil, protobuf, prometheus_client, packaging, overrides, narwhals, murmurhash, json5, jedi, httpcore, fqdn, fonttools, executing, decorator, debugpy, contourpy, blis, async-lru, appnope, anyio, rfc3339-validator, python-dateutil, PyMuPDF, pydeck, pydantic, huggingface-hub, httpx, comm, asttokens, typer, tokenizers, stack-data, pandas, matplotlib, language_data, jupyter_server_terminals, jupyter_client, arrow, transformers, thinc, langcodes, isoduration, ipython, spacy, sentence-transformers, ipywidgets, ipykernel, altair, streamlit, nbclient, jupyter-events, jupyter-console, stqdm, jupyter_server, jupyterlab_server, jupyter-lsp, jupyterlab, notebook, jupyter\n","  Attempting uninstall: fastjsonschema\n","    Found existing installation: fastjsonschema 2.21.1\n","    Uninstalling fastjsonschema-2.21.1:\n","      Successfully uninstalled fastjsonschema-2.21.1\n","  Attempting uninstall: cymem\n","    Found existing installation: cymem 2.0.10\n","    Uninstalling cymem-2.0.10:\n","      Successfully uninstalled cymem-2.0.10\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.17.0\n","    Uninstalling wrapt-1.17.0:\n","      Successfully uninstalled wrapt-1.17.0\n","  Attempting uninstall: widgetsnbextension\n","    Found existing installation: widgetsnbextension 3.6.10\n","    Uninstalling widgetsnbextension-3.6.10:\n","      Successfully uninstalled widgetsnbextension-3.6.10\n","  Attempting uninstall: webcolors\n","    Found existing installation: webcolors 24.11.1\n","    Uninstalling webcolors-24.11.1:\n","      Successfully uninstalled webcolors-24.11.1\n","  Attempting uninstall: traitlets\n","    Found existing installation: traitlets 5.7.1\n","    Uninstalling traitlets-5.7.1:\n","      Successfully uninstalled traitlets-5.7.1\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.66.6\n","    Uninstalling tqdm-4.66.6:\n","      Successfully uninstalled tqdm-4.66.6\n","  Attempting uninstall: tornado\n","    Found existing installation: tornado 6.3.3\n","    Uninstalling tornado-6.3.3:\n","      Successfully uninstalled tornado-6.3.3\n","  Attempting uninstall: srsly\n","    Found existing installation: srsly 2.5.0\n","    Uninstalling srsly-2.5.0:\n","      Successfully uninstalled srsly-2.5.0\n","  Attempting uninstall: six\n","    Found existing installation: six 1.17.0\n","    Uninstalling six-1.17.0:\n","      Successfully uninstalled six-1.17.0\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 75.6.0\n","    Uninstalling setuptools-75.6.0:\n","      Successfully uninstalled setuptools-75.6.0\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.13.1\n","    Uninstalling scipy-1.13.1:\n","      Successfully uninstalled scipy-1.13.1\n","  Attempting uninstall: rpds-py\n","    Found existing installation: rpds-py 0.22.3\n","    Uninstalling rpds-py-0.22.3:\n","      Successfully uninstalled rpds-py-0.22.3\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.32.3\n","    Uninstalling requests-2.32.3:\n","      Successfully uninstalled requests-2.32.3\n","  Attempting uninstall: pyzmq\n","    Found existing installation: pyzmq 24.0.1\n","    Uninstalling pyzmq-24.0.1:\n","      Successfully uninstalled pyzmq-24.0.1\n","  Attempting uninstall: pydantic_core\n","    Found existing installation: pydantic_core 2.27.1\n","    Uninstalling pydantic_core-2.27.1:\n","      Successfully uninstalled pydantic_core-2.27.1\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 17.0.0\n","    Uninstalling pyarrow-17.0.0:\n","      Successfully uninstalled pyarrow-17.0.0\n","  Attempting uninstall: psutil\n","    Found existing installation: psutil 5.9.5\n","    Uninstalling psutil-5.9.5:\n","      Successfully uninstalled psutil-5.9.5\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 4.25.5\n","    Uninstalling protobuf-4.25.5:\n","      Successfully uninstalled protobuf-4.25.5\n","  Attempting uninstall: prometheus_client\n","    Found existing installation: prometheus_client 0.21.1\n","    Uninstalling prometheus_client-0.21.1:\n","      Successfully uninstalled prometheus_client-0.21.1\n","  Attempting uninstall: packaging\n","    Found existing installation: packaging 24.2\n","    Uninstalling packaging-24.2:\n","      Successfully uninstalled packaging-24.2\n","  Attempting uninstall: narwhals\n","    Found existing installation: narwhals 1.18.2\n","    Uninstalling narwhals-1.18.2:\n","      Successfully uninstalled narwhals-1.18.2\n","  Attempting uninstall: murmurhash\n","    Found existing installation: murmurhash 1.0.11\n","    Uninstalling murmurhash-1.0.11:\n","      Successfully uninstalled murmurhash-1.0.11\n","  Attempting uninstall: httpcore\n","    Found existing installation: httpcore 1.0.7\n","    Uninstalling httpcore-1.0.7:\n","      Successfully uninstalled httpcore-1.0.7\n","  Attempting uninstall: fonttools\n","    Found existing installation: fonttools 4.55.3\n","    Uninstalling fonttools-4.55.3:\n","      Successfully uninstalled fonttools-4.55.3\n","  Attempting uninstall: decorator\n","    Found existing installation: decorator 4.4.2\n","    Uninstalling decorator-4.4.2:\n","      Successfully uninstalled decorator-4.4.2\n","  Attempting uninstall: debugpy\n","    Found existing installation: debugpy 1.8.0\n","    Uninstalling debugpy-1.8.0:\n","      Successfully uninstalled debugpy-1.8.0\n","  Attempting uninstall: contourpy\n","    Found existing installation: contourpy 1.3.1\n","    Uninstalling contourpy-1.3.1:\n","      Successfully uninstalled contourpy-1.3.1\n","  Attempting uninstall: blis\n","    Found existing installation: blis 1.1.0\n","    Uninstalling blis-1.1.0:\n","      Successfully uninstalled blis-1.1.0\n","  Attempting uninstall: anyio\n","    Found existing installation: anyio 3.7.1\n","    Uninstalling anyio-3.7.1:\n","      Successfully uninstalled anyio-3.7.1\n","  Attempting uninstall: python-dateutil\n","    Found existing installation: python-dateutil 2.8.2\n","    Uninstalling python-dateutil-2.8.2:\n","      Successfully uninstalled python-dateutil-2.8.2\n","  Attempting uninstall: PyMuPDF\n","    Found existing installation: PyMuPDF 1.25.1\n","    Uninstalling PyMuPDF-1.25.1:\n","      Successfully uninstalled PyMuPDF-1.25.1\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 2.10.3\n","    Uninstalling pydantic-2.10.3:\n","      Successfully uninstalled pydantic-2.10.3\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.26.5\n","    Uninstalling huggingface-hub-0.26.5:\n","      Successfully uninstalled huggingface-hub-0.26.5\n","  Attempting uninstall: httpx\n","    Found existing installation: httpx 0.28.1\n","    Uninstalling httpx-0.28.1:\n","      Successfully uninstalled httpx-0.28.1\n","  Attempting uninstall: typer\n","    Found existing installation: typer 0.15.1\n","    Uninstalling typer-0.15.1:\n","      Successfully uninstalled typer-0.15.1\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.20.3\n","    Uninstalling tokenizers-0.20.3:\n","      Successfully uninstalled tokenizers-0.20.3\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 2.2.2\n","    Uninstalling pandas-2.2.2:\n","      Successfully uninstalled pandas-2.2.2\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.8.0\n","    Uninstalling matplotlib-3.8.0:\n","      Successfully uninstalled matplotlib-3.8.0\n","  Attempting uninstall: language_data\n","    Found existing installation: language_data 1.3.0\n","    Uninstalling language_data-1.3.0:\n","      Successfully uninstalled language_data-1.3.0\n","  Attempting uninstall: jupyter_client\n","    Found existing installation: jupyter-client 6.1.12\n","    Uninstalling jupyter-client-6.1.12:\n","      Successfully uninstalled jupyter-client-6.1.12\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.46.3\n","    Uninstalling transformers-4.46.3:\n","      Successfully uninstalled transformers-4.46.3\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 8.3.3\n","    Uninstalling thinc-8.3.3:\n","      Successfully uninstalled thinc-8.3.3\n","  Attempting uninstall: langcodes\n","    Found existing installation: langcodes 3.5.0\n","    Uninstalling langcodes-3.5.0:\n","      Successfully uninstalled langcodes-3.5.0\n","  Attempting uninstall: ipython\n","    Found existing installation: ipython 7.34.0\n","    Uninstalling ipython-7.34.0:\n","      Successfully uninstalled ipython-7.34.0\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 3.8.3\n","    Uninstalling spacy-3.8.3:\n","      Successfully uninstalled spacy-3.8.3\n","  Attempting uninstall: sentence-transformers\n","    Found existing installation: sentence-transformers 3.2.1\n","    Uninstalling sentence-transformers-3.2.1:\n","      Successfully uninstalled sentence-transformers-3.2.1\n","  Attempting uninstall: ipywidgets\n","    Found existing installation: ipywidgets 7.7.1\n","    Uninstalling ipywidgets-7.7.1:\n","      Successfully uninstalled ipywidgets-7.7.1\n","  Attempting uninstall: ipykernel\n","    Found existing installation: ipykernel 5.5.6\n","    Uninstalling ipykernel-5.5.6:\n","      Successfully uninstalled ipykernel-5.5.6\n","  Attempting uninstall: altair\n","    Found existing installation: altair 5.5.0\n","    Uninstalling altair-5.5.0:\n","      Successfully uninstalled altair-5.5.0\n","  Attempting uninstall: nbclient\n","    Found existing installation: nbclient 0.10.1\n","    Uninstalling nbclient-0.10.1:\n","      Successfully uninstalled nbclient-0.10.1\n","  Attempting uninstall: jupyter-console\n","    Found existing installation: jupyter-console 6.1.0\n","    Uninstalling jupyter-console-6.1.0:\n","      Successfully uninstalled jupyter-console-6.1.0\n","  Attempting uninstall: jupyter_server\n","    Found existing installation: jupyter-server 1.24.0\n","    Uninstalling jupyter-server-1.24.0:\n","      Successfully uninstalled jupyter-server-1.24.0\n","  Attempting uninstall: notebook\n","    Found existing installation: notebook 6.5.5\n","    Uninstalling notebook-6.5.5:\n","      Successfully uninstalled notebook-6.5.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.10.1 requires pyarrow<18.0.0a0,>=14.0.0, but you have pyarrow 18.0.0 which is incompatible.\n","gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\n","google-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.29.5 which is incompatible.\n","google-colab 1.0.0 requires ipython==7.34.0, but you have ipython 8.29.0 which is incompatible.\n","google-colab 1.0.0 requires notebook==6.5.5, but you have notebook 7.2.2 which is incompatible.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.1 which is incompatible.\n","google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\n","google-colab 1.0.0 requires tornado==6.3.3, but you have tornado 6.4.1 which is incompatible.\n","moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n","pylibcudf-cu12 24.10.1 requires pyarrow<18.0.0a0,>=14.0.0, but you have pyarrow 18.0.0 which is incompatible.\n","tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\n","tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed PyMuPDF-1.23.26 PyMuPDFb-1.23.22 altair-5.4.1 anyio-4.6.2.post1 appnope-0.1.4 arrow-1.3.0 asttokens-2.4.1 async-lru-2.0.4 blis-0.7.11 comm-0.2.2 contourpy-1.3.0 cymem-2.0.8 debugpy-1.8.7 decorator-5.1.1 executing-2.1.0 fastjsonschema-2.20.0 fonttools-4.54.1 fqdn-1.5.1 httpcore-1.0.6 httpx-0.27.2 huggingface-hub-0.26.2 ipykernel-6.29.5 ipython-8.29.0 ipywidgets-8.1.5 isoduration-20.11.0 jedi-0.19.1 json5-0.9.25 jupyter-1.1.1 jupyter-console-6.6.3 jupyter-events-0.10.0 jupyter-lsp-2.2.5 jupyter_client-8.6.3 jupyter_server-2.14.2 jupyter_server_terminals-0.5.3 jupyterlab-4.2.5 jupyterlab_server-2.27.3 langcodes-3.4.1 language_data-1.2.0 matplotlib-3.8.3 murmurhash-1.0.10 narwhals-1.14.2 nbclient-0.10.0 notebook-7.2.2 overrides-7.7.0 packaging-24.1 pandas-2.2.1 prometheus_client-0.21.0 protobuf-5.28.3 psutil-6.1.0 pure_eval-0.2.3 pyarrow-18.0.0 pydantic-2.9.2 pydantic_core-2.23.4 pydeck-0.9.1 python-dateutil-2.9.0.post0 python-json-logger-2.0.7 pyzmq-26.2.0 requests-2.31.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rpds-py-0.20.1 scipy-1.14.1 sentence-transformers-2.5.1 setuptools-75.3.0 six-1.16.0 spacy-3.7.5 srsly-2.4.8 stack-data-0.6.3 stqdm-0.0.5 streamlit-1.40.1 thinc-8.2.5 tokenizers-0.15.2 tornado-6.4.1 tqdm-4.66.2 traitlets-5.14.3 transformers-4.38.2 typer-0.12.5 types-python-dateutil-2.9.0.20241003 uri-template-1.3.0 watchdog-6.0.0 webcolors-24.8.0 widgetsnbextension-4.0.13 wrapt-1.16.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["IPython","_distutils_hack","blis","cymem","dateutil","debugpy","decorator","huggingface_hub","ipywidgets","langcodes","matplotlib","mpl_toolkits","murmurhash","psutil","pyarrow","requests","setuptools","six","spacy","srsly","thinc","tornado","tqdm","transformers"]},"id":"4468ab80fcf045c59b9fb7d0984c76d0"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["File Liivi exists.\n"]}],"source":["!pip install -r rv.txt\n","# Download PDF file\n","import os\n","import requests\n","\n","# Step 1: Download the PDF\n","#!wget https://www.hel.fi/static/liitteet/kaupunkiymparisto/julkaisut/julkaisut/suunnittelu-ja-kaavoituskatsaus.pdf\n","\n","# Step 2: Install pdfminer.six\n","#!pip install pdfminer.six\n","\n","# Step 3: Extract text using pdfminer.six\n","#from pdfminer.high_level import extract_text\n","\n","#pdf_path = 'suunnittelu-ja-kaavoituskatsaus.pdf'\n","#text = extract_text(pdf_path)\n","\n","#print(text)\n","\n","\n","\n","# Get PDF document\n","#pdf_path = \"J0604_Suomalaiset ensimmaisessa_net.pdf\"\n","pdf_path = \"Liivi\"\n","\n","\n","# Download PDF if it doesn't already exist\n","if not os.path.exists(pdf_path):\n","  print(\"File doesn't exist, downloading...\")\n","  url = \"https://www.doria.fi/bitstream/handle/10024/167604/HT001_opt.pdf?sequence=1\"\n","\n","  # The URL of the PDF you want to download\n","  #url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n","  #url = \"https://julkaisut.valtioneuvosto.fi/bitstream/handle/10024/161110/J0604_Suomalaiset%20ensimmaisessa_net.pdf\"\n","\n","  # The local filename to save the downloaded file\n","  filename = pdf_path\n","\n","  # Send a GET request to the URL\n","  response = requests.get(url)\n","\n","  # Check if the request was successful\n","  if response.status_code == 200:\n","      # Open a file in binary write mode and save the content to it\n","      with open(filename, \"wb\") as file:\n","          file.write(response.content)\n","      print(f\"The file has been downloaded and saved as {filename}\")\n","  else:\n","      print(f\"Failed to download the file. Status code: {response.status_code}\")\n","else:\n","  print(f\"File {pdf_path} exists.\")"]},{"cell_type":"markdown","metadata":{"id":"456ajhhgyTG7"},"source":["PDF acquired!\n","\n","We can import the pages of our PDF to text by first defining the PDF path and then opening and reading it with PyMuPDF (`import fitz`).\n","\n","We'll write a small helper function to preprocess the text as it gets read. Note that not all text will be read in the same so keep this in mind for when you prepare your text.\n","\n","We'll save each page to a dictionary and then append that dictionary to a list for ease of use later."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":267,"referenced_widgets":["269a2daf7dca430cbf7b5f50e109a44c","d97d0e3aee5240b1922142bb1e490e96","9e38047c1cf34a6ea4a5de494ad8aa73","08ad642053a24789829c8be4cc57c535","9457b8fb15b640c1ba58b57ccedf962e","afa35a3fcda041a1a459fd627cfd15ae","871d5bd9f2434977bdcc8106761da7f4","4b51ad28368a47c8a2164d87afac42c3","8caf4a37bd0a4f0a9b7051baca00c229","3410af54b4a64ae7b0fdc53dfef22924","575650e705744edab2f5f1c93194014b"]},"id":"LntumSDpyTG7","outputId":"e2c08eaa-d8d2-4f83-c437-d8ba720a436d","executionInfo":{"status":"ok","timestamp":1734539002447,"user_tz":-120,"elapsed":1110,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"269a2daf7dca430cbf7b5f50e109a44c"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["[{'page_number': -41,\n","  'page_char_count': 0,\n","  'page_word_count': 1,\n","  'page_sentence_count_raw': 1,\n","  'page_token_count': 0.0,\n","  'text': ''},\n"," {'page_number': -40,\n","  'page_char_count': 131,\n","  'page_word_count': 12,\n","  'page_sentence_count_raw': 1,\n","  'page_token_count': 32.75,\n","  'text': 'HISTORIALLISIA TUTKIMCIKSIA JULKAISSUT SUOMEN HISTORIALLINEN SECiRR I WERNER TRWASTST)ERNR POHJOISMAIDEN VIISIKOLMATTAVUOTINEN SOTA'}]"]},"metadata":{},"execution_count":8}],"source":["# Requires !pip install PyMuPDF, see: https://github.com/pymupdf/pymupdf\n","import fitz # (pymupdf, found this is better than pypdf for our use case, note: licence is AGPL-3.0, keep that in mind if you want to use any code commercially)\n","from tqdm.auto import tqdm # for progress bars, requires !pip install tqdm\n","\n","def text_formatter(text: str) -> str:\n","  cleaned_text = text.replace(\"\\n\", \"\").strip()\n","  cleaned_text = cleaned_text.replace(\"- \", \"\").strip()\n","  cleaned_text = cleaned_text.replace(\"-\", \"\").strip()\n","  cleaned_text = cleaned_text.replace(\"\\t\", \"\").strip()\n","\n","\n","  return cleaned_text\n","\n","\n","\n","\n","\n","\n","# Open PDF and get lines/pages\n","# Note: this only focuses on text, rather than images/figures etc\n","def open_and_read_pdf(pdf_path: str) -> list[dict]:\n","    \"\"\"\n","    Opens a PDF file, reads its text content page by page, and collects statistics.\n","\n","    Parameters:\n","        pdf_path (str): The file path to the PDF document to be opened and read.\n","\n","    Returns:\n","        list[dict]: A list of dictionaries, each containing the page number\n","        (adjusted), character count, word count, sentence count, token count, and the extracted text\n","        for each page.\n","    \"\"\"\n","    doc = fitz.open(pdf_path)  # open a document\n","    pages_and_texts = []\n","    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages\n","        text = page.get_text()  # get plain text encoded as UTF-8\n","        text = text_formatter(text)\n","        pages_and_texts.append({\"page_number\": page_number - 41,  # adjust page numbers since our PDF starts on page 42\n","                                \"page_char_count\": len(text),\n","                                \"page_word_count\": len(text.split(\" \")),\n","                                \"page_sentence_count_raw\": len(text.split(\". \")),\n","                                \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n","                                \"text\": text})\n","    return pages_and_texts\n","\n","pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n","pages_and_texts[:2]"]},{"cell_type":"markdown","metadata":{"id":"eM-v93vtyTG7"},"source":["Now let's get a random sample of the pages."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pdj8i5XXyTG8","outputId":"ef132552-8d3b-45dd-9ae5-7142e7439377","executionInfo":{"status":"ok","timestamp":1734539002447,"user_tz":-120,"elapsed":21,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'page_number': 15,\n","  'page_char_count': 1454,\n","  'page_word_count': 188,\n","  'page_sentence_count_raw': 19,\n","  'page_token_count': 363.5,\n","  'text': '35 Se vain tiedetään, että Viron puolella ei saatu välirauhan toimeen, vaikka aselevosta vuoden lopussa sovittiin Suomen rajalla, kuten tuonnempana saamme nähdä. Ruokavarojen puute, jota sotajoukko kärsi, näyttää estäneen Klaus Aakenpojan toimintaa, koska hän ei käyttänyt voiton hänelle tarjoaman tilaisuutta Ruotsin vallan levittämiseksi. Edellisenä syksynä oli Tallinnaan tuotu liian vähän nuonaa. Kun sitten Suomenlahti kylmän talven ja myöhäisen kevääntulon johdosta oli helluntaihin saakka kovassa jäässä, ei vielä toukokuussa saatu toimitetuksi Ruotsista apua 1. Tässä pulassa hallitus muun Muassa pyysi Raaseporin Ebba (Leijonhufvud) rouvalta viljaa sekä muita ruokavaroja lainaksi. Samanlainen pyyntö tehtiin leskikuningattarelle, Katariinalle, joka silloin näyttää olleen Kastelholmassa Ahvenanmaalla 2. Kesällä (1573) Ruotsin sotajoukko vihdoin pelastui nälästä siten, että isoin osa siitä lähetettiin Virosta Suomeen linnaleirille. 1  Samassa kirjeessä, jossa Juhani III antoi valtakunnan piispoill määräyksen, että kirkoissa oli tehtävä kiitokset Klaus Aakenpojan voitoe johdosta, käski hän heidän kehoittaa kansaa rukoilemaan, ettei pitkällinen pakkanen vahingoittaisi karjaa eikä viljaa. V. R. Kirje kaikille piispoille huhtikuun 15 p. 1573. 2  V. R. Kirje Ebba rouvalle toukokuun 9 p.. 1573. Kirje Henrikki Klaunpojalle ja Hermanni Flemingille toukokuun 15 p. 1573. — Koko Ahvenanmaa oli siihen aikaan läänityksenä Katariina Stenbockilla.'},\n"," {'page_number': 411,\n","  'page_char_count': 1885,\n","  'page_word_count': 250,\n","  'page_sentence_count_raw': 20,\n","  'page_token_count': 471.25,\n","  'text': '431 tuli perustaa jalkaväen lippukunta, jossa oli vähintänsä 300 miestä. Suuren ylivoiman uhatessa oli pyydettävä apua puolalaisten linnoista. Tallinnan ja Narvan tullirahat sekä kaikkikin Virosta ja Inkeristä tulevat verot saatiin käyttää varustuksiin huolimatta siitä, mihin tarkotuksiin niitä ennen oli määrätty. Venäjän kauppiaat, joita oli Narvassa, Tallinnassa tai muissa kaupungeissa, olivat pidätettävät ja heidän tavaransa otettavat takavarikkoon. Oudovaa oli jatkettiin kuninkaan antamissa opastuksissa — ryhdyttävä piirittämään, ennen kuin Venäjän sotajoukko oli ennättänyt perille. Jos tämä linna voitettaisiin Ruotsin haltuun ja sitten huomattaisiin, että sitä oli mahdoton puolustaa, oli se poltettava. Siinä tapauksessa taas, että Oudovaa ei voitaisi valloittaa, oli sen lääniä ynnä Someron aluetta ryöstettävä sekä karja sieltä ajettava Ruotsin hallussa oleviin linnoihin. Pieni Jaaman linna saatettiin vaaran uhatessa räjähdyttää, sen jälkeen kuin sieltä väki, muonavarat ja tykit oli viety toisiin linnoihin. Samoin oli meneteltävä Rakveren linnaan nähden I. Mitä Jaaman aiottuun hajoittamiseen tulee, niin Kustaa Baner kirjoitti kuninkaalle kuulleensa Kaarle Henrikinpojalta ja muilta, että linnaa ei saataisi muutamassa päivässä maahanrevityksi, jollei tähän työhön käytettäisi isoa miesjoukkoa. Hän oli sen tähden lykännyt asian toistaiseksi, erittäinkin koska venäläisten saapumisesta ei mitään kuulunut. Jaama jäi siten paikoilleen ja Rakveren linna myöskin 2. Kun luultiin, että venäläisten hyökkäys oli pääasiassa kohdistuva Narvaan, käskettiin läntisen Suomen maaherran Akseli Lejonhufvudin kiireesti toimittaa sinne ruokavaroja sekä Turun linnasta ja kaupungista että myöskin Turun 1  V. R. Kirjeet Kustaa Banerille lokakuun 12, 18 ja 19 p. 1588. 2  Banerin kirje Juhani III:lle 12/i 1589 kokoelmassa Ståthållare i Estland och Reval till K. M. 15701592 R. A.'},\n"," {'page_number': 751,\n","  'page_char_count': 1108,\n","  'page_word_count': 177,\n","  'page_sentence_count_raw': 36,\n","  'page_token_count': 277.0,\n","  'text': '771 Matti Simonpoika 501. Matti Tuomaanpoika 388, 389. Matvei Gregoritsh, 542. Maunu Gudmundinpoika 285, 484. Maunu Henrikin poika. Kts. Tawast. Maunu herttua 3, 4, 19, 20, 2529, 33, 39, 42, 65, 80, 88, 95, 121, 138, 165, 166, 701. Maunu Ladonlukko 365. Maunu Maununpoika 432. Maunu Pietarinpoika 116, 117. Maunu Sveninpoika 311, 341, 742. Medici, di, Katariina, 239, 355. Melkkeri Juhonpoika 48. Mheer, Jörgen, 82. Minden, von, Henrikki Abel, 48, 59, 71, 72, 92, 99, 105, 116, 117, L88, 196, 204, 206, 208, 234, 244, 279, 282, 288, 289, 315, 318, 587, 591, 592, 603, 624, 662, 678, 731, 732, 737. Mikko Paavalinpoika. Kts. Munck. Mjöhund, Perttu Yrjönpoika, 120, 123, 652. Moine, le, 102, 134, 356. Mornay, de, Charles, 96, 97, 102, 278, 295. Mstislavski, Feodor, 158. Munck, Mikko Paavalinpoika, 489, 737. Mundt, Markus, 234, 235. Mundus, Gerdt. Kts. Mundus, Rotker. Mundus, Rotker, 167, 322, 600, 610, 618. Muraviev, Reko, 601. Möller, Yrjö, 530. Nasimov, Feodor, 680, 682. Nassakin, Afanasi, 686, 687. Nassakin, Pietari, 686, 687. Nieroth, Reinholtti, 318, 329, 330, 610, 618, 692. Niilo Laurinpoika 178.'}]"]},"metadata":{},"execution_count":9}],"source":["import random\n","\n","random.sample(pages_and_texts, k=3)"]},{"cell_type":"markdown","metadata":{"id":"v63TENvdyTG8"},"source":["### Get some stats on the text\n","\n","Let's perform a rough exploratory data analysis (EDA) to get an idea of the size of the texts (e.g. character counts, word counts etc) we're working with.\n","\n","The different sizes of texts will be a good indicator into how we should split our texts.\n","\n","Many embedding models have limits on the size of texts they can ingest, for example, the [`sentence-transformers`](https://www.sbert.net/docs/pretrained_models.html) model [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) has an input size of 384 tokens.\n","\n","This means that the model has been trained in ingest and turn into embeddings texts with 384 tokens (1 token ~= 4 characters ~= 0.75 words).\n","\n","Texts over 384 tokens which are encoded by this model will be auotmatically reduced to 384 tokens in length, potentially losing some information.\n","\n","We'll discuss this more in the embedding section.\n","\n","For now, let's turn our list of dictionaries into a DataFrame and explore it."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"S4vpBkzwyTG8","outputId":"c4e1a956-a806-46fd-b549-eef35055c17c","executionInfo":{"status":"ok","timestamp":1734539002998,"user_tz":-120,"elapsed":560,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n","0          -41                0                1                        1   \n","1          -40              131               12                        1   \n","2          -39              184               21                        4   \n","3          -38                0                1                        1   \n","4          -37              185               23                        4   \n","\n","   page_token_count                                               text  \n","0              0.00                                                     \n","1             32.75  HISTORIALLISIA TUTKIMCIKSIA JULKAISSUT SUOMEN ...  \n","2             46.00  POHJOISMRIbEN VIISI KOLMATTAVUOTIIYEIY SOTA VU...  \n","3              0.00                                                     \n","4             46.25  POHJOISMflIbEN VIISI KOLMATTAVUOTI h Eh SOTA V...  "],"text/html":["\n","  <div id=\"df-53099b36-6f66-43e4-a194-7eff4b61d285\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>page_number</th>\n","      <th>page_char_count</th>\n","      <th>page_word_count</th>\n","      <th>page_sentence_count_raw</th>\n","      <th>page_token_count</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-41</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.00</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-40</td>\n","      <td>131</td>\n","      <td>12</td>\n","      <td>1</td>\n","      <td>32.75</td>\n","      <td>HISTORIALLISIA TUTKIMCIKSIA JULKAISSUT SUOMEN ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-39</td>\n","      <td>184</td>\n","      <td>21</td>\n","      <td>4</td>\n","      <td>46.00</td>\n","      <td>POHJOISMRIbEN VIISI KOLMATTAVUOTIIYEIY SOTA VU...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-38</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.00</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-37</td>\n","      <td>185</td>\n","      <td>23</td>\n","      <td>4</td>\n","      <td>46.25</td>\n","      <td>POHJOISMflIbEN VIISI KOLMATTAVUOTI h Eh SOTA V...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-53099b36-6f66-43e4-a194-7eff4b61d285')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-53099b36-6f66-43e4-a194-7eff4b61d285 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-53099b36-6f66-43e4-a194-7eff4b61d285');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-34287653-d0b9-4fdb-8cd4-f29ad3fe73fa\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-34287653-d0b9-4fdb-8cd4-f29ad3fe73fa')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-34287653-d0b9-4fdb-8cd4-f29ad3fe73fa button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 806,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 232,\n        \"min\": -41,\n        \"max\": 764,\n        \"num_unique_values\": 806,\n        \"samples\": [\n          656,\n          255,\n          186\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 390,\n        \"min\": 0,\n        \"max\": 2727,\n        \"num_unique_values\": 521,\n        \"samples\": [\n          1114,\n          2291,\n          1566\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 49,\n        \"min\": 1,\n        \"max\": 362,\n        \"num_unique_values\": 182,\n        \"samples\": [\n          263,\n          205,\n          117\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_raw\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 1,\n        \"max\": 76,\n        \"num_unique_values\": 57,\n        \"samples\": [\n          1,\n          60,\n          21\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 97.67848568602398,\n        \"min\": 0.0,\n        \"max\": 681.75,\n        \"num_unique_values\": 521,\n        \"samples\": [\n          278.5,\n          572.75,\n          391.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 803,\n        \"samples\": [\n          \"YHDESTOISTA LUKU. Narvan purjehdusvesien sotalaivasto vuosien 1575 ja 1577 v\\u00e4lill\\u00e4. Gyllenankarin hy\\u00f6kk\\u00e4ys Narvan satamaan (1577). Kun sotalaivat syksyll\\u00e4 v. 1574 olivat palanneet Narvaan tehdylt\\u00e4 retkelt\\u00e4 1, tuotiin isoimmat niist\\u00e4 talvimajoille Tukholmaan, mutta pienemm\\u00e4t sijoitettiin Turkuun 2. Tosin Virossa olevan sotav\\u00e4en ylip\\u00e4\\u00e4llikk\\u00f6 Pontus de la Gardie oli kuninkaan m\\u00e4\\u00e4r\\u00e4yksen mukaan tahtonut pid\\u00e4tt\\u00e4\\u00e4 Viron puolella kolme tahi nelj\\u00e4 niist\\u00e4 pienist\\u00e4 haaksista, jotka olivat saattaneet lybekkil\\u00e4isilt\\u00e4 valloitettuja tavaroita Tallinnaan, mutta sotakomissaarit eiv\\u00e4t antaneet t\\u00e4h\\u00e4n suostumustaan, vaan l\\u00e4hettiv\\u00e4t haahdet takaisin Suomeen. My\\u00f6hemmin syksyll\\u00e4 p\\u00e4\\u00e4si siten muukalaisia kauppalaivoja tullia maksamatta k\\u00e4ym\\u00e4\\u00e4n Narvassa. Moniaat n\\u00e4ist\\u00e4 olivat useita p\\u00e4ivi\\u00e4 tyvenn\\u00f6ss\\u00e4 Naissaaren luona sek\\u00e4 aivan Tallinnan sataman edess\\u00e4, eik\\u00e4 t\\u00e4\\u00e4lt\\u00e4 voitu l\\u00e4hett\\u00e4\\u00e4 ainoaakaan purtta niit\\u00e4 h\\u00e4tyytt\\u00e4m\\u00e4\\u00e4n 3. Yliamiraali Klaus Fleming ilmoitti alkupuolella vuotta . 1575 kuninkaalle, ettei Suomesta saataisi kokoon muonavaroja 1  Kts. sivua 106. 2  Seuraavana talvena oli Viipurissa pursimiehi\\u00e4 seitsem\\u00e4st\\u00e4toista laivasta. 3 Wieselgren, Delagardiska Archivet, IV, 211.\",\n          \"275 jatkaa taistelua vihollisen voimain ollessa masennettuina, kuin sallia h\\u00e4nen uudestaan toipua. Jollei t\\u00e4n\\u00e4 talvena mit\\u00e4\\u00e4n tehd\\u00e4, niin tulevana kes\\u00e4n\\u00e4k\\u00e4\\u00e4n ei saada paljoa toimeen. Meid\\u00e4n on nimitt\\u00e4in mahdoton silloin panna sotavoimaa liikkeelle, koska Elisabeth ruhtinattaren h\\u00e4\\u00e4t tuovat suuria kustannuksia ja koska rutto, n\\u00e4lk\\u00e4 ja kallis aika rasittavat valtakuntaa\\u00bb 1. Juuri kuin Juhani III oli keskell\\u00e4 taistelupuuhiansa, saapui er\\u00e4s suurruhtinaan l\\u00e4hettil\\u00e4s Ruotsiin. T\\u00e4m\\u00e4n tulo ei kuitenkaan voinut laimentaa Juhanin sotaista intoa 2. Kuningas antoi jo 1580 vuoden alussa tarkkoja ohjeita, miten K\\u00e4kisalmessa oli menetelt\\u00e4v\\u00e4 valloituksen j\\u00e4lkeen 8. N\\u00e4m\\u00e4t ohjeet olivat kuitenkin sill\\u00e4 kertaa tarpeettomat, koska Yrj\\u00f6 Boije ei p\\u00e4\\u00e4ssytk\\u00e4\\u00e4n l\\u00e4htem\\u00e4\\u00e4n matkalle. Mutta sotahankkeista oli seurauksena, ett\\u00e4 Suomessa taas saatiin pitkin vuotta el\\u00e4tt\\u00e4\\u00e4 isoja sotilasjoukkoja, josta talonpojille sukeutui siet\\u00e4m\\u00e4t\\u00f6n rasitus 4. Sen johdosta ett\\u00e4 heit\\u00e4 usein k\\u00e4vi valittamassa Juhani III:lle linnaleirist\\u00e4, kirjoitti h\\u00e4n Suomen maaherralle Klaus Aakenpojalle: \\u00bbKoska tahtomme on ollut semmoinen, ett\\u00e4 sotav\\u00e4ke\\u00e4 ei majoitettaisi alamaisten luo maaseudulle, vaan linnoihin ja kartanoihimme, niin on meille hyvin vastenmielist\\u00e4, ett\\u00e4 k\\u00e4sky\\u00e4mme ei ole 1 V. R. Kirje Klaus Aakenpojalle marraskuun 22 p. 1579. 2  V. R. Kirje Yrj\\u00f6 Boijelle tammikuun 5 p. 1580. 8 V. R. Memoriaali Hannu Krankalle tammikuun 20 p. 1580. \\u2014 Krankka tuli kuninkaan l\\u00e4hettil\\u00e4\\u00e4n\\u00e4 sotajoukon luo. 4  V. R. Kirje Yrj\\u00f6 Boijelle toukokuun 21 p. 1580. \\u2014 Talvella 1579 \\u20141580 oli hallitus m\\u00e4\\u00e4r\\u00e4nnyt Klaus Aakenpojan, Hermanni Flemingin ja rahastonhoitaja Antti Martinpojan maksamaan sotav\\u00e4elle palkan. Joka sotamiehelle annettiin silloin paitse rahaa ja verkaa 4 lammasnahkaa sek\\u00e4 sukkaja kenk\\u00e4pari. T\\u00e4m\\u00e4n vaateavun, joka otettiin lis\\u00e4verona Suomesta, sotamiehet saivat lahjaksi. V. R. Kirje Yrj\\u00f6 Boijelle marraskuun 20 p. 1579. K\\u00e4sky kaikille Suomen voudeille marraskuun 24 p. 1579. Kev\\u00e4\\u00e4ll\\u00e4 (1580) hallitus k\\u00e4ski k\\u00e4ytt\\u00e4\\u00e4 kaikki Suomesta saadut veroja sakkorahat sotav\\u00e4en palkanmaksuun. V. R. Kirje Klaus Aakenpojalle huhtikuun 29 p. 1580.\",\n          \"149 vihdoin niin paljon, ett\\u00e4 k\\u00e4vi varsin vaikeaksi hankkia kaikille elatusta. Silloin Tallinnan hallitus l\\u00e4hetti heit\\u00e4 saalista ottamaan Ven\\u00e4j\\u00e4n hallussa olevalta alueelta. Viron onnettomat asukkaat ry\\u00f6stiv\\u00e4t siten ruokavaroja toisiltansa. He eiv\\u00e4t en\\u00e4\\u00e4 tienneet, kuka maata hallitsi ja kehen oli turvauduttava. Koska Ruotsi ei siihen aikaan \\u2022voinut enemm\\u00e4n kuin Puola ja Tanskakaan vastustaa Ven\\u00e4j\\u00e4n aseita, niin Iivana IV katsoi otollisen hetken tulleeksi Liivinmaan lopullista valloitusta varten. H\\u00e4n p\\u00e4\\u00e4tti eneiksi l\\u00e4hett\\u00e4\\u00e4 sotajoul on Tallinnaa piiritt\\u00e4m \\u00e4\\u00e4n. Huhtikuun 9 p. 1576 oli Henrikki.Klaunpoila, Horn m\\u00e4\\u00e4r\\u00e4tty menem\\u00e4\\u00e4n i\\u00e4st\\u00e4ns\\u00e4 ja heikkoudestansa huolimatta Tallinnaan, jossa h\\u00e4nen oli \\u2022 otettava maaherranvirk a huostaansa 2. Pontus de la Gardien l\\u00e4hdetty\\u00e4 Ruotsiin, ja ennen kuin Henrikki Klaunpoika oli tullut Viroon, oli .Kaarle. Horn v\\u00e4liaikaisesti k\\u00e4skynhaltijana Tallinnassa. H\\u00e4n pyysi, samoin kuin Pontus oli tehnyt, useita kertoja kuninkaalta p\\u00e4\\u00e4st\\u00e4 linnan is\\u00e4nnyydest\\u00e4 vapaaksi, mutta Juhani III antoi n\\u00e4ihin pyynt\\u00f6ihin kielt\\u00e4v\\u00e4n vastauksena. Paljon huolta tuotti Kaarle Hornille ruotsalaisen sotav\\u00e4en niskoittelu. T\\u00e4m\\u00e4n p\\u00e4\\u00e4syy n\\u00e4ytt\\u00e4\\u00e4 olleen siin\\u00e4, ett\\u00e4 sotamiehi\\u00e4 ei p\\u00e4\\u00e4stetty Tallinnasta niin pian, kuin he olivat toivoneet. Jo alussa vuotta 1576 olivat muutamat jalkav\\u00e4en lippukunnat anoneet vapautusta linnanpalveluk sesta ja helmikuun 5 p\\u00e4iv\\u00e4n\\u00e4 kuningas oli ilmoittanut heille, ett\\u00e4 h\\u00e4n seuraavana kes\\u00e4n\\u00e4 oli suostuva heid\\u00e4n pyynt\\u00f6\\u00f6ns\\u00e4 4. Viel\\u00e4 kuukautta 1  V. R. 2  Samaan aikaan Juhani III kuitenkin Kirjoitti yliamiraali Klaus Flemingille, ett\\u00e4 t\\u00e4m\\u00e4n ja Klaus Aakenpojan tuli olla valmiina vapauttamaan Henrikki Klaunpoika ja Kaarle Horn p\\u00e4\\u00e4llikkyydest\\u00e4, jos kuningas sit\\u00e4 heilt\\u00e4 syyspuoleen vaatisi. V. R. 6/iv 76. 3  V. R. Vastaus Kaarle Henrikinpojalle syyskuun 18 p. 1576. \\u2014 Elokuun 8 p. 1576 m\\u00e4\\u00e4r\\u00e4ttiin tosin ruotsalainen Yrj\\u00f6 Eerikinpoika Ulfsparre Kaarle Hornin sijaiseksi, mutta n\\u00e4ht\\u00e4v\\u00e4sti j\\u00e4lkimm\\u00e4inen ei syksyll\\u00e4k\\u00e4\\u00e4n ollut poissa Tallinnasta. V. R.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":10}],"source":["import pandas as pd\n","\n","df = pd.DataFrame(pages_and_texts)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"PuWN2GLByTG8","outputId":"07b4e110-4a6c-4624-9526-c5fd084b878d","executionInfo":{"status":"ok","timestamp":1734539002999,"user_tz":-120,"elapsed":19,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n","count       806.00           806.00           806.00                   806.00   \n","mean        361.50          1896.07           248.54                    24.98   \n","std         232.82           390.71            49.97                     9.28   \n","min         -41.00             0.00             1.00                     1.00   \n","25%         160.25          1854.00           240.00                    19.00   \n","50%         361.50          1979.00           258.00                    24.00   \n","75%         562.75          2100.75           275.00                    30.00   \n","max         764.00          2727.00           362.00                    76.00   \n","\n","       page_token_count  \n","count            806.00  \n","mean             474.02  \n","std               97.68  \n","min                0.00  \n","25%              463.50  \n","50%              494.75  \n","75%              525.19  \n","max              681.75  "],"text/html":["\n","  <div id=\"df-edcf464e-1686-4538-afe2-2f119a63325b\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>page_number</th>\n","      <th>page_char_count</th>\n","      <th>page_word_count</th>\n","      <th>page_sentence_count_raw</th>\n","      <th>page_token_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>806.00</td>\n","      <td>806.00</td>\n","      <td>806.00</td>\n","      <td>806.00</td>\n","      <td>806.00</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>361.50</td>\n","      <td>1896.07</td>\n","      <td>248.54</td>\n","      <td>24.98</td>\n","      <td>474.02</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>232.82</td>\n","      <td>390.71</td>\n","      <td>49.97</td>\n","      <td>9.28</td>\n","      <td>97.68</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>-41.00</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>160.25</td>\n","      <td>1854.00</td>\n","      <td>240.00</td>\n","      <td>19.00</td>\n","      <td>463.50</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>361.50</td>\n","      <td>1979.00</td>\n","      <td>258.00</td>\n","      <td>24.00</td>\n","      <td>494.75</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>562.75</td>\n","      <td>2100.75</td>\n","      <td>275.00</td>\n","      <td>30.00</td>\n","      <td>525.19</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>764.00</td>\n","      <td>2727.00</td>\n","      <td>362.00</td>\n","      <td>76.00</td>\n","      <td>681.75</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-edcf464e-1686-4538-afe2-2f119a63325b')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-edcf464e-1686-4538-afe2-2f119a63325b button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-edcf464e-1686-4538-afe2-2f119a63325b');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-76ba5884-59b9-47ac-be75-164fc67ac081\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-76ba5884-59b9-47ac-be75-164fc67ac081')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-76ba5884-59b9-47ac-be75-164fc67ac081 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 294.396873369946,\n        \"min\": -41.0,\n        \"max\": 806.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          806.0,\n          361.5,\n          562.75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 951.136529097652,\n        \"min\": 0.0,\n        \"max\": 2727.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1896.07,\n          1979.0,\n          806.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 244.1467649905629,\n        \"min\": 1.0,\n        \"max\": 806.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          248.54,\n          258.0,\n          806.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_raw\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 276.55618399728985,\n        \"min\": 1.0,\n        \"max\": 806.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          24.98,\n          24.0,\n          806.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 271.26188186945313,\n        \"min\": 0.0,\n        \"max\": 806.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          474.02,\n          494.75,\n          806.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":11}],"source":["# Get stats\n","df.describe().round(2)"]},{"cell_type":"markdown","metadata":{"id":"_Qc0YbjPyTG9"},"source":["Okay, looks like our average token count per page is 287.\n","\n","For this particular use case, it means we could embed an average whole page with the `all-mpnet-base-v2` model (this model has an input capacity of 384)."]},{"cell_type":"markdown","metadata":{"id":"58VCs_zPyTG9"},"source":["### Further text processing (splitting pages into sentences)\n","\n","The ideal way of processing text before embedding it is still an active area of research.\n","\n","A simple method I've found helpful is to break the text into chunks of sentences.\n","\n","As in, chunk a page of text into groups of 5, 7, 10 or more sentences (these values are not set in stone and can be explored).\n","\n","But we want to follow the workflow of:\n","\n","`Ingest text -> split it into groups/chunks -> embed the groups/chunks -> use the embeddings`\n","\n","Some options for splitting text into sentences:\n","\n","1. Split into sentences with simple rules (e.g. split on \". \" with `text = text.split(\". \")`, like we did above).\n","2. Split into sentences with a natural language processing (NLP) library such as [spaCy](https://spacy.io/) or [nltk](https://www.nltk.org/).\n","\n","Why split into sentences?\n","\n","* Easier to handle than larger pages of text (especially if pages are densely filled with text).\n","* Can get specific and find out which group of sentences were used to help within a RAG pipeline.\n","\n","> **Resource:** See [spaCy install instructions](https://spacy.io/usage).\n","\n","Let's use spaCy to break our text into sentences since it's likely a bit more robust than just using `text.split(\". \")`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"AB7TB1h8yTG9","outputId":"343afb81-a766-4247-864c-794b74474824","executionInfo":{"status":"ok","timestamp":1734539033220,"user_tz":-120,"elapsed":30235,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n","Collecting pip\n","  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (75.1.0)\n","Collecting setuptools\n","  Downloading setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.45.1)\n","Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: setuptools, pip\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 75.1.0\n","    Uninstalling setuptools-75.1.0:\n","      Successfully uninstalled setuptools-75.1.0\n","  Attempting uninstall: pip\n","    Found existing installation: pip 24.1.2\n","    Uninstalling pip-24.1.2:\n","      Successfully uninstalled pip-24.1.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed pip-24.3.1 setuptools-75.6.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["_distutils_hack","setuptools"]},"id":"49abb89f32cb4755ab3ae3924c08ea1e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n","Collecting spacy\n","  Downloading spacy-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n","Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n","  Downloading thinc-8.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.5.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n","Collecting blis<1.2.0,>=1.1.0 (from thinc<8.4.0,>=8.3.0->spacy)\n","  Downloading blis-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n","Downloading spacy-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.1/29.1 MB\u001b[0m \u001b[31m141.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading thinc-8.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m120.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading blis-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m130.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: blis, thinc, spacy\n","  Attempting uninstall: blis\n","    Found existing installation: blis 0.7.11\n","    Uninstalling blis-0.7.11:\n","      Successfully uninstalled blis-0.7.11\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 8.2.5\n","    Uninstalling thinc-8.2.5:\n","      Successfully uninstalled thinc-8.2.5\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 3.7.5\n","    Uninstalling spacy-3.7.5:\n","      Successfully uninstalled spacy-3.7.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","en-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.8.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed blis-1.1.0 spacy-3.8.3 thinc-8.3.3\n","Collecting fi-core-news-sm==3.8.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/fi_core_news_sm-3.8.0/fi_core_news_sm-3.8.0-py3-none-any.whl (14.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: fi-core-news-sm\n","Successfully installed fi-core-news-sm-3.8.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('fi_core_news_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n","/bin/bash: line 1: import: command not found\n","/bin/bash: line 1: np.float_: command not found\n","/bin/bash: line 1: from: command not found\n","/bin/bash: line 1: np.float_: command not found\n"]},{"output_type":"execute_result","data":{"text/plain":["[Tässä lause suomeksi., Tässä toinen.]"]},"metadata":{},"execution_count":12}],"source":["!pip install -U pip setuptools wheel\n","!pip install -U spacy\n","!python -m spacy download fi_core_news_sm\n","\n","!import numpy as np\n","!np.float_ = np.float64\n","!from prophet import Prophet\n","\n","from spacy.lang.fi import Finnish # see https://spacy.io/usage for install instructions\n","\n","nlp = Finnish()\n","!np.float_ = np.float64 #AttributeError: `np.float_` was removed in the NumPy 2.0 release. Use `np.float64` instead.\n","# Add a sentencizer pipeline, see https://spacy.io/api/sentencizer/\n","nlp.add_pipe(\"sentencizer\")\n","\n","# Create a document instance as an example\n","doc = nlp(\"Tässä lause suomeksi. Tässä toinen.\")\n","assert len(list(doc.sents)) == 2  #TÄMÄN PITÄÄ NÄEMMÄ OLLA 2\n","\n","# Access the sentences of the document\n","list(doc.sents)\n","\n","#AttributeError: `np.float_` was removed in the NumPy 2.0 release. Use `np.float64` instead."]},{"cell_type":"markdown","metadata":{"id":"uRtWvCo4yTG-"},"source":["We don't necessarily need to use spaCy, however, it's an open-source library designed to do NLP tasks like this at scale.\n","\n","So let's run our small sentencizing pipeline on our pages of text."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["9b5bffac562f4c069161a29a8912fe83","4dd0ff3297c0423ca7cb9de4ee4bd489","06fc34d7727c4e149afad683fce5e141","84c62366f81e4b1ab3f3ab6b3475159c","de8547414ba6456587519aeab3b070aa","f91f95529476491ba8bf0bef15f7f151","b7d550a0550542c4b198aa05c6fdeb34","4f2fab2c96e04e45a44a4e9067439b09","e59235cceb9c46bbb2713a216e8286d9","640f340daaf34838b142fa6ed9318efa","2ea5cc112a644562bc84290a74cd5778"]},"id":"aOU7bh-DyTG-","outputId":"a311d4ad-34ca-4ddb-aa07-b032b46bd716","executionInfo":{"status":"ok","timestamp":1734539036191,"user_tz":-120,"elapsed":3001,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/806 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b5bffac562f4c069161a29a8912fe83"}},"metadata":{}}],"source":["for item in tqdm(pages_and_texts):\n","    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n","\n","    # Make sure all sentences are strings\n","    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n","\n","    # Count the sentences\n","    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cdD-L5-gyTG-","outputId":"d551cad1-dd4b-40ee-c652-ddca75f7eecf","executionInfo":{"status":"ok","timestamp":1734539036192,"user_tz":-120,"elapsed":30,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'page_number': 97,\n","  'page_char_count': 2168,\n","  'page_word_count': 288,\n","  'page_sentence_count_raw': 39,\n","  'page_token_count': 542.0,\n","  'text': 'p. (1574) annettu käsky 11 117 Suomen hallitusmiehiä kohtaan sekä lopuksi, otettuaan väkisin lippunsa päälliköiltä, (syyskuussa) lähteneet Viipurin linnasta kulkemaan Ruotsiin päin tehden matkallansa väkivaltaa maakansalle. Kun kuningas sai tiedon} näistä asioista, olivat he jo edenneet Turun lääniin saakka, josta aikoivat päästä meren poikki. Juhani käski silloin Klaus Flemingin ja Klaus Aakenpojan vielä koettaa estellä heidän matkaansa. Jos miehet eivät hyvällä ottaisi palataksensa Viipuriin, annettiin näille käskynhaltijoille valta nostattaa niin paljon yhteistä kansaa, että voisivat pakoittaa heidät tänne 1.  Eräässä kirjoituksessa sanoo Juhani noiden häiriöiden syntyneen joidenkuiden kavaltajien yllytyksestä, jotka hokivat, että sotamiehet eivät olleet velvolliset taistelemaan Suornen puolesta, koska tämä ei ollut heidän isänmaatansa, ja että oli yhdentekevä, tuliko Suomi puolustetuksi vai ei 2. Hän uhkasi karkureille kovinta rangaistusta, jos he eivät kohta kääntyisi takaisin 3. Kaikesta päättäen nämät eivät sittenkään palanneet Viipuriin. Jotta kaikki asiat täällä tulisivat paremmin valvotuiksi, asetettiin yliamiraali Klaus Fleming Viipurin linnan ylimmäiseksi käskynhaltijaksi 4. Eerikki Hakuninpoika Slang, Antti Niilonpoika Sabelfana sekä moniaat muut aatelismiehet saivat käskyn olla hänelle avullisia isännyyden hoidossa. Siinä tapauksessa että vihollinen yrittäisi rynnätä Suomeen, oli myös Klaus Aakenpojan jääminen tähän linnaan. Hermanni Fleming valtuutettiin taas kaiken Suomeen sijoitetun sotaväen ylipäälliköksi; Henrikki von Minden ja Lauri von Köllen määrättiin johtamaan jalkaväkeä 5. 1  V. R. Kirje Kaarle herttualle syysk. 30 n. Kirje ja Klaus Aakenpojalle lokak. 8 p. 1574. 2  V. R. Kirje Maunu Pietarinpojan lipun alla oleville Smaalannin jalkamiehille lokakuun 24 p. 1574. 2 V. R. Kirje kaikille Viipurin linnasta luopuneille Ruotsin sotamiehille lokakuun 7 p. 1574. 4 Henrikki Klaunpojalle oli syyskuun lähteä kuninkaan asioille Viroon. V. R. s V. R. Kirje Klaus Flemingille ja Klaus Aakenpojalle lokakuun p. 1574. Hermanni Flemingin valtakirja sam. päiv. Klaus Flemingin valtakirja lokakuun 9 p. 1574. Klaus Flemingille 8',\n","  'sentences': ['p. (1574) annettu käsky 11 117 Suomen hallitusmiehiä kohtaan sekä lopuksi, otettuaan väkisin lippunsa päälliköiltä, (syyskuussa) lähteneet Viipurin linnasta kulkemaan Ruotsiin päin tehden matkallansa väkivaltaa maakansalle.',\n","   'Kun kuningas sai tiedon} näistä asioista, olivat he jo edenneet Turun lääniin saakka, josta aikoivat päästä meren poikki.',\n","   'Juhani käski silloin Klaus Flemingin ja Klaus Aakenpojan vielä koettaa estellä heidän matkaansa.',\n","   'Jos miehet eivät hyvällä ottaisi palataksensa Viipuriin, annettiin näille käskynhaltijoille valta nostattaa niin paljon yhteistä kansaa, että voisivat pakoittaa heidät tänne 1.',\n","   ' Eräässä kirjoituksessa sanoo Juhani noiden häiriöiden syntyneen joidenkuiden kavaltajien yllytyksestä, jotka hokivat, että sotamiehet eivät olleet velvolliset taistelemaan Suornen puolesta, koska tämä ei ollut heidän isänmaatansa, ja että oli yhdentekevä, tuliko Suomi puolustetuksi vai ei 2.',\n","   'Hän uhkasi karkureille kovinta rangaistusta, jos he eivät kohta kääntyisi takaisin 3.',\n","   'Kaikesta päättäen nämät eivät sittenkään palanneet Viipuriin.',\n","   'Jotta kaikki asiat täällä tulisivat paremmin valvotuiksi, asetettiin yliamiraali Klaus Fleming Viipurin linnan ylimmäiseksi käskynhaltijaksi 4.',\n","   'Eerikki Hakuninpoika Slang, Antti Niilonpoika Sabelfana sekä moniaat muut aatelismiehet saivat käskyn olla hänelle avullisia isännyyden hoidossa.',\n","   'Siinä tapauksessa että vihollinen yrittäisi rynnätä Suomeen, oli myös Klaus Aakenpojan jääminen tähän linnaan.',\n","   'Hermanni Fleming valtuutettiin taas kaiken Suomeen sijoitetun sotaväen ylipäälliköksi; Henrikki von Minden ja Lauri von Köllen määrättiin johtamaan jalkaväkeä 5.',\n","   '1  V. R. Kirje Kaarle herttualle syysk.',\n","   '30 n. Kirje ja Klaus Aakenpojalle lokak.',\n","   '8 p. 1574.',\n","   '2  V. R. Kirje Maunu Pietarinpojan lipun alla oleville Smaalannin jalkamiehille lokakuun 24 p. 1574.',\n","   '2 V. R. Kirje kaikille Viipurin linnasta luopuneille Ruotsin sotamiehille lokakuun 7 p. 1574.',\n","   '4 Henrikki Klaunpojalle oli syyskuun lähteä kuninkaan asioille Viroon.',\n","   'V. R. s V. R. Kirje Klaus Flemingille ja Klaus Aakenpojalle lokakuun p. 1574.',\n","   'Hermanni Flemingin valtakirja sam.',\n","   'päiv.',\n","   'Klaus Flemingin valtakirja lokakuun 9 p. 1574.',\n","   'Klaus Flemingille 8'],\n","  'page_sentence_count_spacy': 22}]"]},"metadata":{},"execution_count":14}],"source":["# Inspect an example\n","random.sample(pages_and_texts, k=1)"]},{"cell_type":"markdown","metadata":{"id":"V4nsH8w2yTG_"},"source":["Wonderful!\n","\n","Now let's turn out list of dictionaries into a DataFrame and get some stats."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"oojuIIRByTG_","outputId":"0f1e28e2-6b8b-4daf-8d5a-c5c749895025","executionInfo":{"status":"ok","timestamp":1734539036193,"user_tz":-120,"elapsed":22,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n","count       806.00           806.00           806.00                   806.00   \n","mean        361.50          1896.07           248.54                    24.98   \n","std         232.82           390.71            49.97                     9.28   \n","min         -41.00             0.00             1.00                     1.00   \n","25%         160.25          1854.00           240.00                    19.00   \n","50%         361.50          1979.00           258.00                    24.00   \n","75%         562.75          2100.75           275.00                    30.00   \n","max         764.00          2727.00           362.00                    76.00   \n","\n","       page_token_count  page_sentence_count_spacy  \n","count            806.00                     806.00  \n","mean             474.02                      18.53  \n","std               97.68                       7.35  \n","min                0.00                       0.00  \n","25%              463.50                      15.00  \n","50%              494.75                      18.00  \n","75%              525.19                      21.00  \n","max              681.75                      65.00  "],"text/html":["\n","  <div id=\"df-dada0194-9c94-45c9-a131-62dccb9ec8a1\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>page_number</th>\n","      <th>page_char_count</th>\n","      <th>page_word_count</th>\n","      <th>page_sentence_count_raw</th>\n","      <th>page_token_count</th>\n","      <th>page_sentence_count_spacy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>806.00</td>\n","      <td>806.00</td>\n","      <td>806.00</td>\n","      <td>806.00</td>\n","      <td>806.00</td>\n","      <td>806.00</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>361.50</td>\n","      <td>1896.07</td>\n","      <td>248.54</td>\n","      <td>24.98</td>\n","      <td>474.02</td>\n","      <td>18.53</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>232.82</td>\n","      <td>390.71</td>\n","      <td>49.97</td>\n","      <td>9.28</td>\n","      <td>97.68</td>\n","      <td>7.35</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>-41.00</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>160.25</td>\n","      <td>1854.00</td>\n","      <td>240.00</td>\n","      <td>19.00</td>\n","      <td>463.50</td>\n","      <td>15.00</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>361.50</td>\n","      <td>1979.00</td>\n","      <td>258.00</td>\n","      <td>24.00</td>\n","      <td>494.75</td>\n","      <td>18.00</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>562.75</td>\n","      <td>2100.75</td>\n","      <td>275.00</td>\n","      <td>30.00</td>\n","      <td>525.19</td>\n","      <td>21.00</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>764.00</td>\n","      <td>2727.00</td>\n","      <td>362.00</td>\n","      <td>76.00</td>\n","      <td>681.75</td>\n","      <td>65.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dada0194-9c94-45c9-a131-62dccb9ec8a1')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-dada0194-9c94-45c9-a131-62dccb9ec8a1 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-dada0194-9c94-45c9-a131-62dccb9ec8a1');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-98e8b1ba-b38a-4af9-946f-46842a44636f\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-98e8b1ba-b38a-4af9-946f-46842a44636f')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-98e8b1ba-b38a-4af9-946f-46842a44636f button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 294.396873369946,\n        \"min\": -41.0,\n        \"max\": 806.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          806.0,\n          361.5,\n          562.75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 951.136529097652,\n        \"min\": 0.0,\n        \"max\": 2727.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1896.07,\n          1979.0,\n          806.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 244.1467649905629,\n        \"min\": 1.0,\n        \"max\": 806.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          248.54,\n          258.0,\n          806.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_raw\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 276.55618399728985,\n        \"min\": 1.0,\n        \"max\": 806.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          24.98,\n          24.0,\n          806.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 271.26188186945313,\n        \"min\": 0.0,\n        \"max\": 806.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          474.02,\n          494.75,\n          806.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_spacy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 278.31810849149883,\n        \"min\": 0.0,\n        \"max\": 806.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          18.53,\n          18.0,\n          806.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":15}],"source":["df = pd.DataFrame(pages_and_texts)\n","df.describe().round(2)"]},{"cell_type":"markdown","metadata":{"id":"inrjNKfByTG_"},"source":["For our set of text, it looks like our raw sentence count (e.g. splitting on `\". \"`) is quite close to what spaCy came up with.\n","\n","Now we've got our text split into sentences, how about we gorup those sentences?"]},{"cell_type":"markdown","metadata":{"id":"hJazrBICyTG_"},"source":["### Chunking our sentences together\n","\n","Let's take a step to break down our list of sentences/text into smaller chunks.\n","\n","As you might've guessed, this process is referred to as **chunking**.\n","\n","Why do we do this?\n","\n","1. Easier to manage similar sized chunks of text.\n","2. Don't overload the embedding models capacity for tokens (e.g. if an embedding model has a capacity of 384 tokens, there could be information loss if you try to embed a sequence of 400+ tokens).\n","3. Our LLM context window (the amount of tokens an LLM can take in) may be limited and requires compute power so we want to make sure we're using it as well as possible.\n","\n","Something to note is that there are many different ways emerging for creating chunks of information/text.\n","\n","For now, we're going to keep it simple and break our pages of sentences into groups of 10 (this number is arbitrary and can be changed, I just picked it because it seemed to line up well with our embedding model capacity of 384).\n","\n","On average each of our pages has 10 sentences.\n","\n","And an average total of 287 tokens per page.\n","\n","So our groups of 10 sentences will also be ~287 tokens long.\n","\n","This gives us plenty of room for the text to embedded by our `all-mpnet-base-v2` model (it has a capacity of 384 tokens).\n","\n","To split our groups of sentences into chunks of 10 or less, let's create a function which accepts a list as input and recursively breaks into down into sublists of a specified size."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":812,"referenced_widgets":["d9e5515a5b8143338b9a3efc79e69011","c426b488faca43218a9b92af39cf79fc","b1b98728e8f243758b413da90830121b","a267d8fda61c4be3a32922f65d32c352","065b5b8a5892465da087a1ef4399b3c4","e340c6009d5346239be90c74501e84df","65b97e58fdfc42be9449c9e6ff728d9b","e33ad5526eb34d46b31e662860dd6550","8d451cc0481d4d08801faee82b3c0b4b","9d22c9afd161452fb27d8004f8a8b33f","0ccf0ae5e30c4bf386e6695c513d6b4d","9a3d6c760faa4636812ac807fe5fe97a","47b35ccf6b2e4d5aa4c9e5f44c3477a9","90deb48eba0f4d799766ba4b8b4e8208","36576091012d453e96e9cc89ffdfc23a","60b551776a36455b85917a96481655bf","02ff9932871b4f798eacb23175d5d20f","384bb7b6e1724fddb768e5032bc884ef","44b4ccc6cc0a4242bc7c7acd16b1a434","a1ea44054c9c41bf8dc9fe9f67f5c15d","ed06f8cbcdbf4b0db1db368df6762a21","c5f2ecbb36af4645a69bdec35a20da33","ea2f3eb7625e4615b07c02b6f7f1c966","681b4841a5be49df867110cdc37deaaf","0491fae7f47f4387a17992b6d4b6b2fd","e7a36c6ed744444d83f415ad82244ea4","3397e3a4fb7b44aaa862e80154c31e83","0dad2c4ef7f84d30a5617485fd88e313","8b270b71f7784f74889fae69b9c68209","a8bf1c82f8cc4566bfb495eea7034650","59b754ecf7c547ca91c8a0092d9d9dcd","331dc725fe0a4cdabe8f934ddef65c87","db23974cd2164bbb91767a754d3d4257","f2e6ed4fbf8d488ea65e1f2966ac1ecd","49b6751252b74537a298313b12f6bff1","449d668f64f140e78c2d19cfbb2195fd","aecb1f8916894298aed3e234fcc0db88","a091618115df49ce8d532a8414cd97c2","4ad3f3b91eba41bca97d479e09cce0a0","e5a7f7ae42f94cd18750d0f8d5923562","65551abf3c0b419ca8b88de382bf176b","7b1e07747d5c47de87258b5bf697f785","f176d307feb7467ab5a738beb84c62ee","65f12179f1374e059f5cef735f3d752e","e9702475ec2944e4b5e00c0ff7a601c0","6a7b01281eed4c20976a82d135d9eb2a","5c684c08d7a6449183176b633486ce02","4aea1bd03bf34a9fb0b0bda7f2be6623","1a20bd247f85499498b48978bf0816d5","3b1804943b054fc8bbcc7a6b765d265e","b1d239fae99a4ce89d3f6a39a063e8eb","58967ff037bb454cba710d4b25b3d6ef","fdea44dea216431482312d4af49157f6","6f524bab7f7548e1ac1ac226cea0144e","c7b942cb9bfa4ad497a0b9e2756af6d3"]},"id":"4GlCH37Qdsht","executionInfo":{"status":"ok","timestamp":1734539046428,"user_tz":-120,"elapsed":10253,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}},"outputId":"81d61de7-4a9b-437c-d6dd-67dff5098503"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting semchunk\n","  Downloading semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n","Collecting mpire[dill] (from semchunk)\n","  Downloading mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from semchunk) (4.66.6)\n","Requirement already satisfied: pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from mpire[dill]->semchunk) (2.18.0)\n","Collecting multiprocess (from mpire[dill]->semchunk)\n","  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n","Collecting dill>=0.3.9 (from multiprocess->mpire[dill]->semchunk)\n","  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n","Downloading semchunk-2.2.2-py3-none-any.whl (10 kB)\n","Downloading mpire-2.10.2-py3-none-any.whl (272 kB)\n","Downloading multiprocess-0.70.17-py310-none-any.whl (134 kB)\n","Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n","Installing collected packages: mpire, dill, multiprocess, semchunk\n","Successfully installed dill-0.3.9 mpire-2.10.2 multiprocess-0.70.17 semchunk-2.2.2\n","Collecting tiktoken\n","  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n","Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken\n","Successfully installed tiktoken-0.8.0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9e5515a5b8143338b9a3efc79e69011"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/894k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a3d6c760faa4636812ac807fe5fe97a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/552k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea2f3eb7625e4615b07c02b6f7f1c966"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/2.30M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2e6ed4fbf8d488ea65e1f2966ac1ecd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/295 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9702475ec2944e4b5e00c0ff7a601c0"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00, 6132.02it/s]\n"]}],"source":["#https://github.com/umarbutler/semchunk\n","\n","!pip install semchunk\n","!pip install tiktoken\n","\n","import semchunk\n","from transformers import AutoTokenizer # Neither `transformers` nor `tiktoken` are required,\n","import tiktoken                        # they are here for demonstration purposes.\n","\n","chunk_size = 2 # A low chunk size is used here for demonstration purposes. Keep in mind that\n","               # `semchunk` doesn't take special tokens into account unless you're using a\n","               # custom token counter, so you probably want to deduct your chunk size by the\n","               # number of special tokens added by your tokenizer.\n","text = 'The quick brown fox jumps over the lazy dog.'\n","\n","# As you can see below, `semchunk.chunkerify` will accept the names of all OpenAI models, OpenAI\n","# `tiktoken` encodings and Hugging Face models (in that order of precedence), along with custom\n","# tokenizers that have an `encode()` method (such as `tiktoken`, `transformers` and `tokenizers`\n","# tokenizers) and finally any function that can take a text and return the number of tokens in it.\n","chunker = semchunk.chunkerify('umarbutler/emubert', chunk_size) or \\\n","          semchunk.chunkerify('gpt-4', chunk_size) or \\\n","          semchunk.chunkerify('cl100k_base', chunk_size) or \\\n","          semchunk.chunkerify(AutoTokenizer.from_pretrained('umarbutler/emubert'), chunk_size) or \\\n","          semchunk.chunkerify(tiktoken.encoding_for_model('gpt-4'), chunk_size) or \\\n","          semchunk.chunkerify(lambda text: len(text.split()), chunk_size)\n","\n","# The resulting `chunker` can take and chunk a single text or a list of texts, returning a list of\n","# chunks or a list of lists of chunks, respectively.\n","assert chunker(text) == ['The quick', 'brown', 'fox', 'jumps', 'over the', 'lazy', 'dog.']\n","assert chunker([text], progress = True) == [['The quick', 'brown', 'fox', 'jumps', 'over the', 'lazy', 'dog.']]\n","\n","# If you have a large number of texts to chunk and speed is a concern, you can also enable\n","# multiprocessing by setting `processes` to a number greater than 1.\n","assert chunker([text], processes = 2) == [['The quick', 'brown', 'fox', 'jumps', 'over the', 'lazy', 'dog.']]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":110},"id":"oGj9Hdxbdshu","executionInfo":{"status":"error","timestamp":1734539267497,"user_tz":-120,"elapsed":312,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}},"outputId":"13b70afb-c9a5-4144-c9e0-85239e208e85"},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"incomplete input (<ipython-input-19-ca5796e4c5ff>, line 7)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-ca5796e4c5ff>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    ) -> Callable[[str | Sequence[str], bool, bool], list[str] | list[list[str]]]:\u001b[0m\n\u001b[0m                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"]}],"source":["def chunkerify(\n","    tokenizer_or_token_counter: str | tiktoken.Encoding | transformers.PreTrainedTokenizer | \\\n","                                tokenizers.Tokenizer | Callable[[str], int],\n","    chunk_size: int = None,\n","    max_token_chars: int = None,\n","    memoize: bool = True,\n",") -> Callable[[str | Sequence[str], bool, bool], list[str] | list[list[str]]]:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":110},"id":"ICEyTQDHdshu","executionInfo":{"status":"error","timestamp":1734539274756,"user_tz":-120,"elapsed":238,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}},"outputId":"be60cd86-2dd4-46f4-c978-fc1c63dbc353"},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"expected ':' (<ipython-input-20-612c3ca67f9b>, line 6)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-612c3ca67f9b>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    ) -> list[str]\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"]}],"source":["def chunk(\n","    text: str,\n","    chunk_size: int,\n","    token_counter: Callable,\n","    memoize: bool = True,\n",") -> list[str]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["3b6fdfc879494fed81143086a9506744","1d5478a382774db19c95a93aee876f28","6f47aee176624102a614f6a758d1f911","2b1b4870262842f4be8b550b1683a0c7","e8b35554292645ff8f1118dc254c9052","48ce176cc387467ca45642790f58c792","ea4bb2bfc3b945afa6fb9b30cfcf9a56","6cb3f4cb53254af7aa776db9159fda66","308de8bd98eb4442b7ab1f1f1fa2b800","ed3364fb297d48fbb9f050f54be488c0","d0bf09202afc4602a7af03d24c47be8c"]},"id":"QNrWs4LQyTHA","outputId":"ac8527c6-ce50-4338-b85b-c98c0f60d870","executionInfo":{"status":"ok","timestamp":1734539277127,"user_tz":-120,"elapsed":245,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/806 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b6fdfc879494fed81143086a9506744"}},"metadata":{}}],"source":["# Define split size to turn groups of sentences into chunks\n","num_sentence_chunk_size = 10\n","\n","# Create a function that recursively splits a list into desired sizes\n","def split_list(input_list: list,\n","               slice_size: int) -> list[list[str]]:\n","    \"\"\"\n","    Splits the input_list into sublists of size slice_size (or as close as possible).\n","\n","    For example, a list of 17 sentences would be split into two lists of [[10], [7]]\n","    \"\"\"\n","    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n","\n","# Loop through pages and texts and split sentences into chunks\n","for item in tqdm(pages_and_texts):\n","    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n","                                         slice_size=num_sentence_chunk_size)\n","    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bkw74NYgyTHA","outputId":"1deec182-e861-419d-e5b9-49508bb39763","executionInfo":{"status":"ok","timestamp":1734539280351,"user_tz":-120,"elapsed":221,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'page_number': -27,\n","  'page_char_count': 2114,\n","  'page_word_count': 281,\n","  'page_sentence_count_raw': 60,\n","  'page_token_count': 528.5,\n","  'text': 'XIII — Viipuriin jäävä varusväki. — Tallinnassa vähemmin sotaväkeä ja muonavaroja kuin Hornille oli luvattu. — Horn lähtee piirittämään Narvaa. — Laivasto, jonka piti tuoda sinne järeät tykit. ei saavukaan. Pitkälliset sateet ja muonavarojen puute heikontavat väkeä. Syyskuun viime päivinä lopetetaan piiritys. —Paluumatka vaikea. — Sotajoukko hajaantumistilassa. — Huovit saavat linnaleirin Suomessa. — Neuvoskunta tutkii Hornin syyllisyyttä. KAHDEKSASTOISTA LUKU. Puolan sekaantuminen sotaan. Siv. 262272. Bathori Puolan kuninkaana. — Kysymys Ruotsin ja Puolan välisestä liitosta. — Lorichs Juhani kuninkaan asiamiehenä Puolassa. — Pontus de la Gardie lähettiläänä Saksassa ja paavin luona. —Possevino tulee Ruotsiin. — Vennon tappelu. — Bathori sotii Venäjällä. — Jam Sapolskin rauha. — Puola saa haltuunsa Liivinmaan linnat. — Ruotsin ja Puolan välit huononevat. YHDEKSÄSTOISTA LUKU. Pontus de la Gardie ja Käkisalmen valloitus. Siv. 273299. Juhani III käskee Yrjö Boijen lähteä valloittamaan Käkisalmea. — Linnaleiri Suomessa. — Kesällä (1580) ei saada mitään toimeen. — Pontus de la Gardie määrätään ylipäälliköksi. — Hänen aikaisemmat vaiheensa. — Pontuksella lavea valta. — Hänen alapäällikkönsä. — Ruotsin sotavoima. — Kuinka paljon väkeä Pontuksella oli. — Lokakuun lopulla lähdetään Viipurista. — Käkisalmen piiritys ja antautuminen. — De la Gardien, joka aikoo mennä Inkeriin, täytyy palata matkalta. — Sotaväen majoittaminen Suomeen. — Pontus suosittelee auttajiansa kuninkaalle. — Arvi Stålarmin aikaisemmat vaiheet. — Julius Gyllenhielmin ja Antti Sabelfanan kuolema. KAHDESKYMMENES LUKU. Ruotsalaisten voitot Virossa ja lnkerissä v. 1581. Siv. 300336. Ruotsalaiset valloittavat Paadisten linnan. — Rutto. — Suomessa olevaa sotaväkeä varten otetaan lisäveroja. — Pontus kokoo täällä keskitalvella 3,000 tai 4,000 miestä ja lähtee jäämatkaa Viroon. — Rakveren ja Toolsenlinnan valloitus. — Ruotsalaiset tekevät Rakverestä ja Tallinnasta ryöstöja tiedusteluietkiä. — Lännenmaan linnojen valloitus. Lorichs lähtee Narvaa valloittamaan. — Pontus valmistaa syyskesällä uutta sotaretkeä. — Hänelle annettu',\n","  'sentences': ['XIII — Viipuriin jäävä varusväki. —',\n","   'Tallinnassa vähemmin sotaväkeä ja muonavaroja kuin Hornille oli luvattu. —',\n","   'Horn lähtee piirittämään Narvaa. —',\n","   'Laivasto, jonka piti tuoda sinne järeät tykit.',\n","   'ei saavukaan.',\n","   'Pitkälliset sateet ja muonavarojen puute heikontavat väkeä.',\n","   'Syyskuun viime päivinä lopetetaan piiritys. —',\n","   'Paluumatka vaikea. —',\n","   'Sotajoukko hajaantumistilassa. —',\n","   'Huovit saavat linnaleirin Suomessa. —',\n","   'Neuvoskunta tutkii Hornin syyllisyyttä.',\n","   'KAHDEKSASTOISTA LUKU.',\n","   'Puolan sekaantuminen sotaan.',\n","   'Siv.',\n","   '262272.',\n","   'Bathori Puolan kuninkaana. —',\n","   'Kysymys Ruotsin ja Puolan välisestä liitosta. —',\n","   'Lorichs Juhani kuninkaan asiamiehenä Puolassa. —',\n","   'Pontus de la Gardie lähettiläänä Saksassa ja paavin luona. —',\n","   'Possevino tulee Ruotsiin. —',\n","   'Vennon tappelu. —',\n","   'Bathori sotii Venäjällä. —',\n","   'Jam Sapolskin rauha. —',\n","   'Puola saa haltuunsa Liivinmaan linnat. —',\n","   'Ruotsin ja Puolan välit huononevat.',\n","   'YHDEKSÄSTOISTA LUKU.',\n","   'Pontus de la Gardie ja Käkisalmen valloitus.',\n","   'Siv.',\n","   '273299.',\n","   'Juhani III käskee Yrjö Boijen lähteä valloittamaan Käkisalmea. —',\n","   'Linnaleiri Suomessa. —',\n","   'Kesällä (1580) ei saada mitään toimeen. —',\n","   'Pontus de la Gardie määrätään ylipäälliköksi. —',\n","   'Hänen aikaisemmat vaiheensa. —',\n","   'Pontuksella lavea valta. —',\n","   'Hänen alapäällikkönsä. —',\n","   'Ruotsin sotavoima. —',\n","   'Kuinka paljon väkeä Pontuksella oli. —',\n","   'Lokakuun lopulla lähdetään Viipurista. —',\n","   'Käkisalmen piiritys ja antautuminen. —',\n","   'De la Gardien, joka aikoo mennä Inkeriin, täytyy palata matkalta. —',\n","   'Sotaväen majoittaminen Suomeen. —',\n","   'Pontus suosittelee auttajiansa kuninkaalle. —',\n","   'Arvi Stålarmin aikaisemmat vaiheet. —',\n","   'Julius Gyllenhielmin ja Antti Sabelfanan kuolema.',\n","   'KAHDESKYMMENES LUKU.',\n","   'Ruotsalaisten voitot Virossa ja lnkerissä v. 1581.',\n","   'Siv.',\n","   '300336.',\n","   'Ruotsalaiset valloittavat Paadisten linnan. —',\n","   'Rutto. —',\n","   'Suomessa olevaa sotaväkeä varten otetaan lisäveroja. —',\n","   'Pontus kokoo täällä keskitalvella 3,000 tai 4,000 miestä ja lähtee jäämatkaa Viroon. —',\n","   'Rakveren ja Toolsenlinnan valloitus. —',\n","   'Ruotsalaiset tekevät Rakverestä ja Tallinnasta ryöstöja tiedusteluietkiä. —',\n","   'Lännenmaan linnojen valloitus.',\n","   'Lorichs lähtee Narvaa valloittamaan. —',\n","   'Pontus valmistaa syyskesällä uutta sotaretkeä. —',\n","   'Hänelle annettu'],\n","  'page_sentence_count_spacy': 59,\n","  'sentence_chunks': [['XIII — Viipuriin jäävä varusväki. —',\n","    'Tallinnassa vähemmin sotaväkeä ja muonavaroja kuin Hornille oli luvattu. —',\n","    'Horn lähtee piirittämään Narvaa. —',\n","    'Laivasto, jonka piti tuoda sinne järeät tykit.',\n","    'ei saavukaan.',\n","    'Pitkälliset sateet ja muonavarojen puute heikontavat väkeä.',\n","    'Syyskuun viime päivinä lopetetaan piiritys. —',\n","    'Paluumatka vaikea. —',\n","    'Sotajoukko hajaantumistilassa. —',\n","    'Huovit saavat linnaleirin Suomessa. —'],\n","   ['Neuvoskunta tutkii Hornin syyllisyyttä.',\n","    'KAHDEKSASTOISTA LUKU.',\n","    'Puolan sekaantuminen sotaan.',\n","    'Siv.',\n","    '262272.',\n","    'Bathori Puolan kuninkaana. —',\n","    'Kysymys Ruotsin ja Puolan välisestä liitosta. —',\n","    'Lorichs Juhani kuninkaan asiamiehenä Puolassa. —',\n","    'Pontus de la Gardie lähettiläänä Saksassa ja paavin luona. —',\n","    'Possevino tulee Ruotsiin. —'],\n","   ['Vennon tappelu. —',\n","    'Bathori sotii Venäjällä. —',\n","    'Jam Sapolskin rauha. —',\n","    'Puola saa haltuunsa Liivinmaan linnat. —',\n","    'Ruotsin ja Puolan välit huononevat.',\n","    'YHDEKSÄSTOISTA LUKU.',\n","    'Pontus de la Gardie ja Käkisalmen valloitus.',\n","    'Siv.',\n","    '273299.',\n","    'Juhani III käskee Yrjö Boijen lähteä valloittamaan Käkisalmea. —'],\n","   ['Linnaleiri Suomessa. —',\n","    'Kesällä (1580) ei saada mitään toimeen. —',\n","    'Pontus de la Gardie määrätään ylipäälliköksi. —',\n","    'Hänen aikaisemmat vaiheensa. —',\n","    'Pontuksella lavea valta. —',\n","    'Hänen alapäällikkönsä. —',\n","    'Ruotsin sotavoima. —',\n","    'Kuinka paljon väkeä Pontuksella oli. —',\n","    'Lokakuun lopulla lähdetään Viipurista. —',\n","    'Käkisalmen piiritys ja antautuminen. —'],\n","   ['De la Gardien, joka aikoo mennä Inkeriin, täytyy palata matkalta. —',\n","    'Sotaväen majoittaminen Suomeen. —',\n","    'Pontus suosittelee auttajiansa kuninkaalle. —',\n","    'Arvi Stålarmin aikaisemmat vaiheet. —',\n","    'Julius Gyllenhielmin ja Antti Sabelfanan kuolema.',\n","    'KAHDESKYMMENES LUKU.',\n","    'Ruotsalaisten voitot Virossa ja lnkerissä v. 1581.',\n","    'Siv.',\n","    '300336.',\n","    'Ruotsalaiset valloittavat Paadisten linnan. —'],\n","   ['Rutto. —',\n","    'Suomessa olevaa sotaväkeä varten otetaan lisäveroja. —',\n","    'Pontus kokoo täällä keskitalvella 3,000 tai 4,000 miestä ja lähtee jäämatkaa Viroon. —',\n","    'Rakveren ja Toolsenlinnan valloitus. —',\n","    'Ruotsalaiset tekevät Rakverestä ja Tallinnasta ryöstöja tiedusteluietkiä. —',\n","    'Lännenmaan linnojen valloitus.',\n","    'Lorichs lähtee Narvaa valloittamaan. —',\n","    'Pontus valmistaa syyskesällä uutta sotaretkeä. —',\n","    'Hänelle annettu']],\n","  'num_chunks': 6}]"]},"metadata":{},"execution_count":22}],"source":["# Sample an example from the group (note: many samples have only 1 chunk as they have <=10 sentences total)\n","random.sample(pages_and_texts, k=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"DvsTtXg4yTHA","outputId":"171be13b-85f0-4ba8-8f60-b414138c3f65","executionInfo":{"status":"ok","timestamp":1734539283769,"user_tz":-120,"elapsed":328,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n","count       806.00           806.00           806.00                   806.00   \n","mean        361.50          1896.07           248.54                    24.98   \n","std         232.82           390.71            49.97                     9.28   \n","min         -41.00             0.00             1.00                     1.00   \n","25%         160.25          1854.00           240.00                    19.00   \n","50%         361.50          1979.00           258.00                    24.00   \n","75%         562.75          2100.75           275.00                    30.00   \n","max         764.00          2727.00           362.00                    76.00   \n","\n","       page_token_count  page_sentence_count_spacy  num_chunks  \n","count            806.00                     806.00      806.00  \n","mean             474.02                      18.53        2.28  \n","std               97.68                       7.35        0.78  \n","min                0.00                       0.00        0.00  \n","25%              463.50                      15.00        2.00  \n","50%              494.75                      18.00        2.00  \n","75%              525.19                      21.00        3.00  \n","max              681.75                      65.00        7.00  "],"text/html":["\n","  <div id=\"df-1f4a277a-b48e-4a35-9ca3-6fa28cf753f6\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>page_number</th>\n","      <th>page_char_count</th>\n","      <th>page_word_count</th>\n","      <th>page_sentence_count_raw</th>\n","      <th>page_token_count</th>\n","      <th>page_sentence_count_spacy</th>\n","      <th>num_chunks</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>806.00</td>\n","      <td>806.00</td>\n","      <td>806.00</td>\n","      <td>806.00</td>\n","      <td>806.00</td>\n","      <td>806.00</td>\n","      <td>806.00</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>361.50</td>\n","      <td>1896.07</td>\n","      <td>248.54</td>\n","      <td>24.98</td>\n","      <td>474.02</td>\n","      <td>18.53</td>\n","      <td>2.28</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>232.82</td>\n","      <td>390.71</td>\n","      <td>49.97</td>\n","      <td>9.28</td>\n","      <td>97.68</td>\n","      <td>7.35</td>\n","      <td>0.78</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>-41.00</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>160.25</td>\n","      <td>1854.00</td>\n","      <td>240.00</td>\n","      <td>19.00</td>\n","      <td>463.50</td>\n","      <td>15.00</td>\n","      <td>2.00</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>361.50</td>\n","      <td>1979.00</td>\n","      <td>258.00</td>\n","      <td>24.00</td>\n","      <td>494.75</td>\n","      <td>18.00</td>\n","      <td>2.00</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>562.75</td>\n","      <td>2100.75</td>\n","      <td>275.00</td>\n","      <td>30.00</td>\n","      <td>525.19</td>\n","      <td>21.00</td>\n","      <td>3.00</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>764.00</td>\n","      <td>2727.00</td>\n","      <td>362.00</td>\n","      <td>76.00</td>\n","      <td>681.75</td>\n","      <td>65.00</td>\n","      <td>7.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f4a277a-b48e-4a35-9ca3-6fa28cf753f6')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-1f4a277a-b48e-4a35-9ca3-6fa28cf753f6 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-1f4a277a-b48e-4a35-9ca3-6fa28cf753f6');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-148aa6a1-2397-4a59-b9e9-72dded97a5af\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-148aa6a1-2397-4a59-b9e9-72dded97a5af')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-148aa6a1-2397-4a59-b9e9-72dded97a5af button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 294.396873369946,\n        \"min\": -41.0,\n        \"max\": 806.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          806.0,\n          361.5,\n          562.75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 951.136529097652,\n        \"min\": 0.0,\n        \"max\": 2727.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1896.07,\n          1979.0,\n          806.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 244.1467649905629,\n        \"min\": 1.0,\n        \"max\": 806.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          248.54,\n          258.0,\n          806.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_raw\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 276.55618399728985,\n        \"min\": 1.0,\n        \"max\": 806.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          24.98,\n          24.0,\n          806.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 271.26188186945313,\n        \"min\": 0.0,\n        \"max\": 806.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          474.02,\n          494.75,\n          806.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_spacy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 278.31810849149883,\n        \"min\": 0.0,\n        \"max\": 806.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          18.53,\n          18.0,\n          806.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_chunks\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 284.10997275954355,\n        \"min\": 0.0,\n        \"max\": 806.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          806.0,\n          2.28,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":23}],"source":["# Create a DataFrame to get stats\n","df = pd.DataFrame(pages_and_texts)\n","df.describe().round(2)"]},{"cell_type":"markdown","metadata":{"id":"8qFlMknmyTHA"},"source":["Note how the average number of chunks is around 1.5, this is expected since many of our pages only contain an average of 10 sentences."]},{"cell_type":"markdown","metadata":{"id":"brghqBQ9yTHB"},"source":["### Splitting each chunk into its own item\n","\n","We'd like to embed each chunk of sentences into its own numerical representation.\n","\n","So to keep things clean, let's create a new list of dictionaries each containing a single chunk of sentences with relative information such as page number as well statistics about each chunk."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["05c51eefd14146a8ba40a608f133fd36","c6f66b75d4ef48059f7bb0f6f4ba7439","b8796628f457451ba54a37ce814b775b","6e0cf4fe1370470392ebe4d9a1932659","034a55be84424bb082b92c179ceac4a6","8ff68361a8454779b45f16a2e7f51025","19f2807d12a14f2cb4df2f79b4139765","89c9bc5fe9884541a9a6e19808b16f11","6e7c43a023fa4305861f5b725291d503","b3ac78ee969745659c758e63de4356a9","35e9e72addf649cda6830f95aa4a7415"]},"id":"matjnPMLyTHB","outputId":"7a2864ad-47aa-4c71-c751-d7369c6163a2","executionInfo":{"status":"ok","timestamp":1734539286509,"user_tz":-120,"elapsed":456,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/806 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05c51eefd14146a8ba40a608f133fd36"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["1835"]},"metadata":{},"execution_count":24}],"source":["import re\n","\n","# Split each chunk into its own item\n","pages_and_chunks = []\n","for item in tqdm(pages_and_texts):\n","    for sentence_chunk in item[\"sentence_chunks\"]:\n","        chunk_dict = {}\n","        chunk_dict[\"page_number\"] = item[\"page_number\"]\n","\n","        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n","        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n","        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo\n","        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n","\n","        # Get stats about the chunk\n","        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n","        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n","        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n","\n","        pages_and_chunks.append(chunk_dict)\n","\n","# How many chunks do we have?\n","len(pages_and_chunks)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cc1vpR7CyTHB","outputId":"8c3677d0-b76c-494a-cbd2-0b0422a8ef9f","executionInfo":{"status":"ok","timestamp":1734539289252,"user_tz":-120,"elapsed":270,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'page_number': 726,\n","  'sentence_chunk': '746 Badenin rajakreivin puoliso Cecilia (Ruotsin kuninkaan sisar), SaksiLauenburgin herttua, Rostokin kaupungin neuvosto y. m. varustivat laivoja ryöstölle. Täytyy kummastella, että kauppiaat ensinkään uskalsivat lähettää aluksia pitkille matkoille. Maamatkat olivat myöskin hankaloita, sillä tiet olivat Suomessa siihen aikaan sangen puutteelliset. Maamme eteläosissa oli kuitenkin maanteitä, joilla voitiin käyttää rattaita. Tämän saattaa päättää muun muassa siitä, että Juhani III vuonna 1587, jolloin hän oli aikeissa tulla hoveinensa Suomeen, käski korjata yleisiä teitä, niin että niitä kävi kulkeminen vaunuilla 1. Tavallisesti liikuttiin kuitenkin ratsain. Kyytimies seurasi matkustajaa eri hevosella. Kun sotaherrain ja muiden viranomaisten täytyi matkalla sattuvien vaikeuksien tähden pitää mukanansa muutamia palvelijoita, niin oli heitä miltei joka kerta kyydittävä useilla hevosilla. Kaksi valtatietä, jotka kulkivat Turusta Viipuriin, toinen Suomen etelärannikkoa pitkin, toinen Hämeenlinnan ja Hollolan Lahden kautta, olivat sotaväen liikkeisiin nähden ja muutoinkin tärkeimmät. Paitse näitä ja niiden jatkoa, jota myöten päästiin Viipurista Inkeriin, oli vielä pari yleistä tietä, joiden pääsuunta kävi lännestä itään.',\n","  'chunk_char_count': 1234,\n","  'chunk_word_count': 153,\n","  'chunk_token_count': 308.5}]"]},"metadata":{},"execution_count":25}],"source":["# View a random sample\n","random.sample(pages_and_chunks, k=1)"]},{"cell_type":"markdown","metadata":{"id":"8uBn0P1zyTHB"},"source":["Excellent!\n","\n","Now we've broken our whole textbook into chunks of 10 sentences or less as well as the page number they came from.\n","\n","This means we could reference a chunk of text and know its source.\n","\n","Let's get some stats about our chunks."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"g7pMN8JDyTHB","outputId":"ab1dfbe1-b3ed-4f2e-cc59-10b6efcd92cd","executionInfo":{"status":"ok","timestamp":1734539291657,"user_tz":-120,"elapsed":278,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       page_number  chunk_char_count  chunk_word_count  chunk_token_count\n","count      1835.00           1835.00           1835.00            1835.00\n","mean        354.29            828.96            105.87             207.24\n","std         239.31            494.55             60.89             123.64\n","min         -40.00              2.00              1.00               0.50\n","25%         144.50            400.00             55.00             100.00\n","50%         349.00            801.00            104.00             200.25\n","75%         561.00           1227.00            153.00             306.75\n","max         764.00           2105.00            269.00             526.25"],"text/html":["\n","  <div id=\"df-c26677d7-3957-458c-aa07-85d413061c2a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>page_number</th>\n","      <th>chunk_char_count</th>\n","      <th>chunk_word_count</th>\n","      <th>chunk_token_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>1835.00</td>\n","      <td>1835.00</td>\n","      <td>1835.00</td>\n","      <td>1835.00</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>354.29</td>\n","      <td>828.96</td>\n","      <td>105.87</td>\n","      <td>207.24</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>239.31</td>\n","      <td>494.55</td>\n","      <td>60.89</td>\n","      <td>123.64</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>-40.00</td>\n","      <td>2.00</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>144.50</td>\n","      <td>400.00</td>\n","      <td>55.00</td>\n","      <td>100.00</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>349.00</td>\n","      <td>801.00</td>\n","      <td>104.00</td>\n","      <td>200.25</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>561.00</td>\n","      <td>1227.00</td>\n","      <td>153.00</td>\n","      <td>306.75</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>764.00</td>\n","      <td>2105.00</td>\n","      <td>269.00</td>\n","      <td>526.25</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c26677d7-3957-458c-aa07-85d413061c2a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-c26677d7-3957-458c-aa07-85d413061c2a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-c26677d7-3957-458c-aa07-85d413061c2a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-97199cf0-65d6-4440-9352-41028d1e9cfb\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-97199cf0-65d6-4440-9352-41028d1e9cfb')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-97199cf0-65d6-4440-9352-41028d1e9cfb button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 583.1261354544143,\n        \"min\": -40.0,\n        \"max\": 1835.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          354.29,\n          349.0,\n          1835.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 720.9494106570763,\n        \"min\": 2.0,\n        \"max\": 2105.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          828.96,\n          801.0,\n          1835.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 616.1256923944752,\n        \"min\": 1.0,\n        \"max\": 1835.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          105.87,\n          104.0,\n          1835.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 595.896735731416,\n        \"min\": 0.5,\n        \"max\": 1835.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          207.24,\n          200.25,\n          1835.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":26}],"source":["# Get stats about our chunks\n","df = pd.DataFrame(pages_and_chunks)\n","df.describe().round(2)"]},{"cell_type":"markdown","metadata":{"id":"gWEFOxyPyTHC"},"source":["Hmm looks like some of our chunks have quite a low token count.\n","\n","How about we check for samples with less than 30 tokens (about the length of a sentence) and see if they are worth keeping?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kkqEa-aRyTHC","outputId":"310f2f3f-9fa3-4173-a4b1-d2cefc30456f","executionInfo":{"status":"ok","timestamp":1734539293825,"user_tz":-120,"elapsed":305,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Chunk token count: 3.75 | Text: V. R. 8/x 1577.\n","Chunk token count: 13.75 | Text: Vrt. V. R. Kirje KIaus Aakenpojalle lokakuun G p. 1573.\n","Chunk token count: 23.0 | Text: on tehnyt sen. Siinä tapauksessa hän olikin se päällikkö, joka toimitti jalkaväen Viipuriin.\n","Chunk token count: 12.0 | Text: Sven Elavinpoika 48. Sven Hakuninpoika 466, 593.\n","Chunk token count: 16.5 | Text: V. R. Vastaus Pontus herralle huhtikuun 1 p. 1582.2 Kts.sivua 269.\n"]}],"source":["# Show random chunks with under 30 tokens in length\n","min_token_length = 30\n","for row in df[df[\"chunk_token_count\"] <= min_token_length].sample(5).iterrows():\n","    print(f'Chunk token count: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')"]},{"cell_type":"markdown","metadata":{"id":"jk4ZREDFyTHC"},"source":["Looks like many of these are headers and footers of different pages.\n","\n","They don't seem to offer too much information.\n","\n","Let's filter our DataFrame/list of dictionaries to only include chunks with over 30 tokens in length."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-RMWkbskyTHC","outputId":"922dc85c-50a3-4ebc-bc92-c4309f0dcb9c","executionInfo":{"status":"ok","timestamp":1734539295559,"user_tz":-120,"elapsed":198,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'page_number': -40,\n","  'sentence_chunk': 'HISTORIALLISIA TUTKIMCIKSIA JULKAISSUT SUOMEN HISTORIALLINEN SECiRR I WERNER TRWASTST)ERNR POHJOISMAIDEN VIISIKOLMATTAVUOTINEN SOTA',\n","  'chunk_char_count': 131,\n","  'chunk_word_count': 12,\n","  'chunk_token_count': 32.75},\n"," {'page_number': -39,\n","  'sentence_chunk': 'POHJOISMRIbEN VIISI KOLMATTAVUOTIIYEIY SOTA VUOSIEN 1570 )A 1590 VÄLINEN AIKA. KIRJOITTANUT WERNER TAWASTST)ERNA SUOM. KIR). SEURAN KIRJAPAINO JA HELSINGIN SENTRRLIKIRJRPRINO 19181920.',\n","  'chunk_char_count': 184,\n","  'chunk_word_count': 21,\n","  'chunk_token_count': 46.0}]"]},"metadata":{},"execution_count":28}],"source":["pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n","pages_and_chunks_over_min_token_len[:2]"]},{"cell_type":"markdown","metadata":{"id":"AKdZfvlOyTHD"},"source":["Smaller chunks filtered!\n","\n","Time to embed our chunks of text!"]},{"cell_type":"markdown","metadata":{"id":"wZr6_pU4yTHD"},"source":["### Embedding our text chunks\n","\n","While humans understand text, machines understand numbers best.\n","\n","An [embedding](https://vickiboykis.com/what_are_embeddings/index.html) is a broad concept.\n","\n","But one of my favourite and simple definitions is \"a useful numerical representation\".\n","\n","The most powerful thing about modern embeddings is that they are *learned* representations.\n","\n","Meaning rather than directly mapping words/tokens/characters to numbers directly (e.g. `{\"a\": 0, \"b\": 1, \"c\": 3...}`), the numerical representation of tokens is learned by going through large corpuses of text and figuring out how different tokens relate to each other.\n","\n","Ideally, embeddings of text will mean that similar meaning texts have similar numerical representation.\n","\n","> **Note:** Most modern NLP models deal with \"tokens\" which can be considered as multiple different sizes and combinations of words and characters rather than always whole words or single characters. For example, the string `\"hello world!\"` gets mapped to the token values `{15339: b'hello', 1917: b' world', 0: b'!'}` using [Byte pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding) (or BPE via OpenAI's [`tiktoken`](https://github.com/openai/tiktoken) library). Google has a tokenization library called [SentencePiece](https://github.com/google/sentencepiece).\n","\n","Our goal is to turn each of our chunks into a numerical representation (an embedding vector, where a vector is a sequence of numbers arranged in order).\n","\n","Once our text samples are in embedding vectors, us humans will no longer be able to understand them.\n","\n","However, we don't need to.\n","\n","The embedding vectors are for our computers to understand.\n","\n","We'll use our computers to find patterns in the embeddings and then we can use their text mappings to further our understanding.\n","\n","Enough talking, how about we import a text embedding model and see what an embedding looks like.\n","\n","To do so, we'll use the [`sentence-transformers`](https://www.sbert.net/docs/installation.html) library which contains many pre-trained embedding models.\n","\n","Specifically, we'll get the `all-mpnet-base-v2` model (you can see the model's intended use on the [Hugging Face model card](https://huggingface.co/sentence-transformers/all-mpnet-base-v2#intended-uses))."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"sbpYrmFdyTHD","outputId":"c4f90092-e05a-466d-f1db-c0566266951b","executionInfo":{"status":"error","timestamp":1734539302812,"user_tz":-120,"elapsed":3484,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.5.1)\n","Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.38.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.26.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.14.1)\n","Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2024.9.11)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.5)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.8.30)\n"]},{"output_type":"error","ename":"ImportError","evalue":"scipy.special._ufuncs_cxx does not export expected C variable _export_beta_pdf_float","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-f44ad8d25d24>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#TurkuNLP/sbert-uncased-finnish-paraphrase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Tämä on esimerkkilause.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tämä on toinen lause.\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.5.1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0m__MODEL_HUB_ORGANIZATION__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sentence-transformers\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentencesDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParallelSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mLoggingHandler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoggingHandler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSentenceTransformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mDenoisingAutoEncoderDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDenoisingAutoEncoderDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mNoDuplicatesDataLoader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNoDuplicatesDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mParallelSentencesDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallelSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSentencesDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSentenceLabelDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceLabelDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/datasets/ParallelSentencesDataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputExample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__MODEL_HUB_ORGANIZATION__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m from .util import (\n\u001b[1;32m     26\u001b[0m     \u001b[0mimport_from_string\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/evaluation/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSentenceEvaluator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSimilarityFunction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimilarityFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mBinaryClassificationEvaluator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBinaryClassificationEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mEmbeddingSimilarityEvaluator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbeddingSimilarityEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mInformationRetrievalEvaluator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInformationRetrievalEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/evaluation/BinaryClassificationEvaluator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpaired_cosine_distances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaired_euclidean_distances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaired_manhattan_distances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maverage_precision_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0m_distributor_init\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     )\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HTMLDocumentationLinkMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidate_parameter_constraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_routing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bunch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_chunking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_chunking.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_is_arraylike_not_scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_get_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPositiveSpectrumWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_is_numpy_namespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_preserve_dia_indices_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_isfinite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFiniteStatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcy_isfinite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0m_NUMPY_NAMESPACE_NAMES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array_api_compat.numpy\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    608\u001b[0m from ._warnings_errors import (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    609\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 610\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_stats_py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_variation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# See https://github.com/scipy/scipy/issues/15765#issuecomment-1875564522\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_mstats_basic\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_distn_infrastructure\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrv_discrete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrv_continuous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrv_frozen\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_continuous_distns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_discrete_distns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_continuous_distns.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_lazyselect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_lazywhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m from ._tukeylambda_stats import (tukeylambda_variance as _tlvar,\n\u001b[1;32m     26\u001b[0m                                  tukeylambda_kurtosis as _tlkurt)\n","\u001b[0;32m_stats.pyx\u001b[0m in \u001b[0;36minit scipy.stats._stats\u001b[0;34m()\u001b[0m\n","\u001b[0;32mcython_special.pyx\u001b[0m in \u001b[0;36minit scipy.special.cython_special\u001b[0;34m()\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: scipy.special._ufuncs_cxx does not export expected C variable _export_beta_pdf_float","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["# Requires\n","!pip install sentence-transformers\n","\n","#\"SystemError: initialization of _pywrap_checkpoint_reader raised unreported exception\"  and \"AttributeError: _ARRAY_API not found\"  before setting numpy version 1.9.0:\n","#A module that was compiled using NumPy 1.x cannot be run in\n","#NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n","#versions of NumPy, modules must be compiled with NumPy 2.0.\n","#Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n","\n","#If you are a user of the module, the easiest solution will be to\n","#downgrade to 'numpy<2' or try to upgrade the affected module.\n","\n","\n","#!pip uninstall numpy\n","#!pip install numpy==1.2.2\n","#!pip install sentence-transformers\n","\n","#TurkuNLP/sbert-uncased-finnish-paraphrase\n","\n","from sentence_transformers import SentenceTransformer\n","sentences = [\"Tämä on esimerkkilause.\", \"Tämä on toinen lause.\"]\n","\n","embedding_model = SentenceTransformer('TurkuNLP/sbert-uncased-finnish-paraphrase')\n","model = embedding_model\n","embeddings = model.encode(sentences)\n","print(embeddings)\n","\n","#from sentence_transformers import SentenceTransformer\n","#embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n","#                                      device=\"cpu\") # choose the device to load the model to (note: GPU will often be *much* faster than CPU)\n","\n","# Create a list of sentences to turn into numbers\n","#sentences = [\n","#    \"The Sentences Transformers library provides an easy and open-source way to create embeddings.\",\n","#    \"Sentences can be embedded one by one or as a list of strings.\",\n","#    \"Embeddings are one of the most powerful concepts in machine learning!\",\n","#    \"Learn to use embeddings well and you'll be well on your way to being an AI engineer.\"\n","#]\n","\n","\n","# Sentences are encoded/embedded by calling model.encode()\n","embeddings = embedding_model.encode(sentences)\n","embeddings_dict = dict(zip(sentences, embeddings))\n","\n","# See the embeddings\n","for sentence, embedding in embeddings_dict.items():\n","    print(\"Sentence:\", sentence)\n","    print(\"Embedding:\", embedding)\n","    print(\"\")"]},{"cell_type":"markdown","metadata":{"id":"NplLWtakyTHD"},"source":["Woah! That's a lot of numbers.\n","\n","How about we do just once sentence?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"55XTQPM3yTHE","outputId":"28b06ed8-631a-467c-9761-c13da20bb092","executionInfo":{"status":"error","timestamp":1734539316247,"user_tz":-120,"elapsed":228,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'embedding_model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-b800210f853e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msingle_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Yksittäinen lause tässä moi.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msingle_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Sentence: {single_sentence}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Embedding:\\n{single_embedding}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Embedding size: {single_embedding.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'embedding_model' is not defined"]}],"source":["single_sentence = \"Yksittäinen lause tässä moi.\"\n","single_embedding = embedding_model.encode(single_sentence)\n","print(f\"Sentence: {single_sentence}\")\n","print(f\"Embedding:\\n{single_embedding}\")\n","print(f\"Embedding size: {single_embedding.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"faTCtDagyTHE"},"source":["Nice! We've now got a way to numerically represent each of our chunks.\n","\n","Our embedding has a shape of `(768,)` meaning it's a vector of 768 numbers which represent our text in high-dimensional space, too many for a human to comprehend but machines love high-dimensional space.\n","\n","> **Note:** No matter the size of the text input to our `all-mpnet-base-v2` model, it will be turned into an embedding size of `(768,)`. This value is fixed. So whether a sentence is 1 token long or 1000 tokens long, it will be truncated/padded with zeros to size 384 and then turned into an embedding vector of size `(768,)`. Of course, other embedding models may have different input/output shapes.\n","\n","How about we add an embedding field to each of our chunk items?\n","\n","Let's start by trying to create embeddings on the CPU, we'll time it with the `%%time` magic to see how long it takes."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":128},"id":"0GypOFCXyTHE","outputId":"f5a9ae15-9b48-471c-8d39-be820c435257","executionInfo":{"status":"ok","timestamp":1734539318823,"user_tz":-120,"elapsed":283,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'embedding_model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'embedding_model' is not defined"]}],"source":["%%time\n","\n","# Uncomment to see how long it takes to create embeddings on CPU\n","# # Make sure the model is on the CPU\n","embedding_model.to(\"cpu\")\n","\n","# # Embed each chunk one by one\n","for item in tqdm(pages_and_chunks_over_min_token_len):\n","    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"]},{"cell_type":"markdown","metadata":{"id":"K1woGEt-yTHE"},"source":["Ok not too bad... but this would take a *really* long time if we had a larger dataset.\n","\n","Now let's see how long it takes to create the embeddings with a GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":128},"id":"sLD_F-xuyTHE","outputId":"892036fc-d603-4d41-9af2-37358ba8dc16","executionInfo":{"status":"ok","timestamp":1734539321138,"user_tz":-120,"elapsed":475,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'embedding_model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'embedding_model' is not defined"]}],"source":["%%time\n","\n","# Send the model to the GPU\n","embedding_model.to(\"cuda\") # requires a GPU installed, for reference on my local machine, I'm using a NVIDIA RTX 4090\n","\n","# Create embeddings one by one on the GPU\n","for item in tqdm(pages_and_chunks_over_min_token_len):\n","    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"]},{"cell_type":"markdown","metadata":{"id":"sKz-N23cyTHE"},"source":["Woah! Looks like the embeddings get created much faster (~10x faster on my machine) on the GPU!\n","\n","You'll likely notice this trend with many of your deep learning workflows. If you have access to a GPU, especially a NVIDIA GPU, you should use one if you can.\n","\n","But what if I told you we could go faster again?\n","\n","You see many modern models can handle batched predictions.\n","\n","This means computing on multiple samples at once.\n","\n","Those are the types of operations where a GPU flourishes!\n","\n","We can perform batched operations by turning our target text samples into a single list and then passing that list to our embedding model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZVm0Q4ryTHF"},"outputs":[],"source":["# Turn text chunks into a single list\n","text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":128},"id":"Q04chNCSyTHF","outputId":"1a876f43-9f09-4b00-ae73-eb9ba0f56760","executionInfo":{"status":"ok","timestamp":1734539324529,"user_tz":-120,"elapsed":6,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'embedding_model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'embedding_model' is not defined"]}],"source":["%%time\n","\n","# Embed all texts in batches\n","text_chunk_embeddings = embedding_model.encode(text_chunks,\n","                                               batch_size=32, # you can use different batch sizes here for speed/performance, I found 32 works well for this use case\n","                                               convert_to_tensor=True) # optional to return embeddings as tensor instead of array\n","\n","text_chunk_embeddings"]},{"cell_type":"markdown","metadata":{"id":"S_2pEMeYyTHF"},"source":["That's what I'm talking about!\n","\n","A ~4x improvement (on my GPU) in speed thanks to batched operations.\n","\n","So the tip here is to use a GPU when you can and use batched operations if you can too.\n","\n","Now let's save our chunks and their embeddings so we could import them later if we wanted."]},{"cell_type":"markdown","metadata":{"id":"nND7Gy-ZyTHF"},"source":["### Save embeddings to file\n","\n","Since creating embeddings can be a timely process (not so much for our case but it can be for more larger datasets), let's turn our `pages_and_chunks_over_min_token_len` list of dictionaries into a DataFrame and save it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5GalHE9uyTHF"},"outputs":[],"source":["# Save embeddings to file\n","text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)\n","embeddings_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n","text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"]},{"cell_type":"markdown","metadata":{"id":"NH8LmgfMyTHF"},"source":["And we can make sure it imports nicely by loading it."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"OPJ9Y1UpyTHG","outputId":"d86a7ea9-f9cf-4055-c3d2-70c751d294b4","executionInfo":{"status":"ok","timestamp":1734539329929,"user_tz":-120,"elapsed":344,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   page_number                                     sentence_chunk  \\\n","0          -40  HISTORIALLISIA TUTKIMCIKSIA JULKAISSUT SUOMEN ...   \n","1          -39  POHJOISMRIbEN VIISI KOLMATTAVUOTIIYEIY SOTA VU...   \n","2          -37  POHJOISMflIbEN VIISI KOLMATTAVUOTI h Eh SOTA V...   \n","3          -35  Esipuhe. Jo kauan aikaa sitten aloin kirjoitta...   \n","4          -34  VI Teoksen alussa on lähteet muutamissa kohdin...   \n","\n","   chunk_char_count  chunk_word_count  chunk_token_count  \n","0               131                12              32.75  \n","1               184                21              46.00  \n","2               185                23              46.25  \n","3              1455               172             363.75  \n","4               850               106             212.50  "],"text/html":["\n","  <div id=\"df-14ffd608-4565-45b0-a329-4c787e070f82\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>page_number</th>\n","      <th>sentence_chunk</th>\n","      <th>chunk_char_count</th>\n","      <th>chunk_word_count</th>\n","      <th>chunk_token_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-40</td>\n","      <td>HISTORIALLISIA TUTKIMCIKSIA JULKAISSUT SUOMEN ...</td>\n","      <td>131</td>\n","      <td>12</td>\n","      <td>32.75</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-39</td>\n","      <td>POHJOISMRIbEN VIISI KOLMATTAVUOTIIYEIY SOTA VU...</td>\n","      <td>184</td>\n","      <td>21</td>\n","      <td>46.00</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-37</td>\n","      <td>POHJOISMflIbEN VIISI KOLMATTAVUOTI h Eh SOTA V...</td>\n","      <td>185</td>\n","      <td>23</td>\n","      <td>46.25</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-35</td>\n","      <td>Esipuhe. Jo kauan aikaa sitten aloin kirjoitta...</td>\n","      <td>1455</td>\n","      <td>172</td>\n","      <td>363.75</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-34</td>\n","      <td>VI Teoksen alussa on lähteet muutamissa kohdin...</td>\n","      <td>850</td>\n","      <td>106</td>\n","      <td>212.50</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-14ffd608-4565-45b0-a329-4c787e070f82')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-14ffd608-4565-45b0-a329-4c787e070f82 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-14ffd608-4565-45b0-a329-4c787e070f82');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-5fc675f4-ac3f-499b-bd68-77a994e3a280\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5fc675f4-ac3f-499b-bd68-77a994e3a280')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-5fc675f4-ac3f-499b-bd68-77a994e3a280 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"text_chunks_and_embedding_df_load","summary":"{\n  \"name\": \"text_chunks_and_embedding_df_load\",\n  \"rows\": 1705,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 240,\n        \"min\": -40,\n        \"max\": 764,\n        \"num_unique_values\": 801,\n        \"samples\": [\n          659,\n          630,\n          25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence_chunk\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1705,\n        \"samples\": [\n          \"Pentti Juhonpojan kirje Juhani III:lle 19/ix 1584. Pentti S\\u00f6yringinpoika Juustenin kirje Pontus herralle 10/xi 1585. Kaikki n\\u00e4rn\\u00e4t kirjeet l\\u00f6ydet\\u00e4\\u00e4n Gr\\u00f6nbladin kopioista S. A. 3 Siit\\u00e4 Savonlinnan kirjuri Henrikki Kupiainen ilmoitti K\\u00e4kisalmen is\\u00e4nnille.4 Ky\\u00f6tikk\\u00e4 Fincken kirje Juhani Ill:lle 6/Ix 1584 Gr\\u00f6nbladin kokoelmissa S. A. 5 V. R. Kirje Ky\\u00f6tikk\\u00e4 Finckelle elokuun 14 p. 1585. Kirje Pontus herralle sam.p\\u00e4iv.6 R. A. Ky\\u00f6tikk\\u00e4 Fincken kirje Juhani III:Ile 1/v 1586.\",\n          \"Heti ensimm\\u00e4isell\\u00e4 hy\\u00f6kk\\u00e4yksell\\u00e4 saatiinkin (syys kuun 6 p.) sek\\u00e4 kaupunki ett\\u00e4 linna valloitetuksi 3. De la Gardie oli julkisella kuulutuksella antanut kaikille niille, jotka ottivat osaa rynt\\u00e4ykseen, luvan ry\\u00f6st\\u00e4\\u00e4 kaupunkia 24 tunnin aikana 4, ja Ruotsin sotav\\u00e4ki riehui nyt s\\u00e4\\u00e4lim\\u00e4tt\\u00f6m\\u00e4n julmasti kukistetussa Narvassa surmaten 7,000 ihmist\\u00e4, nimitt\\u00e4in 2;000 streletsia, 300 pa,jaria sek\\u00e4 porvareita, vaimoja ja lapsiakin. Ep\\u00e4ilem\\u00e4tt\\u00e4 Pontus ansaitsi moitetta siit\\u00e4, ettei h\\u00e4n osannut 1 Raskaiden piiritystykkien lis\\u00e4ksi oli sotajoukon mukana 18 kentt\\u00e4tykki\\u00e4.2 Klaus Fleming sanoo: nelj\\u00e4lt\\u00e4 taholla, Memoriale Chron.7.3 Hi\\u00e4rn.1.c. 332. \\u2014R. A. Livonica, Arvi Henrikinpojan kirje Juhani IIl:lle 8/I% 1581.4 V. R. Mandat af Herr Pontus 4/1x 1581.\",\n          \"Siin\\u00e4 sanottiin kuninkaan kuulleen, ett\\u00e4 muutamat muukalaiset, jotka olivat kulkeneet Ruotsin vesill\\u00e4, olivat tehneet v\\u00e4kivaltaa h\\u00e4nen alamaisillensa. T\\u00e4t\\u00e4 tuli sotalaivaston est\\u00e4\\u00e4. Amiraalin oli my\\u00f6s ottaminen kaikki ne laivat kiinni, jotka pyrkiv\\u00e4t Narvaan vied\\u00e4kseen ven\\u00e4l\\u00e4isille sotatarpeita. Alukset sek\\u00e4 niiden tavarat olivat pidett\\u00e41 V. R. 27/v 1580.2 Zettersten, Svenska flottans historia, 8.3 Ne olivat: \\u00d6rnen, Falken, Strutsen, Storken, Svalan, Jonas, Svarta Pinkan ja Klosshuggaren.4 V. R. Kirje Eerikki Pertunpojalle kes\\u00e4kuun 29 p. 1580.5 Suomessa majailevien sotalaivojen varustamiseksi otettiin kev\\u00e4\\u00e4ll\\u00e4 v. 1580 sarkaa ja palttinaa lis\\u00e4verona Suomen rannikkopit\\u00e4jist\\u00e4 sek\\u00e4 H\\u00e4meest\\u00e4. Laivoihin tarvittiin n\\u00e4et uudet purjeet. Sit\\u00e4 paitse oli Suomesta maksettava tervavero Tukholmassa olevien sotalaivojen kuntoonpanemiseksi.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 463,\n        \"min\": 121,\n        \"max\": 2105,\n        \"num_unique_values\": 1054,\n        \"samples\": [\n          276,\n          336,\n          1363\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 56,\n        \"min\": 12,\n        \"max\": 269,\n        \"num_unique_values\": 236,\n        \"samples\": [\n          76,\n          238,\n          226\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 115.7783889987371,\n        \"min\": 30.25,\n        \"max\": 526.25,\n        \"num_unique_values\": 1054,\n        \"samples\": [\n          69.0,\n          84.0,\n          340.75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":36}],"source":["# Import saved file and view\n","text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)\n","text_chunks_and_embedding_df_load.head()"]},{"cell_type":"markdown","metadata":{"id":"42qHvY1TyTHG"},"source":["### Chunking and embedding questions\n","\n","> **Which embedding model should I use?**\n","\n","This depends on many factors. My best advice is to experiment, experiment, experiment!\n","\n","If you want the model to run locally, you'll have to make sure it's feasible to run on your own hardware.\n","\n","A good place to see how different models perform on a wide range of embedding tasks is the [Hugging Face Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n","\n","> **What other forms of text chunking/splitting are there?**\n","\n","There are a fair few options here too. We've kept it simple with groups of sentences.\n","\n","For more, [Pinecone has a great guide on different kinds of chunking](https://www.pinecone.io/learn/chunking-strategies/) including for different kinds of data such as markdown and LaTeX.\n","\n","Libraries such as [LangChain also have a good amount of in-built text splitting options](https://python.langchain.com/docs/modules/data_connection/document_transformers/).\n","\n","> **What should I think about when creating my embeddings?**\n","\n","Our model turns text inputs up to 384 tokens long in embedding vectors of size 768.\n","\n","Generally, the larger the vector size, the more information that gets encoded into the embedding (however, this is not always the case, as smaller, better models can outperform larger ones).\n","\n","Though with larger vector sizes comes larger storage and compute requirements.\n","\n","Our model is also relatively small (420MB) in size compared to larger models that are available.\n","\n","Larger models may result in better performance but will also require more compute.\n","\n","So some things to think about:\n","* Size of input - If you need to embed longer sequences, choose a model with a larger input capacity.\n","* Size of embedding vector - Larger is generally a better representation but requires more compute/storage.\n","* Size of model - Larger models generally result in better embeddings but require more compute power/time to run.\n","* Open or closed - Open models allow you to run them on your own hardware whereas closed models can be easier to setup but require an API call to get embeddings.\n","\n","> **Where should I store my embeddings?**\n","\n","If you've got a relatively small dataset, for example, under 100,000 examples (this number is rough and only based on first hand experience), `np.array` or `torch.tensor` can work just fine as your dataset.\n","\n","But if you've got a production system and want to work with 100,000+ embeddings, you may want to look into a [vector database]( https://en.wikipedia.org/wiki/Vector_database) (these have become very popular lately and there are many offerings).\n","\n","### Document Ingestion and Embedding Creation Extensions\n","\n","One major extension to the workflow above would to functionize it.\n","\n","Or turn it into a script.\n","\n","As in, take all the functionality we've created and package it into a single process (e.g. go from document -> embeddings file).\n","\n","So you could input a document on one end and have embeddings come out the other end. The hardest part of this is knowing what kind of preprocessing your text may need before it's turned into embeddings. Cleaner text generally means better results.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gQMYBG_eyTHG"},"source":["## 2. RAG - Search and Answer\n","\n","We discussed RAG briefly in the beginning but let's quickly recap.\n","\n","RAG stands for Retrieval Augmented Generation.\n","\n","Which is another way of saying \"given a query, search for relevant resources and answer based on those resources\".\n","\n","Let's breakdown each step:\n","* **Retrieval** - Get relevant resources given a query. For example, if the query is \"what are the macronutrients?\" the ideal results will contain information about protein, carbohydrates and fats (and possibly alcohol) rather than information about which tractors are the best for farming (though that is also cool information).\n","* **Augmentation** - LLMs are capable of generating text given a prompt. However, this generated text is designed to *look* right. And it often has some correct information, however, they are prone to hallucination (generating a result that *looks* like legit text but is factually wrong). In augmentation, we pass relevant information into the prompt and get an LLM to use that relevant information as the basis of its generation.\n","* **Generation** - This is where the LLM will generate a response that has been flavoured/augmented with the retrieved resources. In turn, this not only gives us a potentially more correct answer, it also gives us resources to investigate more (since we know which resources went into the prompt).\n","\n","The whole idea of RAG is to get an LLM to be more factually correct based on your own input as well as have a reference to where the generated output may have come from.\n","\n","This is an incredibly helpful tool.\n","\n","Let's say you had 1000s of customer support documents.\n","\n","You could use RAG to generate direct answers to questions with links to relevant documentation.\n","\n","Or you were an insurance company with large chains of claims emails.\n","\n","You could use RAG to answer questions about the emails with sources.\n","\n","One helpful analogy is to think of LLMs as calculators for words.\n","\n","With good inputs, the LLM can sort them into helpful outputs.\n","\n","How?\n","\n","It starts with better search."]},{"cell_type":"markdown","metadata":{"id":"zW0Cvn4DyTHG"},"source":["### Similarity search\n","\n","Similarity search or semantic search or vector search is the idea of searching on *vibe*.\n","\n","If this sounds like woo, woo. It's not.\n","\n","Perhaps searching via *meaning* is a better analogy.\n","\n","With keyword search, you are trying to match the string \"apple\" with the string \"apple\".\n","\n","Whereas with similarity/semantic search, you may want to search \"macronutrients functions\".\n","\n","And get back results that don't necessarily contain the words \"macronutrients functions\" but get back pieces of text that match that meaning.\n","\n","> **Example:** Using similarity search on our textbook data with the query \"macronutrients function\" returns a paragraph that starts with:\n",">\n",">*There are three classes of macronutrients: carbohydrates, lipids, and proteins. These can be metabolically processed into cellular energy. The energy from macronutrients comes from their chemical bonds. This chemical energy is converted into cellular energy that is then utilized to perform work, allowing our bodies to conduct their basic functions.*\n",">\n","> as the first result. How cool!\n","\n","If you've ever used Google, you know this kind of workflow.\n","\n","But now we'd like to perform that across our own data.\n","\n","Let's import our embeddings we created earlier (tk -link to embedding file) and prepare them for use by turning them into a tensor."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":569},"id":"skagfsKhyTHG","outputId":"931f4ae3-f333-4285-85d7-a47e10747c43","executionInfo":{"status":"error","timestamp":1734539340941,"user_tz":-120,"elapsed":245,"user":{"displayName":"Turkoosi","userId":"15294961855769584895"}}},"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'embedding'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'embedding'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-090152ce65c5>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtext_chunks_and_embedding_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_chunks_and_embedding_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Convert texts and embedding df to list of dicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'embedding'"]}],"source":["import random\n","\n","import torch\n","import numpy as np\n","import pandas as pd\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Import texts and embedding df\n","text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n","\n","# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n","text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n","\n","# Convert texts and embedding df to list of dicts\n","pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n","\n","# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n","embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n","embeddings.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCE6kUgWyTHG"},"outputs":[],"source":["text_chunks_and_embedding_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_mwM-5TSyTHH"},"outputs":[],"source":["embeddings[0]"]},{"cell_type":"markdown","metadata":{"id":"9vcnLY8lyTHH"},"source":["Nice!\n","\n","Now let's prepare another instance of our embedding model. Not because we have to but because we'd like to make it so you can start the notebook from the cell above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Omq5A7CyTHH"},"outputs":[],"source":["from sentence_transformers import util, SentenceTransformer\n","\n","embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n","                                      device=device) # choose the device to load the model to"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HIuZPwu_60WT"},"outputs":[],"source":["\n","\n","from sentence_transformers import util, SentenceTransformer\n","\n","embedding_model = SentenceTransformer(model_name_or_path='TurkuNLP/sbert-uncased-finnish-paraphrase',\n","                                      device=device) # choose the device to load the model to"]},{"cell_type":"markdown","metadata":{"id":"mnobyeNwyTHH"},"source":["Embedding model ready!\n","\n","Time to perform a semantic search.\n","\n","Let's say you were studying the macronutrients.\n","\n","And wanted to search your textbook for \"macronutrients functions\".\n","\n","Well, we can do so with the following steps:\n","1. Define a query string (e.g. `\"macronutrients functions\"`) - note: this could be anything, specific or not.\n","2. Turn the query string in an embedding with same model we used to embed our text chunks.\n","3. Perform a [dot product](https://pytorch.org/docs/stable/generated/torch.dot.html) or [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) function between the text embeddings and the query embedding (we'll get to what these are shortly) to get similarity scores.\n","4. Sort the results from step 3 in descending order (a higher score means more similarity in the eyes of the model) and use these values to inspect the texts.\n","\n","Easy!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5fqtJ-_H6ykE"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hzczmui4yTHH"},"outputs":[],"source":["# 1. Define the query\n","# Note: This could be anything. But since we're working with a nutrition textbook, we'll stick with nutrition-based queries.\n","query = \"Uudet pyöräreitit\"\n","print(f\"Query: {query}\")\n","\n","# 2. Embed the query to the same numerical space as the text examples\n","# Note: It's important to embed your query with the same model you embedded your examples with.\n","query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n","\n","# 3. Get similarity scores with the dot product (we'll time this for fun)\n","from time import perf_counter as timer\n","\n","start_time = timer()\n","dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n","end_time = timer()\n","\n","print(f\"Time take to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n","\n","# 4. Get the top-k results (we'll keep this to 5)\n","top_results_dot_product = torch.topk(dot_scores, k=5)\n","top_results_dot_product"]},{"cell_type":"markdown","metadata":{"id":"9CXujEghyTHH"},"source":["Woah!! Now that was fast!\n","\n","~0.00008 seconds to perform a dot product comparison across 1680 embeddings on my machine (NVIDIA RTX 4090 GPU).\n","\n","GPUs are optimized for these kinds of operations.\n","\n","So even if you we're to increase our embeddings by 100x (1680 -> 168,000), an exhaustive dot product operation would happen in ~0.008 seconds (assuming linear scaling).\n","\n","Heck, let's try it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jxxBReJzyTHI"},"outputs":[],"source":["larger_embeddings = torch.randn(100*embeddings.shape[0], 768).to(device)\n","print(f\"Embeddings shape: {larger_embeddings.shape}\")\n","\n","# Perform dot product across 168,000 embeddings\n","start_time = timer()\n","dot_scores = util.dot_score(a=query_embedding, b=larger_embeddings)[0]\n","end_time = timer()\n","\n","print(f\"Time take to get scores on {len(larger_embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")"]},{"cell_type":"markdown","metadata":{"id":"2zAhyL3tyTHI"},"source":["Wow. That's quick!\n","\n","That means we can get pretty far by just storing our embeddings in `torch.tensor` for now.\n","\n","However, for *much* larger datasets, we'd likely look at a dedicated vector database/indexing libraries such as [Faiss](https://github.com/facebookresearch/faiss).\n","\n","Let's check the results of our original similarity search.\n","\n","[`torch.topk`](https://pytorch.org/docs/stable/generated/torch.topk.html) returns a tuple of values (scores) and indicies for those scores.\n","\n","The indicies relate to which indicies in the `embeddings` tensor have what scores in relation to the query embedding (higher is better).\n","\n","We can use those indicies to map back to our text chunks.\n","\n","First, we'll define a small helper function to print out wrapped text (so it doesn't print a whole text chunk as a single line)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f162b7lKyTHI"},"outputs":[],"source":["# Define helper function to print wrapped text\n","import textwrap\n","\n","def print_wrapped(text, wrap_length=80):\n","    wrapped_text = textwrap.fill(text, wrap_length)\n","    print(wrapped_text)"]},{"cell_type":"markdown","metadata":{"id":"EZZPRwLryTHI"},"source":["Now we can loop through the `top_results_dot_product` tuple and match up the scores and indicies and then use those indicies to index on our `pages_and_chunks` variable to get the relevant text chunk.\n","\n","Sounds like a lot but we can do it!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v6XJVrAbyTHI"},"outputs":[],"source":["print(f\"Query: '{query}'\\n\")\n","print(\"Results:\")\n","# Loop through zipped together scores and indicies from torch.topk\n","for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n","    print(f\"Score: {score:.4f}\")\n","    # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n","    print(\"Text:\")\n","    print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n","    # Print the page number too so we can reference the textbook further (and check the results)\n","    print(f\"Page number: {pages_and_chunks[idx]['page_number']}\")\n","    print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"3Z7mq5lcyTHI"},"source":["The first result looks to have nailed it!\n","\n","We get a very relevant answer to our query `\"macronutrients functions\"` even though its quite vague.\n","\n","That's the power of semantic search!\n","\n","And even better, if we wanted to inspect the result further, we get the page number where the text appears.\n","\n","How about we check the page to verify?\n","\n","We can do so by loading the page number containing the highest result (page 5 but really page 5 + 41 since our PDF page numbers start on page 41)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MVlOnBhByTHJ"},"outputs":[],"source":["import fitz\n","\n","# Open PDF and load target page\n","#pdf_path = \"human-nutrition-text.pdf\" # requires PDF to be downloaded\n","pdf_path = \"suunnittelu-ja-kaavoituskatsaus.pdf\"\n","print(pdf_path)\n","doc = fitz.open(pdf_path)\n","\n","page = doc.load_page(-7-27) # number of page (our doc starts page numbers on page 41)\n","\n","# Get the image of the page\n","img = page.get_pixmap(dpi=300)\n","\n","# Optional: save the image\n","#img.save(\"output_filename.png\")\n","doc.close()\n","\n","# Convert the Pixmap to a numpy array\n","img_array = np.frombuffer(img.samples_mv,\n","                          dtype=np.uint8).reshape((img.h, img.w, img.n))\n","\n","# Display the image using Matplotlib\n","import matplotlib.pyplot as plt\n","plt.figure(figsize=(13, 10))\n","plt.imshow(img_array)\n","plt.title(f\"Query: '{query}' | Most relevant page:\")\n","plt.axis('off') # Turn off axis\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ApL2VoxTyTHJ"},"source":["Nice!\n","\n","Now we can do extra research if we'd like.\n","\n","We could repeat this workflow for any kind of query we'd like on our textbook.\n","\n","And it would also work for other datatypes too.\n","\n","We could use semantic search on customer support documents.\n","\n","Or email threads.\n","\n","Or company plans.\n","\n","Or our old journal entries.\n","\n","Almost anything!\n","\n","The workflow is the same:\n","\n","`ingest documents -> split into chunks -> embed chunks -> make a query -> embed the query -> compare query embedding to chunk embeddings`\n","\n","And we get relevant resources *along with* the source they came from!\n","\n","That's the **retrieval** part of Retrieval Augmented Generation (RAG).\n","\n","Before we get to the next two steps, let's take a small aside and discuss similarity measures."]},{"cell_type":"markdown","metadata":{"id":"OgDJkTfnyTHJ"},"source":["### Similarity measures: dot product and cosine similarity\n","\n","Let's talk similarity measures between vectors.\n","\n","Specifically, embedding vectors which are representations of data with magnitude and direction in high dimensional space (our embedding vectors have 768 dimensions).\n","\n","Two of the most common you'll across are the dot product and cosine similarity.\n","\n","They are quite similar.\n","\n","The main difference is that cosine similarity has a normalization step.\n","\n","| Similarity measure | Description | Code |\n","| ----- | ----- | ----- |\n","| [Dot Product](https://en.wikipedia.org/wiki/Dot_product) | - Measure of magnitude and direction between two vectors<br>- Vectors that are aligned in direction and magnitude have a higher positive value<br>- Vectors that are opposite in direction and magnitude have a higher negative value | [`torch.dot`](https://pytorch.org/docs/stable/generated/torch.dot.html), [`np.dot`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html), [`sentence_transformers.util.dot_score`](https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.dot_score) |\n","| [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) | - Vectors get normalized by magnitude/[Euclidean norm](https://en.wikipedia.org/wiki/Norm_(mathematics))/L2 norm so they have unit length and are compared more so on direction<br>- Vectors that are aligned in direction have a value close to 1<br>- Vectors that are opposite in direction have a value close to -1 | [`torch.nn.functional.cosine_similarity`](https://pytorch.org/docs/stable/generated/torch.nn.functional.cosine_similarity.html), [`1 - scipy.spatial.distance.cosine`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html) (subtract the distance from 1 for similarity measure), [`sentence_transformers.util.cos_sim`](https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.cos_sim) |\n","\n","For text similarity, you generally want to use cosine similarity as you are after the semantic measurements (direction) rather than magnitude.\n","\n","In our case, our embedding model `all-mpnet-base-v2` outputs normalized outputs (see the [Hugging Face model card](https://huggingface.co/sentence-transformers/all-mpnet-base-v2#usage-huggingface-transformers) for more on this) so dot product and cosine similarity return the same results. However, dot product is faster due to not need to perform a normalize step.\n","\n","To make things bit more concrete, let's make simple dot product and cosine similarity functions and view their results on different vectors.\n","\n","> **Note:** Similarity measures between vectors and embeddings can be used on any kind of embeddings, not just text embeddings. For example, you could measure image embedding similarity or audio embedding similarity. Or with text and image models like [CLIP](https://github.com/mlfoundations/open_clip), you can measure the similarity between text and image embeddings."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4wsnbOY8yTHJ"},"outputs":[],"source":["import torch\n","\n","def dot_product(vector1, vector2):\n","    return torch.dot(vector1, vector2)\n","\n","def cosine_similarity(vector1, vector2):\n","    dot_product = torch.dot(vector1, vector2)\n","\n","    # Get Euclidean/L2 norm of each vector (removes the magnitude, keeps direction)\n","    norm_vector1 = torch.sqrt(torch.sum(vector1**2))\n","    norm_vector2 = torch.sqrt(torch.sum(vector2**2))\n","\n","    return dot_product / (norm_vector1 * norm_vector2)\n","\n","# Example tensors\n","vector1 = torch.tensor([1, 2, 3], dtype=torch.float32)\n","vector2 = torch.tensor([1, 2, 3], dtype=torch.float32)\n","vector3 = torch.tensor([4, 5, 6], dtype=torch.float32)\n","vector4 = torch.tensor([-1, -2, -3], dtype=torch.float32)\n","\n","# Calculate dot product\n","print(\"Dot product between vector1 and vector2:\", dot_product(vector1, vector2))\n","print(\"Dot product between vector1 and vector3:\", dot_product(vector1, vector3))\n","print(\"Dot product between vector1 and vector4:\", dot_product(vector1, vector4))\n","\n","# Calculate cosine similarity\n","print(\"Cosine similarity between vector1 and vector2:\", cosine_similarity(vector1, vector2))\n","print(\"Cosine similarity between vector1 and vector3:\", cosine_similarity(vector1, vector3))\n","print(\"Cosine similarity between vector1 and vector4:\", cosine_similarity(vector1, vector4))"]},{"cell_type":"markdown","metadata":{"id":"iAKVZTKKyTHK"},"source":["Notice for both dot product and cosine similarity the comparisons of `vector1` and `vector2` are the opposite of `vector1` and `vector4`.\n","\n","Comparing `vector1` and `vector2` both equations return positive values (14 for dot product and 1.0 for cosine similarity).\n","\n","But comparing `vector1` and `vector4` the result is in the negative direction.\n","\n","This makes sense because `vector4` is the negative version of `vector1`.\n","\n","Whereas comparing `vector1` and `vector3` shows a different outcome.\n","\n","For the dot product, the value is positive and larger then the comparison of two exactly the same vectors (32 vs 14).\n","\n","However, for the cosine similarity, thanks to the normalization step, comparing `vector1` and `vector3` results in a postive value close to 1 but not exactly 1.\n","\n","It is because of this that when comparing text embeddings, cosine similarity is generally favoured as it measures the difference in direction of a pair of vectors rather than difference in magnitude.\n","\n","And it is this difference in direction that is more generally considered to capture the semantic meaning/vibe of the text.\n","\n","The good news is that as mentioned before, the outputs of our embedding model `all-mpnet-base-v2` are already normalized.\n","\n","So we can continue using the dot product (cosine similarity is dot product + normalization).\n","\n","With similarity measures explained, let's functionize our semantic search steps from above so we can repeat them."]},{"cell_type":"markdown","metadata":{"id":"QVU2Wg2SyTHK"},"source":["### Functionizing our semantic search pipeline\n","\n","Let's put all of the steps from above for semantic search into a function or two so we can repeat the workflow."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S6Of1CbRyTHK"},"outputs":[],"source":["def retrieve_relevant_resources(query: str,\n","                                embeddings: torch.tensor,\n","                                model: SentenceTransformer=embedding_model,\n","                                n_resources_to_return: int=5,\n","                                print_time: bool=True):\n","    \"\"\"\n","    Embeds a query with model and returns top k scores and indices from embeddings.\n","    \"\"\"\n","\n","    # Embed the query\n","    query_embedding = model.encode(query,\n","                                   convert_to_tensor=True)\n","\n","    # Get dot product scores on embeddings\n","    start_time = timer()\n","    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n","    end_time = timer()\n","\n","    if print_time:\n","        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n","\n","    scores, indices = torch.topk(input=dot_scores,\n","                                 k=n_resources_to_return)\n","\n","    return scores, indices\n","\n","def print_top_results_and_scores(query: str,\n","                                 embeddings: torch.tensor,\n","                                 pages_and_chunks: list[dict]=pages_and_chunks,\n","                                 n_resources_to_return: int=5):\n","    \"\"\"\n","    Takes a query, retrieves most relevant resources and prints them out in descending order.\n","\n","    Note: Requires pages_and_chunks to be formatted in a specific way (see above for reference).\n","    \"\"\"\n","\n","    scores, indices = retrieve_relevant_resources(query=query,\n","                                                  embeddings=embeddings,\n","                                                  n_resources_to_return=n_resources_to_return)\n","\n","    print(f\"Query: {query}\\n\")\n","    print(\"Results:\")\n","    # Loop through zipped together scores and indicies\n","    for score, index in zip(scores, indices):\n","        print(f\"Score: {score:.4f}\")\n","        # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n","        print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n","        # Print the page number too so we can reference the textbook further and check the results\n","        print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n","        print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"hwVhxkBbyTHK"},"source":["Excellent! Now let's test our functions out."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XCYmQHc6yTHL"},"outputs":[],"source":["query = \"pikaratikka\"\n","\n","# Get just the scores and indices of top related results\n","scores, indices = retrieve_relevant_resources(query=query,\n","                                              embeddings=embeddings)\n","scores, indices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q-ucuJmFyTHL"},"outputs":[],"source":["# Print out the texts of the top scores\n","print_top_results_and_scores(query=query,\n","                             embeddings=embeddings)"]},{"cell_type":"markdown","metadata":{"id":"IbG0V8kgyTHL"},"source":["### Semantic search/vector search extensions\n","\n","We've covered an exmaple of using embedding vector search to find relevant results based on a query.\n","\n","However, you could also add to this pipeline with traditional keyword search.\n","\n","Many modern search systems use keyword and vector search in tandem.\n","\n","Our dataset is small and allows for an exhaustive search (comparing the query to *every* possible result) but if you start to work with large scale datasets with hundred of thousands, millions or even billions of vectors, you'll want to implement an index.\n","\n","You can think of an index as sorting your embeddings before you search through them.\n","\n","So it narrows down the search space.\n","\n","For example, it would be inefficient to search every word in the dictionary to find the word \"duck\", instead you'd go straight to the letter D, perhaps even straight to the back half of the letter D, find words close to \"duck\" before finding it.\n","\n","That's how an index can help search through many examples without comprimising too much on speed or quality (for more on this, check out [nearest neighbour search](https://en.wikipedia.org/wiki/Nearest_neighbor_search)).\n","\n","One of the most popular indexing libraries is [Faiss](https://github.com/facebookresearch/faiss).\n","\n","Faiss is open-source and was originally created by Facebook to deal with internet-scale vectors and implements many algorithms such as [HNSW](https://arxiv.org/abs/1603.09320) (Hierarchical Naviganle Small Worlds)."]},{"cell_type":"markdown","metadata":{"id":"M9fyQzh5yTHL"},"source":["### Getting an LLM for local generation\n","\n","We're got our retrieval pipeline ready, let's now get the generation side of things happening.\n","\n","To perform generation, we're going to use a Large Language Model (LLM).\n","\n","LLMs are designed to generate an output given an input.\n","\n","In our case, we want our LLM to generate and output of text given a input of text.\n","\n","And more specifically, we want the output of text to be generated based on the context of relevant information to the query.\n","\n","The input to an LLM is often referred to as a prompt.\n","\n","We'll augment our prompt with a query as well as context from our textbook related to that query.\n","\n","> **Which LLM should I use?**\n","\n","There are many LLMs available.\n","\n","Two of the main questions to ask from this is:\n","1. Do I want it to run locally?\n","2. If yes, how much compute power can I dedicate?\n","\n","If you're after the absolute best performance, you'll likely want to use an API (not running locally) such as GPT-4 or Claude 3. However, this comes with the tradeoff of sending your data away and then awaiting a response.\n","\n","For our case, since we want to set up a local pipeline and run it on our own GPU, we'd answer \"yes\" to the first question and then the second question will depend on what hardware we have available.\n","\n","To find open-source LLMs, one great resource is the [Hugging Face open LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n","\n","The leaderboard compares many of the latest and greatest LLMs on various benchmarks.\n","\n","Another great resource is [TheBloke on Hugging Face](https://huggingface.co/TheBloke), an account which provides an extensive range of quantized (models that have been made smaller) LLMs.\n","\n","A rule of thumb for LLMs (and deep learning models in general) is that the higher the number of parameters, the better the model performs.\n","\n","It may be tempting to go for the largest size model (e.g. a 70B parameter model rather than a 7B parameter model) but a larger size model may not be able to run on your available hardware.\n","\n","The following table gives an insight into how much GPU memory you'll need to load an LLM with different sizes and different levels of [numerical precision](https://en.wikipedia.org/wiki/Precision_(computer_science)).\n","\n","They are based on the fact that 1 float32 value (e.g. `0.69420`) requires 4 bytes of memory and 1GB is approximately 1,000,000,000 (one billion) bytes.\n","\n","| Model Size (Billion Parameters) | Float32 VRAM (GB) | Float16 VRAM (GB) | 8-bit VRAM (GB) | 4-bit VRAM (GB) |\n","|-----|-----|-----|-----|-----|\n","| 1B                              | ~4                | ~2                | ~1              | ~0.5            |\n","| 7B (e.g., [Llama 2 7B](https://huggingface.co/meta-llama/Llama-2-7b), [Gemma 7B](https://huggingface.co/google/gemma-7b-it), [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1))             | ~28               | ~14               | ~7              | ~3.5            |\n","| 10B                             | ~40               | ~20               | ~10             | ~5              |\n","| 70B (e.g, Llama 2 70B)          | ~280              | ~140              | ~70             | ~35             |\n","| 100B                            | ~400              | ~200              | ~100            | ~50             |\n","| 175B                            | ~700              | ~350              | ~175            | ~87.5           |\n","\n","<br>\n","\n","> **Note:** Loading a model in a lower precision (e.g. 8-bit instead of float16) generally lowers performance. Lower precision can help to reduce computing requirements, however sometimes the performance degradation in terms of model output can be substantial. Finding the right speed/performance tradeoff will often require many experiments."]},{"cell_type":"markdown","metadata":{"id":"kmP-DtMRyTHL"},"source":["### Checking local GPU memory availability\n","\n","Let's find out what hardware we've got available and see what kind of model(s) we'll be able to load.\n","\n","> **Note:** You can also check this with the `!nvidia-smi` command."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0MjH4PO1yTHL"},"outputs":[],"source":["# Get GPU available memory\n","import torch\n","gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n","gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n","print(f\"Available GPU memory: {gpu_memory_gb} GB\")"]},{"cell_type":"markdown","metadata":{"id":"cQfj_pCFyTHM"},"source":["Ok wonderful!\n","\n","I'm running this notebook with a NVIDIA RTX 4090, so I've got 24GB of VRAM available.\n","\n","However, this may be different on your end.\n","\n","Looking at the table above, it seems we can run a ~7-10B parameter model in float16 precision pretty comfortably.\n","\n","But we could also run a smaller one if we'd like.\n","\n","Let's try out the recently released (at the time of writing, March 2024) LLM from Google, [Gemma](https://huggingface.co/blog/gemma).\n","\n","Specifically, we'll use the `gemma-7b-it` version which stands for Gemma 7B Instruction-Tuned.\n","\n","Instruction tuning is the process of tuning a raw language model to follow instructions.\n","\n","These are the kind of models you'll find in most chat-based assistants such as ChatGPT, Gemini or Claude.\n","\n","The following table shows different amounts of GPU memory requirements for different verions of the Gemma LLMs with varying levels of precision.\n","\n","| Model             | Precision | Min-Memory (Bytes) | Min-Memory (MB) | Min-Memory (GB) | Recommended Memory (GB) | Hugging Face ID |\n","|-------------------|-----------|----------------|-------------|-------------| ----- | ----- |\n","| [Gemma 2B](https://huggingface.co/google/gemma-2b-it)          | 4-bit     | 2,106,749,952  | 2009.15     | 1.96        | ~5.0 | [`gemma-2b`](https://huggingface.co/google/gemma-2b) or [`gemma-2b-it`](https://huggingface.co/google/gemma-2b-it) for instruction tuned version |\n","| Gemma 2B          | Float16   | 5,079,453,696  | 4844.14     | 4.73        | ~8.0 | Same as above |\n","| [Gemma 7B](https://huggingface.co/google/gemma-7b-it)          | 4-bit     | 5,515,859,968  | 5260.33     | 5.14        | ~8.0 | [`gemma-7b`](https://huggingface.co/google/gemma-7b) or [`gemma-7b-it`](https://huggingface.co/google/gemma-7b-it) for instruction tuned version |\n","| Gemma 7B          | Float16   | 17,142,470,656 | 16348.33    | 15.97       | ~19 | Same as above |\n","\n","> **Note:** `gemma-7b-it` means \"instruction tuned\", as in, a base LLM (`gemma-7b`) has been fine-tuned to follow instructions, similar to [`Mistral-7B-v0.1`](https://huggingface.co/mistralai/Mistral-7B-v0.1) and [`Mistral-7B-Instruct-v0.1`](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1).\n",">\n","> There are also further quantized and smaller variants of Gemma (and other LLMs) available in various formats such as GGUF. You can see many of these on [TheBloke account on Hugging Face](https://huggingface.co/TheBloke).\n",">\n","> The version of LLM you choose to use will be largely based on project requirements and experimentation.\n","\n","Based on the table above, let's write a simple if/else statement which recommends which Gemma variant we should look into using."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QPVcSRhyTHM"},"outputs":[],"source":["# Note: the following is Gemma focused, however, there are more and more LLMs of the 2B and 7B size appearing for local use.\n","if gpu_memory_gb < 5.1:\n","    print(f\"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.\")\n","elif gpu_memory_gb < 8.1:\n","    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.\")\n","    use_quantization_config = True\n","    model_id = \"google/gemma-2b-it\"\n","elif gpu_memory_gb < 19.0:\n","    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\")\n","    use_quantization_config = False\n","    model_id = \"google/gemma-2b-it\"\n","elif gpu_memory_gb > 19.0:\n","    print(f\"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.\")\n","    use_quantization_config = False\n","    model_id = \"google/gemma-7b-it\"\n","\n","print(f\"use_quantization_config set to: {use_quantization_config}\")\n","print(f\"model_id set to: {model_id}\")"]},{"cell_type":"markdown","metadata":{"id":"4xaf0joQyTHM"},"source":["### Loading an LLM locally\n","\n","Alright! Looks like `gemma-7b-it` it is (for my local machine with an RTX 4090, change the `model_id` and `use_quantization_config` values to suit your needs)!\n","\n","There are plenty of examples of how to load the model on the `gemma-7b-it` [Hugging Face model card](https://huggingface.co/google/gemma-7b-it).\n","\n","Good news is, the Hugging Face [`transformers`](https://huggingface.co/docs/transformers/) library has all the tools we need.\n","\n","To load our LLM, we're going to need a few things:\n","1. A quantization config (optional) - This will determine whether or not we load the model in 4bit precision for lower memory usage. The we can create this with the [`transformers.BitsAndBytesConfig`](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/quantization#transformers.BitsAndBytesConfig) class (requires installing the [`bitsandbytes` library](https://github.com/TimDettmers/bitsandbytes)).\n","2. A model ID - This is the reference Hugging Face model ID which will determine which tokenizer and model gets used. For example `gemma-7b-it`.\n","3. A tokenzier - This is what will turn our raw text into tokens ready for the model. We can create it using the [`transformers.AutoTokenzier.from_pretrained`](https://huggingface.co/docs/transformers/v4.38.2/en/model_doc/auto#transformers.AutoTokenizer) method and passing it our model ID.\n","4. An LLM model - Again, using our model ID we can load a specific LLM model. To do so we can use the [`transformers.AutoModelForCausalLM.from_pretrained`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained) method and passing it our model ID as well as other various parameters.\n","\n","As a bonus, we'll check if [Flash Attention 2](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2) is available using `transformers.utils.is_flash_attn_2_available()`. Flash Attention 2 speeds up the attention mechanism in Transformer architecture models (which is what many modern LLMs are based on, including Gemma). So if it's available and the model is supported (not all models support Flash Attention 2), we'll use it. If it's not available, you can install it by following the instructions on the [GitHub repo](https://github.com/Dao-AILab/flash-attention).\n","\n","> **Note:** Flash Attention 2 currently works on NVIDIA GPUs with a compute capability score of 8.0+ (Ampere, Ada Lovelace, Hopper architectures). We can check our GPU compute capability score with [`torch.cuda.get_device_capability(0)`](https://pytorch.org/docs/stable/generated/torch.cuda.get_device_capability.html).\n","\n","> **Note:** To get access to the Gemma models, you will have to [agree to the terms & conditions](https://huggingface.co/google/gemma-7b-it) on the Gemma model page on Hugging Face. You will then have to authorize your local machine via the [Hugging Face CLI/Hugging Face Hub `login()` function](https://huggingface.co/docs/huggingface_hub/en/quick-start#authentication). Once you've done this, you'll be able to download the models. If you're using Google Colab, you can add a [Hugging Face token](https://huggingface.co/docs/hub/en/security-tokens) to the \"Secrets\" tab.\n",">\n","> Downloading an LLM locally can take a fair bit of time depending on your internet connection. Gemma 7B is about a 16GB download and Gemma 2B is about a 6GB download.\n","\n","Let's do it!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sazuZNLByTHM"},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from transformers.utils import is_flash_attn_2_available\n","\n","# 1. Create quantization config for smaller model loading (optional)\n","# Requires !pip install bitsandbytes accelerate, see: https://github.com/TimDettmers/bitsandbytes, https://huggingface.co/docs/accelerate/\n","# For models that require 4-bit quantization (use this if you have low GPU memory available)\n","from transformers import BitsAndBytesConfig\n","quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n","                                         bnb_4bit_compute_dtype=torch.float16)\n","\n","# Bonus: Setup Flash Attention 2 for faster inference, default to \"sdpa\" or \"scaled dot product attention\" if it's not available\n","# Flash Attention 2 requires NVIDIA GPU compute capability of 8.0 or above, see: https://developer.nvidia.com/cuda-gpus\n","# Requires !pip install flash-attn, see: https://github.com/Dao-AILab/flash-attention\n","if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n","  attn_implementation = \"flash_attention_2\"\n","else:\n","  attn_implementation = \"sdpa\"\n","print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n","\n","# 2. Pick a model we'd like to use (this will depend on how much GPU memory you have available)\n","\n","!pip install huggingface_hub\n","\n","from huggingface_hub import login\n","\n","# Replace 'hf_your_token_here' with your actual Hugging Face token\n","login(token=\"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\")\n","\n","\n","model_id = \"google/gemma-2b-it\"\n","#model_id = model_id # (we already set this above)\n","print(f\"[INFO] Using model_id: {model_id}\")\n","\n","# 3. Instantiate tokenizer (tokenizer turns text into numbers ready for the model)\n","tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n","\n","# 4. Instantiate the model\n","llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n","                                                 torch_dtype=torch.float16, # datatype to use, we want float16\n","                                                 quantization_config=quantization_config if use_quantization_config else None,\n","                                                 low_cpu_mem_usage=False, # use full memory\n","                                                 attn_implementation=attn_implementation) # which attention version to use\n","\n","if not use_quantization_config: # quantization takes care of device setting automatically, so if it's not used, send model to GPU\n","    llm_model.to(\"cuda\")"]},{"cell_type":"markdown","metadata":{"id":"zzwU6z0WyTHM"},"source":["We've got an LLM!\n","\n","Let's check it out."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFjI59s2yTHN"},"outputs":[],"source":["llm_model"]},{"cell_type":"markdown","metadata":{"id":"ZYHYI7d8yTHN"},"source":["Ok, ok a bunch of layers ranging from embedding layers to attention layers (see the `GemmaFlashAttention2` layers!) to MLP and normalization layers.\n","\n","The good news is that we don't have to know too much about these to use the model.\n","\n","How about we get the number of parameters in our model?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8BNQe0iyTHN"},"outputs":[],"source":["def get_model_num_params(model: torch.nn.Module):\n","    return sum([param.numel() for param in model.parameters()])\n","\n","get_model_num_params(llm_model)"]},{"cell_type":"markdown","metadata":{"id":"38B-CE_hyTHN"},"source":["Hmm, turns out that Gemma 7B is really Gemma 8.5B.\n","\n","It pays to do your own investigations!\n","\n","How about we get the models memory requirements?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6g0lZ_QZyTHN"},"outputs":[],"source":["def get_model_mem_size(model: torch.nn.Module):\n","    \"\"\"\n","    Get how much memory a PyTorch model takes up.\n","\n","    See: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822\n","    \"\"\"\n","    # Get model parameters and buffer sizes\n","    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n","    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n","\n","    # Calculate various model sizes\n","    model_mem_bytes = mem_params + mem_buffers # in bytes\n","    model_mem_mb = model_mem_bytes / (1024**2) # in megabytes\n","    model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes\n","\n","    return {\"model_mem_bytes\": model_mem_bytes,\n","            \"model_mem_mb\": round(model_mem_mb, 2),\n","            \"model_mem_gb\": round(model_mem_gb, 2)}\n","\n","get_model_mem_size(llm_model)"]},{"cell_type":"markdown","metadata":{"id":"nMp1C68kyTHN"},"source":["Nice, looks like this model takes up 15.97GB of space on the GPU.\n","\n","Plus a little more for the forward pass (due to all the calculations happening between the layers).\n","\n","Hence why I rounded it up to be ~19GB in the table above.\n","\n","Now let's get to the fun part, generating some text!"]},{"cell_type":"markdown","metadata":{"id":"VWYCXD4pyTHO"},"source":["### Generating text with our LLM\n","\n","We can generate text with our LLM `model` instance by calling the [`generate()` method](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/text_generation#transformers.GenerationConfig) (this method has plenty of options to pass into it alongside the text) on it and passing it a tokenized input.\n","\n","The tokenized input comes from passing a string of text to our `tokenizer`.\n","\n","It's important to note that you should use a tokenizer that has been paired with a model.\n","\n","Otherwise if you try to use a different tokenizer and then pass those inputs to a model, you will likely get errors/strange results.\n","\n","For some LLMs, there's a specific template you should pass to them for ideal outputs.\n","\n","For example, the `gemma-7b-it` model has been trained in a dialogue fashion (instruction tuning).\n","\n","In this case, our `tokenizer` has a [`apply_chat_template()` method](https://huggingface.co/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template) which can prepare our input text in the right format for the model.\n","\n","Let's try it out.\n","\n","> **Note:** The following demo has been modified from the Hugging Face model card for [Gemma 7B](https://huggingface.co/google/gemma-7b-it). Many similar demos of usage are available on the model cards of similar models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SJd0xa5XyTHO"},"outputs":[],"source":["input_text = \"Onko Töölöön tulossa uusia rakennuksia?\"\n","print(f\"Input text:\\n{input_text}\")\n","\n","# Create prompt template for instruction-tuned model\n","dialogue_template = [\n","    {\"role\": \"user\",\n","     \"content\": input_text}\n","]\n","\n","# Apply the chat template\n","prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n","                                       tokenize=False, # keep as raw text (not tokenized)\n","                                       add_generation_prompt=True)\n","print(f\"\\nPrompt (formatted):\\n{prompt}\")"]},{"cell_type":"markdown","metadata":{"id":"VvXdhB0LyTHP"},"source":["Notice the scaffolding around our input text, this is the kind of turn-by-turn instruction tuning our model has gone through.\n","\n","Our next step is to tokenize this formatted text and pass it to our model's `generate()` method.\n","\n","We'll make sure our tokenized text is on the same device as our model (GPU) using `to(\"cuda\")`.\n","\n","Let's generate some text!\n","\n","We'll time it for fun with the `%%time` magic."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"McRDTJ5LyTHP"},"outputs":[],"source":["%%time\n","\n","# Tokenize the input text (turn it into numbers) and send it to GPU\n","input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","print(f\"Model input (tokenized):\\n{input_ids}\\n\")\n","\n","# Generate outputs passed on the tokenized input\n","# See generate docs: https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/text_generation#transformers.GenerationConfig\n","outputs = llm_model.generate(**input_ids,\n","                             max_new_tokens=256) # define the maximum number of new tokens to create\n","print(f\"Model output (tokens):\\n{outputs[0]}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"dijm-7YOyTHP"},"source":["Woohoo! We just generated some text on our local GPU!\n","\n","Well not just yet...\n","\n","Our LLM accepts tokens in and sends tokens back out.\n","\n","We can conver the output tokens to text using [`tokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cRFBfBjAyTHP"},"outputs":[],"source":["# Decode the output tokens to text\n","outputs_decoded = tokenizer.decode(outputs[0])\n","print(f\"Model output (decoded):\\n{outputs_decoded}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"bOLOgaEnyTHP"},"source":["Woah! That looks like a pretty good answer.\n","\n","But notice how the output contains the prompt text as well?\n","\n","How about we do a little formatting to replace the prompt in the output text?\n","\n","> **Note:** `\"<bos>\"` and `\"<eos>\"` are special tokens to denote \"beginning of sentence\" and \"end of sentence\" respectively."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_gGxRKb1yTHP"},"outputs":[],"source":["print(f\"Input text: {input_text}\\n\")\n","print(f\"Output text:\\n{outputs_decoded.replace(prompt, '').replace('<bos>', '').replace('<eos>', '')}\")"]},{"cell_type":"markdown","metadata":{"id":"j2qjcforyTHQ"},"source":["How cool is that!\n","\n","We just officially generated text from an LLM running locally.\n","\n","So we've covered the R (retrieval) and G (generation) of RAG.\n","\n","How about we check out the last step?\n","\n","Augmentation.\n","\n","First, let's put together a list of queries we can try out with our pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ChoxQK88yTHQ"},"outputs":[],"source":["# Nutrition-style questions generated with GPT4\n","gpt4_questions = [\n","    \"Mihin uusia pikaratikkalinjoja tulee?\",\n","\n","]\n","\n","# Manually created question list\n","manual_questions = [\n","    \"Huomioidaanko suunnitelmissa autoilijoita?\"\n","]\n","\n","query_list = gpt4_questions + manual_questions"]},{"cell_type":"markdown","metadata":{"id":"GlE6bnFRyTHQ"},"source":["And now let's check if our `retrieve_relevant_resources()` function works with our list of queries."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DDUtPXsOyTHQ"},"outputs":[],"source":["import random\n","query = random.choice(query_list)\n","\n","print(f\"Query: {query}\")\n","\n","# Get just the scores and indices of top related results\n","scores, indices = retrieve_relevant_resources(query=query,\n","                                              embeddings=embeddings)\n","scores, indices"]},{"cell_type":"markdown","metadata":{"id":"I_308RG4yTHQ"},"source":["Beautiful!\n","\n","Let's augment!"]},{"cell_type":"markdown","metadata":{"id":"Lx502hSnyTHQ"},"source":["### Augmenting our prompt with context items\n","\n","What we'd like to do with augmentation is take the results from our search for relevant resources and put them into the prompt that we pass to our LLM.\n","\n","In essence, we start with a base prompt and update it with context text.\n","\n","Let's write a function called `prompt_formatter` that takes in a query and our list of context items (in our case it'll be select indices from our list of dictionaries inside `pages_and_chunks`) and then formats the query with text from the context items.\n","\n","We'll apply the dialogue and chat template to our prompt before returning it as well.\n","\n","> **Note:** The process of augmenting or changing a prompt to an LLM is known as prompt engineering. And the best way to do it is an active area of research. For a comprehensive guide on different prompt engineering techniques, I'd recommend the Prompt Engineering Guide ([promptingguide.ai](https://www.promptingguide.ai/)), [Brex's Prompt Engineering Guide](https://github.com/brexhq/prompt-engineering) and the paper [Prompt Design and Engineering: Introduction and Advanced Models](https://arxiv.org/abs/2401.14423)."]},{"cell_type":"code","source":["#I Give the nutrition-\n","\n","\n","\n","\n","def prompt_formatter(query: str,\n","                     context_items: list[dict]) -> str:\n","    \"\"\"\n","    Augments query with text-based context from context_items.\n","    \"\"\"\n","    # Join context items into one dotted paragraph\n","    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n","\n","    # Create a base prompt with examples to help the model\n","    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n","    # We could also write this in a txt file and import it in if we wanted.\n","    base_prompt = \"\"\"Based on the following context items, please answer the query.\n","Give yourself room to think by extracting relevant passages from the context before answering the query.\n","Don't return the thinking, only return the answer.\n","Make sure your answers are as explanatory as possible.\n","Use the following examples as reference for the ideal answer style.\n","\\nExample 1:\n","Query: What are the fat-soluble vitamins?\n","Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.\n","\\nExample 2:\n","Query: What are the causes of type 2 diabetes?\n","Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.\n","\\nExample 3:\n","Query: What is the importance of hydration for physical performance?\n","Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.\n","\\nNow use the following context items to answer the user query:\n","{context}\n","\\nRelevant passages: <extract relevant passages from the context here>\n","User query: {query}\n","Answer:\"\"\"\n","\n","\n","aihe = \"pitkäviha, 25-vuotinen sota ja pohjoismainen viisikolmattavuotinen sota\"\n","kys1 = \"Mikä oli Tanskan rooli Liivinmaan sodassa?\"\n","kys2 = \"Mitä vaikutuksia sodalla oli Suomen siviiliväestöön? \"\n","kys3 = \"Miten Ruotsin ja Puolan välinen suhde muuttui sodan aikana? \"\n","kehoite = (\"Käytä oheista englanninkielistä kehoitetta\", base prompt, \"mallina, mutta käännä se suomeksi (lukuun ottamatta muuttujia {context} ja {query}) ja muut aihe koskemaan aihetta)\", aihe, \"sekä vaihda kysymykset kysymyksiin\", kys1, kys2, kys3, \"mutta anna näille niihin vastaavat suomenkieliset vastaukset. \")\n","\n","print(kehoite)\n","\n","base_prompt = \"Perustuen alla esitettyihin kontekstikohteisiin, laadi vastaus käyttäjän kysymykseen. Anna itsellesi tilaa ajattelulle poimimalla ensin asiaankuuluvat katkelmat kontekstista, mutta älä sisällytä tätä ajatteluprosessia lopulliseen vastaukseen. Muista, että lopullinen vastaus on mahdollisimman selittävä ja perusteellinen. Käytä seuraavia esimerkkejä viitteenä ihanteellisesta vastaustyylistä. Esimerkki 1: Kysymys: Mitä uusia joukkoliikennemuotoja on otettu käyttöön Helsingissä? Vastaus: Helsingissä on viime vuosina otettu käyttöön useita uusia joukkoliikennemuotoja, kuten laajennetut raitiovaunulinjat, entistä nopeampia ja ympäristöystävällisempiä sähköbusseja sekä kaupunkipyöräjärjestelmiä. Uusien raitiovaunureittien ansiosta kaupungin eri alueiden välinen liikkuminen on sujuvampaa, ja sähkökäyttöiset linja-autot auttavat vähentämään melua ja päästöjä. Kaupunkipyörät puolestaan tarjoavat helpon ja edullisen tavan siirtyä lyhyitä matkoja, mikä täydentää perinteisiä joukkoliikennevälineitä ja auttaa vähentämään yksityisautoilun tarvetta. Esimerkki 2: Kysymys: Miten uudet joukkoliikennemenetelmät vaikuttavat liikenteen sujuvuuteen? Vastaus: Uusien joukkoliikennemenetelmien käyttöönotto parantaa merkittävästi Helsingin liikenteen sujuvuutta. Raitiovaunujen runkolinjat lisäävät kuljetuskapasiteettia ruuhkaisilla reiteillä, kun taas modernit sähköbussit voivat ajaa joustavasti eri linjoilla ja reagoida matkustajamääriin nopeasti. Kaupunkipyörien saatavuus vähentää lyhyiden matkojen tarvetta käyttää henkilöautoa, mikä helpottaa ruuhkia etenkin keskustan alueella. Näin eri joukkoliikennemuotojen yhteispeli auttaa pitämään liikkumisen sekä tehokkaana että ympäristöystävällisenä. Esimerkki 3: Kysymys: Miten uudet joukkoliikennemuodot vähentävät ympäristövaikutuksia? Vastaus: Uudet joukkoliikennemuodot, kuten sähköbussit ja entistä energiatehokkaammat raitiovaunut, vähentävät selvästi Helsingin liikenteen ympäristövaikutuksia. Sähköiset kulkuneuvot tuottavat vähemmän hiilidioksidipäästöjä ja muuta ilmansaastetta verrattuna perinteisiin dieselbusseihin. Lisäksi kaupunkipyörät korvaavat lyhyillä matkoilla usein auton käytön, jolloin sekä päästöt että liikenteen meluhaitat vähenevät. Ympäristöystävällisempien joukkoliikennemuotojen ansiosta kaupunki voi edetä kohti kestävämpää ja viihtyisämpää elinympäristöä. Nyt hyödynnä alla olevia kontekstikohteita vastataksesi käyttäjän kysymykseen: {context} Asiaankuuluvat katkelmat: <poimi tähän kontekstista olennaiset osat> Käyttäjän kysymys: {query}\"\n"],"metadata":{"id":"-WWJWH6lg0Tj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SEGp_VMDyTHR"},"outputs":[],"source":["\n","\n","\n","\n","def prompt_formatter(query: str,\n","                     context_items: list[dict]) -> str:\n","    \"\"\"\n","    Augments query with text-based context from context_items.\n","    \"\"\"\n","    # Join context items into one dotted paragraph\n","    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n","\n","    # Create a base prompt with examples to help the model\n","    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n","    # We could also write this in a txt file and import it in if we wanted.\n","    base_prompt = \"\"\"Based on the following context items, please answer the query.\n","Give yourself room to think by extracting relevant passages from the context before answering the query.\n","Don't return the thinking, only return the answer.\n","Make sure your answers are as explanatory as possible.\n","Use the following examples as reference for the ideal answer style.\n","\\nExample 1:\n","Query: What are the fat-soluble vitamins?\n","Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.\n","\\nExample 2:\n","Query: What are the causes of type 2 diabetes?\n","Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.\n","\\nExample 3:\n","Query: What is the importance of hydration for physical performance?\n","Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.\n","\\nNow use the following context items to answer the user query:\n","{context}\n","\\nRelevant passages: <extract relevant passages from the context here>\n","User query: {query}\n","Answer:\"\"\"\n","\n","\n","aihe = \"pitkäviha, 25-vuotinen sota ja pohjoismainen viisikolmattavuotinen sota\"\n","kys1 = \"Mikä oli Tanskan rooli Liivinmaan sodassa?\"\n","kys2 = \"Mitä vaikutuksia sodalla oli Suomen siviiliväestöön? \"\n","kys3 = \"Miten Ruotsin ja Puolan välinen suhde muuttui sodan aikana? \"\n","kehoite = (\"Käytä oheista englanninkielistä kehoitetta\", base prompt, \"mallina, mutta käännä se suomeksi (lukuun ottamatta muuttujia {context} ja {query}) ja muut aihe koskemaan aihetta)\", aihe,\n","           \"sekä vaihda kysymykset kysymyksiin\", kys1, kys2, kys3, \"mutta anna näille niihin vastaavat suomenkieliset vastaukset. \")\n","\n","    #base_prompt = \"Perustuen alla esitettyihin kontekstikohteisiin, laadi vastaus käyttäjän kysymykseen. Anna itsellesi tilaa ajattelulle poimimalla ensin asiaankuuluvat katkelmat kontekstista, mutta älä sisällytä tätä ajatteluprosessia lopulliseen vastaukseen. Muista, että lopullinen vastaus on mahdollisimman selittävä ja perusteellinen. Käytä seuraavia esimerkkejä viitteenä ihanteellisesta vastaustyylistä. Esimerkki 1: Kysymys: Mitä uusia joukkoliikennemuotoja on otettu käyttöön Helsingissä? Vastaus: Helsingissä on viime vuosina otettu käyttöön useita uusia joukkoliikennemuotoja, kuten laajennetut raitiovaunulinjat, entistä nopeampia ja ympäristöystävällisempiä sähköbusseja sekä kaupunkipyöräjärjestelmiä. Uusien raitiovaunureittien ansiosta kaupungin eri alueiden välinen liikkuminen on sujuvampaa, ja sähkökäyttöiset linja-autot auttavat vähentämään melua ja päästöjä. Kaupunkipyörät puolestaan tarjoavat helpon ja edullisen tavan siirtyä lyhyitä matkoja, mikä täydentää perinteisiä joukkoliikennevälineitä ja auttaa vähentämään yksityisautoilun tarvetta. Esimerkki 2: Kysymys: Miten uudet joukkoliikennemenetelmät vaikuttavat liikenteen sujuvuuteen? Vastaus: Uusien joukkoliikennemenetelmien käyttöönotto parantaa merkittävästi Helsingin liikenteen sujuvuutta. Raitiovaunujen runkolinjat lisäävät kuljetuskapasiteettia ruuhkaisilla reiteillä, kun taas modernit sähköbussit voivat ajaa joustavasti eri linjoilla ja reagoida matkustajamääriin nopeasti. Kaupunkipyörien saatavuus vähentää lyhyiden matkojen tarvetta käyttää henkilöautoa, mikä helpottaa ruuhkia etenkin keskustan alueella. Näin eri joukkoliikennemuotojen yhteispeli auttaa pitämään liikkumisen sekä tehokkaana että ympäristöystävällisenä. Esimerkki 3: Kysymys: Miten uudet joukkoliikennemuodot vähentävät ympäristövaikutuksia? Vastaus: Uudet joukkoliikennemuodot, kuten sähköbussit ja entistä energiatehokkaammat raitiovaunut, vähentävät selvästi Helsingin liikenteen ympäristövaikutuksia. Sähköiset kulkuneuvot tuottavat vähemmän hiilidioksidipäästöjä ja muuta ilmansaastetta verrattuna perinteisiin dieselbusseihin. Lisäksi kaupunkipyörät korvaavat lyhyillä matkoilla usein auton käytön, jolloin sekä päästöt että liikenteen meluhaitat vähenevät. Ympäristöystävällisempien joukkoliikennemuotojen ansiosta kaupunki voi edetä kohti kestävämpää ja viihtyisämpää elinympäristöä. Nyt hyödynnä alla olevia kontekstikohteita vastataksesi käyttäjän kysymykseen: {context} Asiaankuuluvat katkelmat: <poimi tähän kontekstista olennaiset osat> Käyttäjän kysymys: {query}\"\n","\n","    # Update base prompt with context items and query\n","    base_prompt = base_prompt.format(context=context, query=query)\n","\n","    # Create prompt template for instruction-tuned model\n","    dialogue_template = [\n","        {\"role\": \"user\",\n","        \"content\": base_prompt}\n","    ]\n","\n","    # Apply the chat template\n","    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n","                                          tokenize=False,\n","                                          add_generation_prompt=True)\n","    return prompt"]},{"cell_type":"markdown","metadata":{"id":"Xd4a4PokyTHR"},"source":["Looking good! Let's try our function out."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DW0Q6ZSfyTHR"},"outputs":[],"source":["\n","\n","query = random.choice(query_list)\n","print(f\"Query: {query}\")\n","\n","# Get relevant resources\n","scores, indices = retrieve_relevant_resources(query=query,\n","                                              embeddings=embeddings)\n","\n","# Create a list of context items\n","context_items = [pages_and_chunks[i] for i in indices]\n","\n","# Format prompt with context items\n","prompt = prompt_formatter(query=query,\n","                          context_items=context_items)\n","print(prompt)"]},{"cell_type":"code","source":["#o1-generated prompt from the text with changed variables:\n","\n","#Liivikehoite = 25-vuotinen ja Liivinmaan sota\n","\n","text = open(\"Liivikehoite\", \"r\")\n","vastaus = (text.read())\n","print(vastaus)\n","#Alla esitetään kolme kysymystä, jotka koskevat Liivinmaan sotaa sekä siihen liittyviä pitkittyneitä Pohjois-Euroopan konflikteja (pitkä viha, 25-vuotinen sota ja pohjoismainen viisikolmattavuotinen sota). Anna jokaiseen kysymykseen suomenkielinen vastaus samaan tyyliin kuin yllä olevissa esimerkeissä. Varmista, että vastaukset ovat mahdollisimman selittäviä ja perusteltuja.\n","\n","#Kysymys 1: Mikä oli Tanskan rooli Liivinmaan sodassa?\n","#Vastaus: Tanskan rooli Liivinmaan sodassa liittyi sen pyrkimyksiin laajentaa vaikutusvaltaansa Itämeren alueella ja estää kilpailijavaltioita, kuten Ruotsia, Puolaa ja Venäjää, saamasta kontrollia Baltian strategisista kauppa- ja satama-alueista. Tanska hallitsi aluksi osaa pohjoisen Liivinmaan alueista, erityisesti Eestimaan rannikkoseutuja, joita se pyrki pitämään hallinnassaan ja laajentamaan. Vaikka Tanskan rooli ei ollut yhtä hallitseva kuin Ruotsin tai Puolan, se oli merkittävä tekijä poliittisten ja sotilaallisten tasapainojen muovaamisessa, sillä sen tavoitteet ja liittoutumiset vaikuttivat sotatoimien kulkuun sekä alueen tulevaan valtajärjestykseen.\n","\n","#Kysymys 2: Mitä vaikutuksia sodalla oli Suomen siviiliväestöön?\n","#Vastaus: Liivinmaan sodan vaikutukset Suomen siviiliväestöön olivat moninaisia ja heijastuivat erityisesti arjen epävarmuutena sekä taloudellisena kuormituksena. Vaikka sotatoimet eivät aina ulottuneet syvälle Suomen alueelle, jatkuva uhka, resurssipula ja varustautuminen loivat raskaan ilmapiirin. Maanviljely kärsi työvoiman puutteesta ja verotuksen kiristymisestä, kun valtakunta keräsi varoja sotaponnisteluihin. Kauppa vaikeutui sodan aiheuttamien levottomuuksien ja kulkureittien katkosten vuoksi. Siviiliväestö joutui sopeutumaan jatkuvaan epävarmuuteen, pelkoon ja satunnaisiin kahakoihin raja-alueilla, mikä heikensi elinolosuhteita ja loi pitkäkestoista epäluottamusta tulevaisuuteen.\n","\n","#Kysymys 3: Miten Ruotsin ja Puolan välinen suhde muuttui sodan aikana?\n","#Vastaus: Liivinmaan sota muutti merkittävästi Ruotsin ja Puolan keskinäistä suhdetta, sillä alueiden hallinta ja Itämeren kauppareitit nousivat keskeisiksi kiistakohteiksi. Aiemmat jännitteet syvenivät, ja molemmat valtiot kilpailivat yhä voimakkaammin Baltian alueen herruudesta. Sodan aikana ja sen jälkeen Ruotsin ja Puolan oli neuvoteltava jatkuvasti valtapiireistään, mikä loi pitkään jatkuneen valtapoliittisen jännitteen. Vaikka ajoittaiset rauhansopimukset toivat hetkellistä tasapainoa, epäluottamus ja halu turvata omia etujaan säilyivät, mikä piti maiden suhteet kireinä myös tulevina vuosikymmeninä.\n"],"metadata":{"id":"qzyojfekiNIf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sqmc0wGxyTHS"},"source":["What a good looking prompt!\n","\n","We can tokenize this and pass it straight to our LLM."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TzTDUFAGyTHS"},"outputs":[],"source":["%%time\n","\n","input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","\n","# Generate an output of tokens\n","outputs = llm_model.generate(**input_ids,\n","                             temperature=0.7, # lower temperature = more deterministic outputs, higher temperature = more creative outputs\n","                             do_sample=True, # whether or not to use sampling, see https://huyenchip.com/2024/01/16/sampling.html for more\n","                             max_new_tokens=256) # how many new tokens to generate from prompt\n","\n","# Turn the output tokens into text\n","output_text = tokenizer.decode(outputs[0])\n","\n","print(f\"Query: {query}\")\n","print(f\"RAG answer:\\n{output_text.replace(prompt, '')}\")"]},{"cell_type":"markdown","metadata":{"id":"5p_ajBH8yTHS"},"source":["Yesssssss!!!\n","\n","Our RAG pipeline is complete!\n","\n","We just Retrieved, Augmented and Generated!\n","\n","And all on our own local GPU!\n","\n","How about we functionize the generation step to make it easier to use?\n","\n","We can put a little formatting on the text being returned to make it look nice too.\n","\n","And we'll make an option to return the context items if needed as well."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DpCg2OBuyTHT"},"outputs":[],"source":["query = vastaus\n","\n","def ask(query,\n","        temperature=0.7,\n","        max_new_tokens=512,\n","        format_answer_text=True,\n","        return_answer_only=True):\n","    \"\"\"\n","    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.\n","    \"\"\"\n","\n","    # Get just the scores and indices of top related results\n","    scores, indices = retrieve_relevant_resources(query=query,\n","                                                  embeddings=embeddings)\n","\n","    # Create a list of context items\n","    context_items = [pages_and_chunks[i] for i in indices]\n","\n","    # Add score to context item\n","    for i, item in enumerate(context_items):\n","        item[\"score\"] = scores[i].cpu() # return score back to CPU\n","\n","    # Format the prompt with context items\n","    prompt = prompt_formatter(query=query,\n","                              context_items=context_items)\n","\n","    # Tokenize the prompt\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","\n","    # Generate an output of tokens\n","    outputs = llm_model.generate(**input_ids,\n","                                 temperature=temperature,\n","                                 do_sample=True,\n","                                 max_new_tokens=max_new_tokens)\n","\n","    # Turn the output tokens into text\n","    output_text = tokenizer.decode(outputs[0])\n","\n","    if format_answer_text:\n","        # Replace special tokens and unnecessary help message\n","        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n","\n","    # Only return the answer without the context items\n","    if return_answer_only:\n","        return output_text\n","\n","    return output_text, context_items"]},{"cell_type":"markdown","metadata":{"id":"CA9XHrPAyTHT"},"source":["What a good looking function!\n","\n","The workflow could probably be a little refined but this should work!\n","\n","Let's try it out."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UXcUKGScyTHT"},"outputs":[],"source":["query = random.choice(query_list)\n","print(f\"Query: {query}\")\n","\n","# Answer query with context and return context\n","answer, context_items = ask(query=query,\n","                            temperature=0.7,\n","                            max_new_tokens=512,\n","                            return_answer_only=False)\n","\n","print(f\"Answer:\\n\")\n","print_wrapped(answer)\n","print(f\"Context items:\")\n","context_items"]},{"cell_type":"markdown","metadata":{"id":"nSiwu_euyTHT"},"source":["Local RAG workflow complete!\n","\n","We've now officially got a way to Retrieve, Augment and Generate answers based on a source.\n","\n","For now we can verify our answers manually by reading them and reading through the textbook.\n","\n","But if you want to put this into a production system, it'd be a good idea to have some kind of evaluation on how well our pipeline works.\n","\n","For example, you could use another LLM to rate the answers returned by our LLM and then use those ratings as a proxy evaluation.\n","\n","However, I'll leave this and a few more interesting ideas as extensions."]},{"cell_type":"markdown","metadata":{"id":"uoKVPwslyTHU"},"source":["## Extensions\n","\n","* May want to improve text extraction with something like Marker - https://github.com/VikParuchuri/marker\n","* Guide to more advanced PDF extraction - https://towardsdatascience.com/extracting-text-from-pdf-files-with-python-a-comprehensive-guide-9fc4003d517\n","* See the following prompt engineering resources for more prompting techniques - promptinguide.ai, Brex's Prompt Engineering Guide\n","* What happens when a query comes through that there isn't any context in the textbook on?\n","* Try another embedding model (e.g. Mixed Bread AI large, `mixedbread-ai/mxbai-embed-large-v1`, see: https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n","* Try another LLM... (e.g. Mistral-Instruct)\n","* Try different prompts (e.g. see prompting techniques online)\n","* Our example only focuses on text from a PDF, however, we could extend it to include figures and images\n","* Evaluate the answers -> could use another LLM to rate our answers (e.g. use GPT-4 to make)\n","* Vector database/index for larger setup (e.g. 100,000+ chunks)\n","* Libraries/frameworks such as LangChain / LlamaIndex can help do many of the steps for you - so it's worth looking into those next, wanted to recreate a workflow with lower-level tools to show the principles\n","* Optimizations for speed\n","    * See Hugging Face docs for recommended speed ups on GPU - https://huggingface.co/docs/transformers/perf_infer_gpu_one\n","    * Optimum NVIDIA - https://huggingface.co/blog/optimum-nvidia, GitHub: https://github.com/huggingface/optimum-nvidia\n","    * See NVIDIA TensorRT-LLM - https://github.com/NVIDIA/TensorRT-LLM\n","    * See GPT-Fast for PyTorch-based optimizations - https://github.com/pytorch-labs/gpt-fast\n","    * Flash attention 2 (requires Ampere GPUs or newer) - https://github.com/Dao-AILab/flash-attention\n","* Stream text output so it looks prettier (e.g. each token appears as it gets output from the model)\n","* Turn the workflow into an app, see Gradio type chatbots for this - https://www.gradio.app/guides/creating-a-chatbot-fast, see local example: https://www.gradio.app/guides/creating-a-chatbot-fast#example-using-a-local-open-source-llm-with-hugging-face"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"1PKdKTvSM_twbKTga0GGtzOEIrzzibHaT","timestamp":1734539436762}]},"kernelspec":{"display_name":"Python (myenv)","language":"python","name":"myenv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"269a2daf7dca430cbf7b5f50e109a44c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d97d0e3aee5240b1922142bb1e490e96","IPY_MODEL_9e38047c1cf34a6ea4a5de494ad8aa73","IPY_MODEL_08ad642053a24789829c8be4cc57c535"],"layout":"IPY_MODEL_9457b8fb15b640c1ba58b57ccedf962e"}},"d97d0e3aee5240b1922142bb1e490e96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_afa35a3fcda041a1a459fd627cfd15ae","placeholder":"​","style":"IPY_MODEL_871d5bd9f2434977bdcc8106761da7f4","value":""}},"9e38047c1cf34a6ea4a5de494ad8aa73":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b51ad28368a47c8a2164d87afac42c3","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8caf4a37bd0a4f0a9b7051baca00c229","value":1}},"08ad642053a24789829c8be4cc57c535":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3410af54b4a64ae7b0fdc53dfef22924","placeholder":"​","style":"IPY_MODEL_575650e705744edab2f5f1c93194014b","value":" 806/? [00:01&lt;00:00, 729.45it/s]"}},"9457b8fb15b640c1ba58b57ccedf962e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afa35a3fcda041a1a459fd627cfd15ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"871d5bd9f2434977bdcc8106761da7f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4b51ad28368a47c8a2164d87afac42c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"8caf4a37bd0a4f0a9b7051baca00c229":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3410af54b4a64ae7b0fdc53dfef22924":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"575650e705744edab2f5f1c93194014b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b5bffac562f4c069161a29a8912fe83":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4dd0ff3297c0423ca7cb9de4ee4bd489","IPY_MODEL_06fc34d7727c4e149afad683fce5e141","IPY_MODEL_84c62366f81e4b1ab3f3ab6b3475159c"],"layout":"IPY_MODEL_de8547414ba6456587519aeab3b070aa"}},"4dd0ff3297c0423ca7cb9de4ee4bd489":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f91f95529476491ba8bf0bef15f7f151","placeholder":"​","style":"IPY_MODEL_b7d550a0550542c4b198aa05c6fdeb34","value":"100%"}},"06fc34d7727c4e149afad683fce5e141":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f2fab2c96e04e45a44a4e9067439b09","max":806,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e59235cceb9c46bbb2713a216e8286d9","value":806}},"84c62366f81e4b1ab3f3ab6b3475159c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_640f340daaf34838b142fa6ed9318efa","placeholder":"​","style":"IPY_MODEL_2ea5cc112a644562bc84290a74cd5778","value":" 806/806 [00:02&lt;00:00, 288.17it/s]"}},"de8547414ba6456587519aeab3b070aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f91f95529476491ba8bf0bef15f7f151":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7d550a0550542c4b198aa05c6fdeb34":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f2fab2c96e04e45a44a4e9067439b09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e59235cceb9c46bbb2713a216e8286d9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"640f340daaf34838b142fa6ed9318efa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ea5cc112a644562bc84290a74cd5778":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d9e5515a5b8143338b9a3efc79e69011":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c426b488faca43218a9b92af39cf79fc","IPY_MODEL_b1b98728e8f243758b413da90830121b","IPY_MODEL_a267d8fda61c4be3a32922f65d32c352"],"layout":"IPY_MODEL_065b5b8a5892465da087a1ef4399b3c4"}},"c426b488faca43218a9b92af39cf79fc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e340c6009d5346239be90c74501e84df","placeholder":"​","style":"IPY_MODEL_65b97e58fdfc42be9449c9e6ff728d9b","value":"tokenizer_config.json: 100%"}},"b1b98728e8f243758b413da90830121b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e33ad5526eb34d46b31e662860dd6550","max":1268,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8d451cc0481d4d08801faee82b3c0b4b","value":1268}},"a267d8fda61c4be3a32922f65d32c352":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d22c9afd161452fb27d8004f8a8b33f","placeholder":"​","style":"IPY_MODEL_0ccf0ae5e30c4bf386e6695c513d6b4d","value":" 1.27k/1.27k [00:00&lt;00:00, 86.0kB/s]"}},"065b5b8a5892465da087a1ef4399b3c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e340c6009d5346239be90c74501e84df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65b97e58fdfc42be9449c9e6ff728d9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e33ad5526eb34d46b31e662860dd6550":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d451cc0481d4d08801faee82b3c0b4b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9d22c9afd161452fb27d8004f8a8b33f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ccf0ae5e30c4bf386e6695c513d6b4d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a3d6c760faa4636812ac807fe5fe97a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_47b35ccf6b2e4d5aa4c9e5f44c3477a9","IPY_MODEL_90deb48eba0f4d799766ba4b8b4e8208","IPY_MODEL_36576091012d453e96e9cc89ffdfc23a"],"layout":"IPY_MODEL_60b551776a36455b85917a96481655bf"}},"47b35ccf6b2e4d5aa4c9e5f44c3477a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02ff9932871b4f798eacb23175d5d20f","placeholder":"​","style":"IPY_MODEL_384bb7b6e1724fddb768e5032bc884ef","value":"vocab.json: 100%"}},"90deb48eba0f4d799766ba4b8b4e8208":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_44b4ccc6cc0a4242bc7c7acd16b1a434","max":894176,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a1ea44054c9c41bf8dc9fe9f67f5c15d","value":894176}},"36576091012d453e96e9cc89ffdfc23a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed06f8cbcdbf4b0db1db368df6762a21","placeholder":"​","style":"IPY_MODEL_c5f2ecbb36af4645a69bdec35a20da33","value":" 894k/894k [00:00&lt;00:00, 5.56MB/s]"}},"60b551776a36455b85917a96481655bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02ff9932871b4f798eacb23175d5d20f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"384bb7b6e1724fddb768e5032bc884ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44b4ccc6cc0a4242bc7c7acd16b1a434":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1ea44054c9c41bf8dc9fe9f67f5c15d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ed06f8cbcdbf4b0db1db368df6762a21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5f2ecbb36af4645a69bdec35a20da33":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea2f3eb7625e4615b07c02b6f7f1c966":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_681b4841a5be49df867110cdc37deaaf","IPY_MODEL_0491fae7f47f4387a17992b6d4b6b2fd","IPY_MODEL_e7a36c6ed744444d83f415ad82244ea4"],"layout":"IPY_MODEL_3397e3a4fb7b44aaa862e80154c31e83"}},"681b4841a5be49df867110cdc37deaaf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0dad2c4ef7f84d30a5617485fd88e313","placeholder":"​","style":"IPY_MODEL_8b270b71f7784f74889fae69b9c68209","value":"merges.txt: 100%"}},"0491fae7f47f4387a17992b6d4b6b2fd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8bf1c82f8cc4566bfb495eea7034650","max":552420,"min":0,"orientation":"horizontal","style":"IPY_MODEL_59b754ecf7c547ca91c8a0092d9d9dcd","value":552420}},"e7a36c6ed744444d83f415ad82244ea4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_331dc725fe0a4cdabe8f934ddef65c87","placeholder":"​","style":"IPY_MODEL_db23974cd2164bbb91767a754d3d4257","value":" 552k/552k [00:00&lt;00:00, 29.1MB/s]"}},"3397e3a4fb7b44aaa862e80154c31e83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0dad2c4ef7f84d30a5617485fd88e313":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b270b71f7784f74889fae69b9c68209":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a8bf1c82f8cc4566bfb495eea7034650":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59b754ecf7c547ca91c8a0092d9d9dcd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"331dc725fe0a4cdabe8f934ddef65c87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db23974cd2164bbb91767a754d3d4257":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f2e6ed4fbf8d488ea65e1f2966ac1ecd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_49b6751252b74537a298313b12f6bff1","IPY_MODEL_449d668f64f140e78c2d19cfbb2195fd","IPY_MODEL_aecb1f8916894298aed3e234fcc0db88"],"layout":"IPY_MODEL_a091618115df49ce8d532a8414cd97c2"}},"49b6751252b74537a298313b12f6bff1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ad3f3b91eba41bca97d479e09cce0a0","placeholder":"​","style":"IPY_MODEL_e5a7f7ae42f94cd18750d0f8d5923562","value":"tokenizer.json: 100%"}},"449d668f64f140e78c2d19cfbb2195fd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_65551abf3c0b419ca8b88de382bf176b","max":2300476,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7b1e07747d5c47de87258b5bf697f785","value":2300476}},"aecb1f8916894298aed3e234fcc0db88":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f176d307feb7467ab5a738beb84c62ee","placeholder":"​","style":"IPY_MODEL_65f12179f1374e059f5cef735f3d752e","value":" 2.30M/2.30M [00:00&lt;00:00, 7.35MB/s]"}},"a091618115df49ce8d532a8414cd97c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ad3f3b91eba41bca97d479e09cce0a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5a7f7ae42f94cd18750d0f8d5923562":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65551abf3c0b419ca8b88de382bf176b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b1e07747d5c47de87258b5bf697f785":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f176d307feb7467ab5a738beb84c62ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65f12179f1374e059f5cef735f3d752e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9702475ec2944e4b5e00c0ff7a601c0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6a7b01281eed4c20976a82d135d9eb2a","IPY_MODEL_5c684c08d7a6449183176b633486ce02","IPY_MODEL_4aea1bd03bf34a9fb0b0bda7f2be6623"],"layout":"IPY_MODEL_1a20bd247f85499498b48978bf0816d5"}},"6a7b01281eed4c20976a82d135d9eb2a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b1804943b054fc8bbcc7a6b765d265e","placeholder":"​","style":"IPY_MODEL_b1d239fae99a4ce89d3f6a39a063e8eb","value":"special_tokens_map.json: 100%"}},"5c684c08d7a6449183176b633486ce02":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_58967ff037bb454cba710d4b25b3d6ef","max":295,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fdea44dea216431482312d4af49157f6","value":295}},"4aea1bd03bf34a9fb0b0bda7f2be6623":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f524bab7f7548e1ac1ac226cea0144e","placeholder":"​","style":"IPY_MODEL_c7b942cb9bfa4ad497a0b9e2756af6d3","value":" 295/295 [00:00&lt;00:00, 24.6kB/s]"}},"1a20bd247f85499498b48978bf0816d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b1804943b054fc8bbcc7a6b765d265e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1d239fae99a4ce89d3f6a39a063e8eb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58967ff037bb454cba710d4b25b3d6ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdea44dea216431482312d4af49157f6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6f524bab7f7548e1ac1ac226cea0144e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7b942cb9bfa4ad497a0b9e2756af6d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b6fdfc879494fed81143086a9506744":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1d5478a382774db19c95a93aee876f28","IPY_MODEL_6f47aee176624102a614f6a758d1f911","IPY_MODEL_2b1b4870262842f4be8b550b1683a0c7"],"layout":"IPY_MODEL_e8b35554292645ff8f1118dc254c9052"}},"1d5478a382774db19c95a93aee876f28":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48ce176cc387467ca45642790f58c792","placeholder":"​","style":"IPY_MODEL_ea4bb2bfc3b945afa6fb9b30cfcf9a56","value":"100%"}},"6f47aee176624102a614f6a758d1f911":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6cb3f4cb53254af7aa776db9159fda66","max":806,"min":0,"orientation":"horizontal","style":"IPY_MODEL_308de8bd98eb4442b7ab1f1f1fa2b800","value":806}},"2b1b4870262842f4be8b550b1683a0c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed3364fb297d48fbb9f050f54be488c0","placeholder":"​","style":"IPY_MODEL_d0bf09202afc4602a7af03d24c47be8c","value":" 806/806 [00:00&lt;00:00, 49323.87it/s]"}},"e8b35554292645ff8f1118dc254c9052":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48ce176cc387467ca45642790f58c792":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea4bb2bfc3b945afa6fb9b30cfcf9a56":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6cb3f4cb53254af7aa776db9159fda66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"308de8bd98eb4442b7ab1f1f1fa2b800":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ed3364fb297d48fbb9f050f54be488c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0bf09202afc4602a7af03d24c47be8c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05c51eefd14146a8ba40a608f133fd36":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c6f66b75d4ef48059f7bb0f6f4ba7439","IPY_MODEL_b8796628f457451ba54a37ce814b775b","IPY_MODEL_6e0cf4fe1370470392ebe4d9a1932659"],"layout":"IPY_MODEL_034a55be84424bb082b92c179ceac4a6"}},"c6f66b75d4ef48059f7bb0f6f4ba7439":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ff68361a8454779b45f16a2e7f51025","placeholder":"​","style":"IPY_MODEL_19f2807d12a14f2cb4df2f79b4139765","value":"100%"}},"b8796628f457451ba54a37ce814b775b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_89c9bc5fe9884541a9a6e19808b16f11","max":806,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6e7c43a023fa4305861f5b725291d503","value":806}},"6e0cf4fe1370470392ebe4d9a1932659":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3ac78ee969745659c758e63de4356a9","placeholder":"​","style":"IPY_MODEL_35e9e72addf649cda6830f95aa4a7415","value":" 806/806 [00:00&lt;00:00, 12624.62it/s]"}},"034a55be84424bb082b92c179ceac4a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ff68361a8454779b45f16a2e7f51025":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19f2807d12a14f2cb4df2f79b4139765":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89c9bc5fe9884541a9a6e19808b16f11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e7c43a023fa4305861f5b725291d503":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b3ac78ee969745659c758e63de4356a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35e9e72addf649cda6830f95aa4a7415":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}