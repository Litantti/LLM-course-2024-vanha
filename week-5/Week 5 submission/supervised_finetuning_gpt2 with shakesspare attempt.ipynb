{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gb-GEv7to4Tl"
   },
   "source": [
    "# Fine-tuning a Large Language Model\n",
    "\n",
    "In this lecture we will be looking at how to fine-tune an existing pre-trained language model.\n",
    "\n",
    "## Learning outcomes\n",
    "* You will learn how to download a pre-trained model and a training dataset from Hugging Face.\n",
    "* You will learn how to fine-tune the downloaded model with the dataset using Hugging Face trl library and the supervised fine-tuning (SFT) method.\n",
    "* You will learn how to use the fine-tuned model to generate text based on user input / prompts.\n",
    "* You will learn how to upload the fine-tuned model to your own Hugging Face repository so that it can be used later or shared with other users.\n",
    "\n",
    "## Prerequistes\n",
    "* You will need the following free accounts: Google, Hugging Face and Weights & Biases. You may use your existing accounts or create new accounts for the purposes of this course.\n",
    "* We will use the [Hugging Face](https://huggingface.co/) libraries: transformers (for models), datasets (for datasets), trl (for training). We will also store the fine-tuned models in a Hugging Face repository.\n",
    "* Training is done using [Google Colab](https://colab.research.google.com/), which provides free access to Jupyter notebooks backed with a GPU compute required for fine-tuning.\n",
    "* For monitoring the training run we will use [Weights & Biases](https://wandb.ai/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWKnP1c_o4To"
   },
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAAj4LLio4Tp"
   },
   "source": [
    "Let's first install some pre-requisites using Python's package manager pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26064,
     "status": "ok",
     "timestamp": 1732227954205,
     "user": {
      "displayName": "Turkoosi",
      "userId": "15294961855769584895"
     },
     "user_tz": -120
    },
    "id": "lioAFETZWlKi",
    "outputId": "d5f8b8d2-60a9-4fda-8f98-3798a3576f24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.9/310.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers peft accelerate\n",
    "!pip install -q trl xformers wandb datasets einops sentencepiece bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUcWpVBco4Tq"
   },
   "source": [
    "Then we need to import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2520,
     "status": "ok",
     "timestamp": 1732231141290,
     "user": {
      "displayName": "Turkoosi",
      "userId": "15294961855769584895"
     },
     "user_tz": -120
    },
    "id": "NdjM7XroWqEx",
    "outputId": "20be9451-66a7-4f2a-fa33-e21a6bb39ea8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"openai-community/gpt2\")\n",
    "\n",
    "# Load model directly\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "###\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, TextStreamer\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "import torch, wandb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sycJ8ygSo4Tr"
   },
   "source": [
    "We will download a pre-trained large language model from Hugging Face and a dataset to train the model with. Below we assign these to variables we will use later. We will also set the name of the repository and model for the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTq15hgUW8qB"
   },
   "outputs": [],
   "source": [
    "# Pre trained model\n",
    "#model_name = \"mistralai/Mistral-7B-v0.3\"\n",
    "model_name = \"openai-community/gpt2\"\n",
    "\n",
    "# Dataset name\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Israhassan/Shakespeare\")\n",
    "\n",
    "dataset_name = \"Israhassan/Shakespeare\"\n",
    "#\"vicgalle/alpaca-gpt4\"\n",
    "#https://huggingface.co/datasets/BEE-spoke-data/wikipedia-20230901.en-deduped/resolve/main/README.md\n",
    "\n",
    "\n",
    "# Hugging face repository link to save fine-tuned model(Create new repository in huggingface,copy and paste here)\n",
    "new_model = \"Litantti/vkneljae\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IVmS2Jco4Ts"
   },
   "source": [
    "To access your Hugging Face account, you need to log in. First go to your Hugging Face account, click *Settings* and select *Access Tokens*. Create a new token and copy the token. Then execute the below login command and when asked paste an access token.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "d1f2f7668d444de1ae5e1823c7418eda",
      "6dd0f42e40834659a5d0af25f4b1dcbc",
      "7d547ac1d67346a09ccebc3f68fe2732",
      "8d25e792e3434a40bddd5ca32ecfb00e",
      "a552768046e44779a5762f24054094c5",
      "7a844e3e912e4b8a869d9ee5fd87f420",
      "872147c0859a4f5e99c3f1a1a5c9678f",
      "b42d72766a7245ffb233c8492a577ed6",
      "222cabbaad2d4dfdabd85341d6ecc3ac",
      "3f11a672cb654324b2d897ca7b84c9a0",
      "4374d6a5b4734beca9c1b3b7dd637836",
      "4d78fa5f573345149326478c4accc0cf",
      "e1b431abf1c1408f8e6ac2b11e056735",
      "af07c80607424a8eb05ed7cc5152d78f",
      "d5a8af79e64647449485e64425131073",
      "5347a7cd9f3f4df3b47afe4abee76e3c",
      "9ebf9db99fde47b6a973ac9c70976938",
      "e09e8a2853804dfc9ce4f365b563ced6",
      "0922c712e72b45e7a366469ef94943c2",
      "957912ebca8e4a6a981405931b15de75"
     ]
    },
    "executionInfo": {
     "elapsed": 1451,
     "status": "ok",
     "timestamp": 1732231549050,
     "user": {
      "displayName": "Turkoosi",
      "userId": "15294961855769584895"
     },
     "user_tz": -120
    },
    "id": "xnwXVf_dXeIp",
    "outputId": "909bba86-64e0-4f99-a9ae-d718e19a95b9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f2f7668d444de1ae5e1823c7418eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxGEXDv3o4Tt"
   },
   "source": [
    "Let's then download a subset of the dataset we want to use. Below we limit the dataset to the first 10,000 examples in order to save time. In real life you would probably use the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "executionInfo": {
     "elapsed": 2564,
     "status": "ok",
     "timestamp": 1732231557984,
     "user": {
      "displayName": "Turkoosi",
      "userId": "15294961855769584895"
     },
     "user_tz": -120
    },
    "id": "Bt_36Xa5Xuub",
    "outputId": "b42a50d8-23a3-410e-85b9-085b72452715"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"We must not suppose that Othello's account of his courtship in hisfamous speech before the Senate is intended to be exhaustive. He isaccused of having used drugs or charms in order to win Desdemona; andtherefore his purpose in his defence is merely to show that hiswitchcraft was the story of his life. It is no part of his business totrouble the Senators with the details of his courtship, and he socondenses his narrative of it that it almost appears as though there wasno courtship at all, and as though Desdemona never imagined that he wasin love with her until she had practically confessed her love for him. Hence she has been praised by some for her courage, and blamed by othersfor her forwardness. But at III. iii. 70 f. matters are presented in quite a new light. Therewe find the following words of hers:                             What! Michael Cassio,     That came a-wooing with you, and so many a time,     When I have spoke of you dispraisingly,     Hath ta'en your part. It seems, then, she understood why Othello came so often to her father'shouse, and was perfectly secure of his love before she gave him thatvery broad 'hint to speak. ' I may add that those who find fault with herforget that it was necessary for her to take the first open step. Shewas the daughter of a Venetian grandee, and Othello was a black soldierof fortune. 2. We learn from the lines just quoted that Cassio used to accompanyOthello in his visits to the house; and from III. iii. 93 f. we learnthat he knew of Othello's love from first to last and 'went between' thelovers 'very oft. ' Yet in Act I. it appears that, while Iago on thenight of the marriage knows about it and knows where to find Othello (I. i. 158 f. ), Cassio, even if he knows where to find Othello (which isdoubtful: see I. ii. 44), seems to know nothing about the marriage. SeeI. ii. 49:     _Cas. _   Ancient, what makes he here?     _Iago. _  'Faith, he to-night hath boarded a land carack:              If it prove lawful prize, he's made for ever.     _Cas. _   I do not understand.     _Iago. _                   He's married.     _Cas. _                               To who? It is possible that Cassio does know, and only pretends ignorancebecause he has not been informed by Othello that Iago also knows. Andthis idea is consistent with Iago's apparent ignorance of Cassio's partin the courtship (III. iii. 93). And of course, if this were so, a wordfrom Shakespeare to the actor who played Cassio would enable him to makeall clear to the audience. The alternative, and perhaps more probable,explanation would be that, in writing Act I. , Shakespeare had not yetthought of making Cassio Othello's confidant, and that, after writingAct III. , he neglected to alter the passage in Act I. In that case thefurther information which Act III. gives regarding Othello's courtshipwould probably also be an after-thought. NOTE L. OTHELLO IN THE TEMPTATION SCENE. One reason why some readers think Othello 'easily jealous' is that theycompletely misinterpret him in the early part of this scene. They fancythat he is alarmed and suspicious the moment he hears Iago mutter 'Ha! Ilike not that,' as he sees Cassio leaving Desdemona (III. iii. 35). But,in fact, it takes a long time for Iago to excite surprise, curiosity,and then grave concern--by no means yet jealousy--even about Cassio; andit is still longer before Othello understands that Iago is suggestingdoubts about Desdemona too. ('Wronged' in 143 certainly does not referto her, as 154 and 162 show. ) Nor, even at 171, is the exclamation 'Omisery' meant for an expression of Othello's own present feelings; ashis next speech clearly shows, it expresses an _imagined_ feeling, asalso the speech which elicits it professes to do (for Iago would nothave dared here to apply the term 'cuckold' to Othello). In fact it isnot until Iago hints that Othello, as a foreigner, might easily bedeceived, that he is seriously disturbed about Desdemona. Salvini played this passage, as might be expected, with entireunderstanding. Nor have I ever seen it seriously misinterpreted on thestage. I gather from the Furness Variorum that Fechter and Edwin Boothtook the same view as Salvini. Actors have to ask themselves what wasthe precise state of mind expressed by the words they have to repeat. But many readers never think of asking such a question. The lines which probably do most to lead hasty or unimaginative readersastray are those at 90, where, on Desdemona's departure, Othelloexclaims to himself:     Excellent wretch! Perdition catch my soul     But I do love thee! and when I love thee not,     Chaos is come again. He is supposed to mean by the last words that his love is _now_suspended by suspicion, whereas in fact, in his bliss, he has so totallyforgotten Iago's 'Ha! I like not that,' that the tempter has to beginall over again. The meaning is, 'If ever I love thee not, Chaos willhave come again. ' The feeling of insecurity is due to the excess of_joy_, as in the wonderful words after he rejoins Desdemona at Cyprus(II. i. 191):                     If it were now to die,     'Twere now to be most happy: for, I fear     My soul hath her content so absolute     That not another comfort like to this     Succeeds in unknown fate. If any reader boggles at the use of the present in 'Chaos _is_ comeagain,' let him observe 'succeeds' in the lines just quoted, or let himlook at the parallel passage in _Venus and Adonis_, 1019:     For, he being dead, with him is beauty slain;     And, beauty dead, black Chaos comes again. Venus does not know that Adonis is dead when she speaks thus. NOTE M. QUESTIONS AS TO _OTHELLO_, ACT IV. SCENE I. (1) The first part of the scene is hard to understand, and thecommentators give little help. I take the idea to be as follows. Iagosees that he must renew his attack on Othello; for, on the one hand,Othello, in spite of the resolution he had arrived at to put Desdemonato death, has taken the step, without consulting Iago, of testing her inthe matter of Iago's report about the handkerchief; and, on the otherhand, he now seems to have fallen into a dazed lethargic state, and mustbe stimulated to action. Iago's plan seems to be to remind Othello ofeverything that would madden him again, but to do so by professing tomake light of the whole affair, and by urging Othello to put the bestconstruction on the facts, or at any rate to acquiesce. So he says, ineffect: 'After all, if she did kiss Cassio, that might mean little. Nay,she might even go much further without meaning any harm. [266] Of coursethere is the handkerchief (10); but then why should she _not_ give itaway? ' Then, affecting to renounce this hopeless attempt to disguise histrue opinion, he goes on: 'However, _I_ cannot, as your friend, pretendthat I really regard her as innocent: the fact is, Cassio boasted to mein so many words of his conquest. [Here he is interrupted by Othello'sswoon. ] But, after all, why make such a fuss? You share the fate of mostmarried men, and you have the advantage of not being deceived in thematter. ' It must have been a great pleasure to Iago to express his realcynicism thus, with the certainty that he would not be taken seriouslyand would advance his plot by it. At 208-210 he recurs to the same planof maddening Othello by suggesting that, if he is so fond of Desdemona,he had better let the matter be, for it concerns no one but him. Thisspeech follows Othello's exclamation 'O Iago, the pity of it,' and thisis perhaps the moment when we most of all long to destroy Iago. (2) At 216 Othello tells Iago to get him some poison, that he may killDesdemona that night. Iago objects: 'Do it not with poison: strangle herin her bed, even the bed she hath contaminated? ' Why does he object topoison? Because through the sale of the poison he himself would beinvolved? Possibly. Perhaps his idea was that, Desdemona being killed byOthello, and Cassio killed by Roderigo, he would then admit that he hadinformed Othello of the adultery, and perhaps even that he hadundertaken Cassio's death; but he would declare that he never meant tofulfil his promise as to Cassio, and that he had nothing to do withDesdemona's death (he seems to be preparing for this at 285). His buyingpoison might wreck this plan. But it may be that his objection to poisonsprings merely from contempt for Othello's intellect. He can trust himto use violence, but thinks he may bungle anything that requiresadroitness. (3) When the conversation breaks off here (225) Iago has brought Othelloback to the position reached at the end of the Temptation scene (III. iii. ). Cassio and Desdemona are to be killed; and, in addition, the timeis hastened; it is to be 'to-night,' not 'within three days. 'The constructional idea clearly is that, after the Temptation scene,Othello tends to relapse and wait, which is terribly dangerous to Iago,who therefore in this scene quickens his purpose. Yet Othello relapsesagain. He has declared that he will not expostulate with her (IV. i. 217). But he cannot keep his word, and there follows the scene ofaccusation. Its _dramatic_ purposes are obvious, but Othello seems tohave no purpose in it. He asks no questions, or, rather, none that showsthe least glimpse of doubt or hope. He is merely torturing himself. FOOTNOTES:[Footnote 266: The reader who is puzzled by this passage should refer tothe conversation at the end of the thirtieth tale in the _Heptameron_. ]NOTE N. TWO PASSAGES IN THE LAST SCENE OF _OTHELLO_. (1) V. ii. 71 f. Desdemona demands that Cassio be sent for to 'confess'the truth that she never gave him the handkerchief. Othello answers thatCassio _has_ confessed the truth--has confessed the adultery. Thedialogue goes on:     _Des. _  He will not say so.     _Oth. _                    No, his mouth is stopp'd:             Honest Iago hath ta'en order for 't.     _Des. _  O! my fear interprets: what, is he dead?     _Oth. _  Had all his hairs been lives, my great revenge             Had stomach for them all.     _Des. _  Alas! he is _betray'd_ and _I_ undone. It is a ghastly idea, but I believe Shakespeare means that, at themention of Iago's name, Desdemona suddenly sees that _he_ is the villainwhose existence he had declared to be impossible when, an hour before,Emilia had suggested that someone had poisoned Othello's mind. But herwords rouse Othello to such furious indignation ('Out, strumpet! Weep'stthou for him to my face? ') that 'it is too late. '(2) V. ii. 286 f.     _Oth. _  I look down towards his feet; but that's a fable.             If that thou be'st a devil, I cannot kill thee.                                           [_Wounds Iago. _     _Lod. _  Wrench his sword from him.     _Iago. _                      I bleed, sir, but not killed. Are Iago's strange words meant to show his absorption of interest inhimself amidst so much anguish? I think rather he is meant to bealluding to Othello's words, and saying, with a cold contemptuous smile,'You see he is right; I _am_ a devil. 'NOTE O. OTHELLO ON DESDEMONA'S LAST WORDS. I have said that the last scene of _Othello_, though terribly painful,contains almost nothing to diminish the admiration and love whichheighten our pity for the hero (p. 198). I said 'almost' in view of thefollowing passage (V. ii. 123 ff. ):     _Emil. _ O, who hath done this deed?     _Des. _  Nobody; I myself. Farewell:             Commend  me to my kind lord: O, farewell!    [_Dies. _     _Oth. _  Why, how should she be murdered? [267]     _Emil. _                                Alas, who knows?     _Oth. _  You heard her say herself, it was not I.     _Emil. _ She said so: I must needs report the truth.     _Oth. _  She's, like a liar, gone to burning hell:             'Twas I that kill'd her.     _Emil. _                         O, the more angel she,             And you the blacker devil!     _Oth. _  She turn'd to folly, and she was a whore. This is a strange passage. What did Shakespeare mean us to feel? One isastonished that Othello should not be startled, nay thunder-struck, whenhe hears such dying words coming from the lips of an obdurateadulteress. One is shocked by the moral blindness or obliquity whichtakes them only as a further sign of her worthlessness. Here alone, Ithink, in the scene sympathy with Othello quite disappears. DidShakespeare mean us to feel thus, and to realise how completely confusedand perverted Othello's mind has become? I suppose so: and yet Othello'swords continue to strike me as very strange, and also as not _like_Othello,--especially as at this point he was not in anger, much lessenraged. It has sometimes occurred to me that there is a touch ofpersonal animus in the passage. One remembers the place in _Hamlet_(written but a little while before) where Hamlet thinks he is unwillingto kill the King at his prayers, for fear they may take him to heaven;and one remembers Shakespeare's irony, how he shows that those prayersdo _not_ go to heaven, and that the soul of this praying murderer is atthat moment as murderous as ever (see p. 171), just as here the soul ofthe lying Desdemona is angelic _in_ its lie. Is it conceivable that inboth passages he was intentionally striking at conventional 'religious'ideas; and, in particular, that the belief that a man's everlasting fateis decided by the occupation of his last moment excited in himindignation as well as contempt? I admit that this fancy seemsun-Shakespearean, and yet it comes back on me whenever I read thispassage. [The words 'I suppose so' (l. 3 above) gave my conclusion; butI wish to withdraw the whole Note]FOOTNOTES:[Footnote 267: He alludes to her cry, 'O falsely, falsely murder'd! ']NOTE P. DID EMILIA SUSPECT IAGO? I have answered No (p. 216), and have no doubt about the matter; but atone time I was puzzled, as perhaps others have been, by a single phraseof Emilia's. It occurs in the conversation between her and Iago andDesdemona (IV. ii. 130 f. ):     I will be hang'd if some eternal villain,     Some busy and insinuating rogue,     Some cogging, cozening slave, _to get some office_,     Have not devised this slander; I'll be hang'd else. Emilia, it may be said, knew that Cassio was the suspected man, so thatshe must be thinking of _his_ office, and must mean that Iago haspoisoned Othello's mind in order to prevent his reinstatement and to getthe lieutenancy for himself. And, it may be said, she speaksindefinitely so that Iago alone may understand her (for Desdemona doesnot know that Cassio is the suspected man). Hence too, it may be said,when, at V. ii. 190, she exclaims,                  Villany, villany, villany!     I think upon't, I think: I smell't: O villany!     _I thought so then:_--I'll kill myself for grief;she refers in the words italicised to the occasion of the passage in IV. ii. , and is reproaching herself for not having taken steps on hersuspicion of Iago. I have explained in the text why I think it impossible to suppose thatEmilia suspected her husband; and I do not think anyone who follows herspeeches in V. ii. , and who realises that, if she did suspect him, shemust have been simply _pretending_ surprise when Othello told her thatIago was his informant, will feel any doubt. Her idea in the lines atIV. ii. 130 is, I believe, merely that someone is trying to establish aground for asking a favour from Othello in return for information whichnearly concerns him. It does not follow that, because she knew Cassiowas suspected, she must have been referring to Cassio's office. She wasa stupid woman, and, even if she had not been, she would not put two andtwo together so easily as the reader of the play. In the line,     I thought so then: I'll kill myself for grief,I think she certainly refers to IV. ii. 130 f. and also IV. ii. 15(Steevens's idea that she is thinking of the time when she let Iago takethe handkerchief is absurd). If 'I'll kill myself for grief' is to betaken in close connection with the preceding words (which is notcertain), she may mean that she reproaches herself for not having actedon her general suspicion, or (less probably) that she reproaches herselffor not having suspected that Iago was the rogue. With regard to my view that she failed to think of the handkerchief whenshe saw how angry Othello was, those who believe that she did think ofit will of course also believe that she suspected Iago. But in additionto other difficulties, they will have to suppose that her astonishment,when Othello at last mentioned the handkerchief, was mere acting. Andanyone who can believe this seems to me beyond argument. [I regret thatI cannot now discuss some suggestions made to me in regard to thesubjects of Notes O and P. ]NOTE Q. IAGO'S SUSPICION REGARDING CASSIO AND EMILIA. The one expression of this suspicion appears in a very curious manner. Iago, soliloquising, says (II. i. 311):                          Which thing to do,     If this poor trash of Venice, whom I trash     For his quick hunting, stand the putting on,     I'll have our Michael Cassio on the hip,     Abuse him to the Moor in the rank [F. right] garb--     For I fear Cassio with my night-cap too--     Make the Moor thank me, etc. Why '_For_ I fear Cassio,' etc. ? He can hardly be giving himself anadditional reason for involving Cassio; the parenthesis must beexplanatory of the preceding line or some part of it. I think itexplains 'rank garb' or 'right garb,' and the meaning is, 'For Cassio_is_ what I shall accuse him of being, a seducer of wives. ' He isreturning to the thought with which the soliloquy begins, 'That Cassioloves her, I do well believe it. ' In saying this he is unconsciouslytrying to believe that Cassio would at any rate _like_ to be anadulterer, so that it is not so very abominable to say that he _is_ one. And the idea 'I suspect him with Emilia' is a second and strongerattempt of the same kind. The idea probably was born and died in onemoment. It is a curious example of Iago's secret subjection to morality. NOTE R. REMINISCENCES OF _OTHELLO_ IN _KING LEAR_. The following is a list, made without any special search, and doubtlessincomplete, of words and phrases in _King Lear_ which recall words andphrases in _Othello_, and many of which occur only in these two plays:     'waterish,' I. i. 261, appears only here and in _O. _     III. iii. 15.     'fortune's alms,' I. i. 281, appears only here and in     _O. _ III. iv. 122.     'decline' seems to be used of the advance of age only in     I. ii. 78 and _O. _ III. iii. 265.     'slack' in 'if when they chanced to slack you,' II.     iv. 248, has no exact parallel in Shakespeare, but recalls     'they slack their duties,' _O. _ IV. iii. 88.     'allowance' (=authorisation), I. iv. 228, is used     thus only in _K. L. _, _O. _ I. i. 128, and two places     in _Hamlet_ and _Hen. VIII. _     'besort,' vb. , I. iv. 272, does not occur elsewhere,     but 'besort,' sb. , occurs in _O. _ I. iii. 239 and     nowhere else.     Edmund's 'Look, sir, I bleed,' II. i. 43, sounds like     an echo of Iago's 'I bleed, sir, but not killed,' _O. _     V. ii. 288.     'potential,' II. i. 78, appears only here, in _O. _     I. ii. 13, and in the _Lover's Complaint_ (which, I     think, is certainly not an early poem).     'poise' in 'occasions of some poise,' II. i. 122, is     exactly like 'poise' in 'full of poise and difficult weight,'     _O. _ III. iii. 82, and not exactly like 'poise' in     the three other places where it occurs.     'conjunct,' used only in II. ii. 125 (Q), V.     i. 12, recalls 'conjunctive,' used only in _H_. IV.     vii. 14, _O. _ I. iii. 374 (F).     'grime,' vb. , used only in II. iii. 9, recalls     'begrime,' used only in _O. _ III. iii. 387 and     _Lucrece_.     'unbonneted,' III. i. 14, appears only here and in     _O. _ I. ii. 23.     'delicate,' III. iv. 12, IV. iii. 15,     IV. vi. 188, is not a rare word with Shakespeare; he     uses it about thirty times in his plays. But it is worth     notice that it occurs six times in _O. _     'commit,' used intr. for 'commit adultery,' appears only in     III. iv. 83, but cf. the famous iteration in _O. _     IV. ii. 72 f.     'stand in hard cure,' III. vi. 107, seems to have no     parallel except _O. _ II. i. 51, 'stand in bold cure. '     'secure'=make careless, IV. i. 22, appears only here     and in _O. _ I. iii. 10 and (not quite the same sense)     _Tim. _ II. ii. 185.     Albany's 'perforce must wither,' IV. ii. 35, recalls     Othello's 'It must needs wither,' V. ii. 15.     'deficient,' IV. vi. 23, occurs only here and in _O. _     I. iii. 63.     'the safer sense,' IV. vi. 81, recalls 'my blood     begins my safer guides to rules,' _O. _ II. iii. 205.     'fitchew,' IV. vi. 124, is used only here, in _O. _     IV. i. 150, and in _T. C. _ V. i. 67 (where it     has not the same significance).     Lear's 'I have seen the day, with my good biting falchion I     would have made them skip,' V. iii. 276, recalls     Othello's 'I have seen the day, That with this little arm and     this good sword,' etc. , V. ii. 261. The fact that more than half of the above occur in the first two Acts of_King Lear_ may possibly be significant: for the farther removedShakespeare was from the time of the composition of _Othello_, the lesslikely would be the recurrence of ideas or words used in that play. NOTE S. _KING LEAR_ AND _TIMON OF ATHENS_. That these two plays are near akin in character, and probably in date,is recognised by many critics now; and I will merely add here a fewreferences to the points of resemblance mentioned in the text (p. 246),and a few notes on other points. (1) The likeness between Timon's curses and some of the speeches of Learin his madness is, in one respect, curious. It is natural that Timon,speaking to Alcibiades and two courtezans, should inveigh in particularagainst sexual vices and corruption, as he does in the terrific passageIV. iii. 82-166; but why should Lear refer at length, and with the sameloathing, to this particular subject (IV. vi. 112-132)? It almost looksas if Shakespeare were expressing feelings which oppressed him at thisperiod of his life. The idea may be a mere fancy, but it has seemed to me that thispre-occupation, and sometimes this oppression, are traceable in otherplays of the period from about 1602 to 1605 (_Hamlet_, _Measure forMeasure_, _Troilus and Cressida_, _All's Well_, _Othello_); while inearlier plays the subject is handled less, and without disgust, and inlater plays (e. g. _Antony and Cleopatra_, _The Winter's Tale_,_Cymbeline_) it is also handled, however freely, without this air ofrepulsion (I omit _Pericles_ because the authorship of thebrothel-scenes is doubtful). (2) For references to the lower animals, similar to those in _KingLear_, see especially _Timon_, I. i. 259; II. ii. 180; III. vi. 103 f. ;IV. i. 2, 36; IV. iii. 49 f. , 177 ff. , 325 ff. (surely a passage writtenor, at the least, rewritten by Shakespeare), 392, 426 f. I ignore theconstant abuse of the dog in the conversations where Apemantus appears. (3) Further points of resemblance are noted in the text at pp. 246, 247,310, 326, 327, and many likenesses in word, phrase and idea might beadded, of the type of the parallel 'Thine Do comfort and not burn,'_Lear_, II. iv. 176, and 'Thou sun, that comfort'st, burn! ' _Timon_, V. i. 134. (4) The likeness in style and versification (so far as the purelyShakespearean parts of _Timon_ are concerned) is surely unmistakable,but some readers may like to see an example. Lear speaks here (IV. vi. 164 ff. ):     Thou rascal beadle, hold thy bloody hand!     Why dost thou lash that whore? Strip thine own back;     Thou hotly lust'st to use her in that kind     For which thou whipp'st her. The usurer hangs the cozener.     Through tatter'd clothes small vices do appear;     Robes and furr'd gowns hide all. Plate sin with gold,     And the strong lance of justice hurtless breaks;     Arm it in rags, a pigmy's straw does pierce it.     None does offend, none, I say, none; I'll able 'em:     Take that of me, my friend, who have the power     To seal the accuser's lips. Get thee glass eyes;     And, like a scurvy politician, seem     To see the things thou dost not. And Timon speaks here (IV. iii. 1 ff. ):     O blessed breeding sun, draw from the earth     Rotten humidity; below thy sister's orb     Infect the air! Twinn'd brothers of one womb,     Whose procreation, residence, and birth,     Scarce is dividant, touch them with several fortunes,     The greater scorns the lesser: not nature,     To whom all sores lay siege, can bear great fortune,     But by contempt of nature.     Raise me this beggar, and deny't that lord:     The senator shall bear contempt hereditary,     The beggar native honour.     It is the pasture lards the rother's sides,     The want that makes him lean. Who dares, who dares.     In purity of manhood stand upright     And say 'This man's a flatterer'? if one be,     So are they all: for every grise of fortune     Is smooth'd by that below: the learned pate     Ducks to the golden fool: all is oblique;     There's nothing level in our cursed natures,     But direct villany. The reader may wish to know whether metrical tests throw any light onthe chronological position of _Timon_; and he will find such informationas I can give in Note BB. But he will bear in mind that results arrivedat by applying these tests to the whole play can have little value,since it is practically certain that Shakespeare did not write the wholeplay. It seems to consist (1) of parts that are purely Shakespearean(the text, however, being here, as elsewhere, very corrupt); (2) ofparts untouched or very slightly touched by him; (3) of parts where agood deal is Shakespeare's but not all (_e. g. _, in my opinion, III. v. ,which I cannot believe, with Mr. Fleay, to be wholly, or almost wholly,by another writer). The tests ought to be applied not only to the wholeplay but separately to (1), about which there is little difference ofopinion. This has not been done: but Dr. Ingram has applied one test,and I have applied another, to the parts assigned by Mr. Fleay toShakespeare (see Note BB. ). [268] The result is to place _Timon_ between_King Lear_ and _Macbeth_ (a result which happens to coincide with thatof the application of the main tests to the whole play): and this resultcorresponds, I believe, with the general impression which we derive fromthe three dramas in regard to versification. FOOTNOTES:[Footnote 268: These are I. i. ; II. i.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_name, split=\"train[350:500]\")\n",
    "dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zSbE1sJo4Tt"
   },
   "source": [
    "Let's then download the model. We first create a config object for quantization of the model using bitsandbytes. Bitsandbytes enables accessible large language models via k-bit quantization for PyTorch.\n",
    "\n",
    "We also need to download the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4162,
     "status": "ok",
     "timestamp": 1732231566263,
     "user": {
      "displayName": "Turkoosi",
      "userId": "15294961855769584895"
     },
     "user_tz": -120
    },
    "id": "ecbQfx6mYpSU",
    "outputId": "e9bde3f5-4a16-4ed4-dbad-d5bd3d01b781"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2Model.\n",
      "\n",
      "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.float16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "\n",
    "#model = AutoModelForCausalLM.from_pretrained(\n",
    "#    model_name,\n",
    "#    quantization_config=bnb_config,\n",
    "#    device_map={\"\": 0}\n",
    "#)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "from transformers import GPT2Tokenizer, TFGPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = TFGPT2Model.from_pretrained('gpt2')\n",
    "text = \"Teksti\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)\n",
    "\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer.add_eos_token = True\n",
    "#tokenizer.add_bos_token, tokenizer.add_eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8CHfh1xo4Tt"
   },
   "source": [
    "Below we set the access token to Waights & Biases. You should copy your access token from your account at [https://wandb.ai](https://wandb.ai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225,
     "referenced_widgets": [
      "0dcaae59a74c418da210d73052c6570d",
      "11d6412545794b499a34641c97cd27b7",
      "775db93d2a6347fb92356353b6596baa",
      "cdff08d6819f45b694da95f5297a47ef",
      "74904c275a704625bb34863dc4d89fc9",
      "a27e397d1d0f45b09816154cf14731d5",
      "414bce6eccb54cad8e9c8d333445420c",
      "e832ca64e54c4bd3b07c991ea069c2d2"
     ]
    },
    "executionInfo": {
     "elapsed": 6920,
     "status": "ok",
     "timestamp": 1732231613747,
     "user": {
      "displayName": "Turkoosi",
      "userId": "15294961855769584895"
     },
     "user_tz": -120
    },
    "id": "ow0n2oSOYz6V",
    "outputId": "52cf66bf-1aff-44ec-e749-fc37bb18302f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ts4m1wet) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dcaae59a74c418da210d73052c6570d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='2.331 MB of 2.331 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lemon-smoke-3</strong> at: <a href='https://wandb.ai/litaani-hermannin-nuorisoseura/gpt2-shake/runs/ts4m1wet' target=\"_blank\">https://wandb.ai/litaani-hermannin-nuorisoseura/gpt2-shake/runs/ts4m1wet</a><br/> View project at: <a href='https://wandb.ai/litaani-hermannin-nuorisoseura/gpt2-shake' target=\"_blank\">https://wandb.ai/litaani-hermannin-nuorisoseura/gpt2-shake</a><br/>Synced 4 W&B file(s), 0 media file(s), 34 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241121_223455-ts4m1wet/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ts4m1wet). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20241121_232646-b5zlwpns</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/litaani-hermannin-nuorisoseura/gpt2-shake/runs/b5zlwpns' target=\"_blank\">splendid-flower-4</a></strong> to <a href='https://wandb.ai/litaani-hermannin-nuorisoseura/gpt2-shake' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/litaani-hermannin-nuorisoseura/gpt2-shake' target=\"_blank\">https://wandb.ai/litaani-hermannin-nuorisoseura/gpt2-shake</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/litaani-hermannin-nuorisoseura/gpt2-shake/runs/b5zlwpns' target=\"_blank\">https://wandb.ai/litaani-hermannin-nuorisoseura/gpt2-shake/runs/b5zlwpns</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#monitering login\n",
    "wandb.login(key=\"16c1604d60d0b0ed9180c9298d5712933edc4917\")\n",
    "run = wandb.init(project='gpt2-shake', job_type=\"training\", anonymous=\"allow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYRdS-F2o4Tu"
   },
   "source": [
    "Then we'll create a configuration for the lo-rank adaptation method we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D409OdMjcc-n"
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XL2TWGlqo4Tu"
   },
   "source": [
    "We need to set the training arguments for the training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9sAp50mc4LR"
   },
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    save_steps=1000,\n",
    "    logging_steps=30,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.3,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3XybX8Oo4Tv"
   },
   "source": [
    "Finally we create the trainer object that uses supervised fine-tuning (SFT) as the training method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "executionInfo": {
     "elapsed": 282,
     "status": "error",
     "timestamp": 1732231692510,
     "user": {
      "displayName": "Turkoosi",
      "userId": "15294961855769584895"
     },
     "user_tz": -120
    },
    "id": "7JdMo0sqc_SB",
    "outputId": "b41050b2-b1b3-406d-e29a-d999c813e8a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TFGPT2Model' object has no attribute 'named_modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-58289d8966f5>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create the trainer object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m trainer = SFTTrainer(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_arguments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcustom_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing)\u001b[0m\n\u001b[1;32m    283\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautocast_adapter_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m                 if (\n\u001b[1;32m    287\u001b[0m                     \u001b[0margs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/mapping.py\u001b[0m in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name, mixed, autocast_adapter_dtype, revision)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_prompt_learning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mpeft_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_prompt_learning_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautocast_adapter_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautocast_adapter_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, peft_config, adapter_name, **kwargs)\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"default\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1608\u001b[0m     ) -> None:\n\u001b[0;32m-> 1609\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1610\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_prepare_inputs_for_generation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_empty_weights\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnullcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_additional_trainable_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, config, adapter_name, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_new_adapter_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLoraConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, peft_config, adapter_name, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXLORA\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXLORA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minject_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# Copy the peft_config in the injected model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36minject_adapter\u001b[0;34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0mis_target_modules_in_base_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mkey_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"target_modules\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDUMMY_TARGET_MODULES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TFGPT2Model' object has no attribute 'named_modules'"
     ]
    }
   ],
   "source": [
    "# Setting sft parameters\n",
    "\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Create the trainer object\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=512,  # Adjust as needed\n",
    "    dataset_text_field=\"text\",  # Field in your dataset containing the text\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#trainer = SFTTrainer(\n",
    "#    model=model,\n",
    "#    train_dataset=dataset,\n",
    "#    peft_config=peft_config,\n",
    "#    max_seq_length= None,\n",
    "#    dataset_text_field=\"text\",\n",
    "#    tokenizer=tokenizer,\n",
    "#    args=training_arguments,\n",
    "#    packing=False,\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CDJGQ98o4Tv"
   },
   "source": [
    "Then, we can execute the training run. This will approximately 8 hours using the T4 GPU available in Colab and the dataset of 10,000 samples we downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "executionInfo": {
     "elapsed": 319,
     "status": "error",
     "timestamp": 1732197825684,
     "user": {
      "displayName": "Turkoosi",
      "userId": "15294961855769584895"
     },
     "user_tz": -120
    },
    "id": "pEiMWxk9dFLk",
    "outputId": "ce09e3c1-9c6c-4827-beb9-cd006ddaf37f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-252f3821ece6>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4b3e8960517e4833aac7ee7f46fe9798",
      "f38827661b864905b7c57b98c7d052b2",
      "d00ed4b418f8458f8593c37579e4a26b",
      "64ed3e887b334b55ad2fbdca16073692",
      "9a2ef833bb6a487db847ad36d4fbc148",
      "48d11bde54de4d759bf456fffd1bb641",
      "9d652967bc7546e79e35fdbe163c688e",
      "4b1e9caa6f2f46d68242611b813c0a02"
     ]
    },
    "executionInfo": {
     "elapsed": 13888,
     "status": "ok",
     "timestamp": 1732112481350,
     "user": {
      "displayName": "Turkoosi",
      "userId": "15294961855769584895"
     },
     "user_tz": -120
    },
    "id": "UZg7GbvHdOUc",
    "outputId": "e88bed8e-2441-4662-be40-d2f0ca1cd5a0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3e8960517e4833aac7ee7f46fe9798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.743 MB of 0.743 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1308289571880960.0</td></tr><tr><td>train/epoch</td><td>0.92308</td></tr><tr><td>train/global_step</td><td>6</td></tr><tr><td>train_loss</td><td>1.33972</td></tr><tr><td>train_runtime</td><td>413.3441</td></tr><tr><td>train_samples_per_second</td><td>0.242</td></tr><tr><td>train_steps_per_second</td><td>0.015</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clean-durian-2</strong> at: <a href='https://wandb.ai/litaani-hermannin-nuorisoseura/Fine%20tuning%20mistral%207B/runs/bymf0720' target=\"_blank\">https://wandb.ai/litaani-hermannin-nuorisoseura/Fine%20tuning%20mistral%207B/runs/bymf0720</a><br/> View project at: <a href='https://wandb.ai/litaani-hermannin-nuorisoseura/Fine%20tuning%20mistral%207B' target=\"_blank\">https://wandb.ai/litaani-hermannin-nuorisoseura/Fine%20tuning%20mistral%207B</a><br/>Synced 4 W&B file(s), 0 media file(s), 14 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241120_141210-bymf0720/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32768, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (up_proj): lora.Linear4bit(\n",
       "            (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "wandb.finish()\n",
    "model.config.use_cache = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwXejljEo4Tw"
   },
   "outputs": [],
   "source": [
    "def stream(user_prompt):\n",
    "    runtimeFlag = \"cuda:0\"\n",
    "    system_prompt = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n'\n",
    "    B_INST, E_INST = \"### Instruction:\\n\", \"### Response:\\n\"\n",
    "    prompt = f\"{system_prompt}{B_INST}{user_prompt.strip()}\\n\\n{E_INST}\"\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "executionInfo": {
     "elapsed": 351,
     "status": "error",
     "timestamp": 1732198053312,
     "user": {
      "displayName": "Turkoosi",
      "userId": "15294961855769584895"
     },
     "user_tz": -120
    },
    "id": "uV300fKYo4Tw",
    "outputId": "0828ee20-7c8b-498a-d696-a09f7928970f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stream' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-8c3eb275184e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"what is newtons 3rd law and its formula\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stream' is not defined"
     ]
    }
   ],
   "source": [
    "stream(\"what is newtons 3rd law and its formula\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GU3kbjXJo4Tw"
   },
   "outputs": [],
   "source": [
    "#base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#    model_name, low_cpu_mem_usage=True,\n",
    "#    return_dict=True,torch_dtype=torch.float16,\n",
    "#    device_map= {\"\": 0})\n",
    "#model = PeftModel.from_pretrained(base_model, new_model)\n",
    "#model = model.merge_and_unload()\n",
    "\n",
    "base_model = GPT2Model.from_pretrained('gpt2',\n",
    "    low_cpu_mem_usage=True,\n",
    "    #return_dict=True,torch_dtype=torch.float16,\n",
    "    )\n",
    "#model = PeftModel.from_pretrained(base_model, new_model)\n",
    "#model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', trust_remote_code=True)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "executionInfo": {
     "elapsed": 940,
     "status": "error",
     "timestamp": 1732227816085,
     "user": {
      "displayName": "Turkoosi",
      "userId": "15294961855769584895"
     },
     "user_tz": -120
    },
    "id": "JemcEGzwo4Tw",
    "outputId": "b6839d1a-0cad-411f-866c-3bef27fbb73b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-17c09fb980ca>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.push_to_hub(new_model)\n",
    "tokenizer.push_to_hub(new_model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0922c712e72b45e7a366469ef94943c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0dcaae59a74c418da210d73052c6570d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_11d6412545794b499a34641c97cd27b7",
       "IPY_MODEL_775db93d2a6347fb92356353b6596baa"
      ],
      "layout": "IPY_MODEL_cdff08d6819f45b694da95f5297a47ef"
     }
    },
    "11d6412545794b499a34641c97cd27b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74904c275a704625bb34863dc4d89fc9",
      "placeholder": "​",
      "style": "IPY_MODEL_a27e397d1d0f45b09816154cf14731d5",
      "value": "2.331 MB of 2.331 MB uploaded\r"
     }
    },
    "222cabbaad2d4dfdabd85341d6ecc3ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f11a672cb654324b2d897ca7b84c9a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "414bce6eccb54cad8e9c8d333445420c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4374d6a5b4734beca9c1b3b7dd637836": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "48d11bde54de4d759bf456fffd1bb641": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b1e9caa6f2f46d68242611b813c0a02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4b3e8960517e4833aac7ee7f46fe9798": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f38827661b864905b7c57b98c7d052b2",
       "IPY_MODEL_d00ed4b418f8458f8593c37579e4a26b"
      ],
      "layout": "IPY_MODEL_64ed3e887b334b55ad2fbdca16073692"
     }
    },
    "4d78fa5f573345149326478c4accc0cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5347a7cd9f3f4df3b47afe4abee76e3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64ed3e887b334b55ad2fbdca16073692": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6dd0f42e40834659a5d0af25f4b1dcbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b42d72766a7245ffb233c8492a577ed6",
      "placeholder": "​",
      "style": "IPY_MODEL_222cabbaad2d4dfdabd85341d6ecc3ac",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "74904c275a704625bb34863dc4d89fc9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "775db93d2a6347fb92356353b6596baa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_414bce6eccb54cad8e9c8d333445420c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e832ca64e54c4bd3b07c991ea069c2d2",
      "value": 1
     }
    },
    "7a844e3e912e4b8a869d9ee5fd87f420": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5347a7cd9f3f4df3b47afe4abee76e3c",
      "placeholder": "​",
      "style": "IPY_MODEL_9ebf9db99fde47b6a973ac9c70976938",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "7d547ac1d67346a09ccebc3f68fe2732": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_3f11a672cb654324b2d897ca7b84c9a0",
      "placeholder": "​",
      "style": "IPY_MODEL_4374d6a5b4734beca9c1b3b7dd637836",
      "value": ""
     }
    },
    "872147c0859a4f5e99c3f1a1a5c9678f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "8d25e792e3434a40bddd5ca32ecfb00e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_4d78fa5f573345149326478c4accc0cf",
      "style": "IPY_MODEL_e1b431abf1c1408f8e6ac2b11e056735",
      "value": true
     }
    },
    "957912ebca8e4a6a981405931b15de75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9a2ef833bb6a487db847ad36d4fbc148": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d652967bc7546e79e35fdbe163c688e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ebf9db99fde47b6a973ac9c70976938": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a27e397d1d0f45b09816154cf14731d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a552768046e44779a5762f24054094c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_af07c80607424a8eb05ed7cc5152d78f",
      "style": "IPY_MODEL_d5a8af79e64647449485e64425131073",
      "tooltip": ""
     }
    },
    "af07c80607424a8eb05ed7cc5152d78f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b42d72766a7245ffb233c8492a577ed6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdff08d6819f45b694da95f5297a47ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d00ed4b418f8458f8593c37579e4a26b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d652967bc7546e79e35fdbe163c688e",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4b1e9caa6f2f46d68242611b813c0a02",
      "value": 1
     }
    },
    "d1f2f7668d444de1ae5e1823c7418eda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [],
      "layout": "IPY_MODEL_872147c0859a4f5e99c3f1a1a5c9678f"
     }
    },
    "d5a8af79e64647449485e64425131073": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "e09e8a2853804dfc9ce4f365b563ced6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0922c712e72b45e7a366469ef94943c2",
      "placeholder": "​",
      "style": "IPY_MODEL_957912ebca8e4a6a981405931b15de75",
      "value": "Connecting..."
     }
    },
    "e1b431abf1c1408f8e6ac2b11e056735": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e832ca64e54c4bd3b07c991ea069c2d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f38827661b864905b7c57b98c7d052b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9a2ef833bb6a487db847ad36d4fbc148",
      "placeholder": "​",
      "style": "IPY_MODEL_48d11bde54de4d759bf456fffd1bb641",
      "value": "0.946 MB of 0.946 MB uploaded\r"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
